"""
===============================================================================
CONNECT Iterative Data Analysis and Visualization Toolkit – User Guide
===============================================================================

Overview
--------
This program is a complete toolkit for analyzing and visualizing training data 
generated by the CONNECT iterative process. It automatically loads data from a 
specified project folder (which contains iteration subdirectories), processes the data,
and then produces multiple types of outputs:

  1. **Univariate Analysis:**  
     - Generates histograms, box plots, and summary statistics for likelihood values 
       and Δχ² (delta chi-squared) values. Only if enabled or the likelihood-filter was 
     - Uses the Analyze_likelihoods module.

  2. **Iteration Plots:**  
     - Creates detailed two-panel scatter plots that show both the new points generated 
       in an iteration (upper panel) and the accumulated state of the training data 
       up to that iteration (lower panel).  
     - Uses the PlotIterations module.

  3. **Triangle (Corner) Plot:**  
     - Produces a pairplot (corner plot) of the final iteration’s snapshot, contrasting 
       accepted vs. discarded points across multiple parameters.  
     - Uses the TrianglePlot module.

This guide explains how to run the entire program via the command line, the meaning and 
format of each option, and how to customize nearly every aspect of the analysis and visualization.

Prerequisites and Installation
-------------------------------
Before running the program, ensure that you have the following Python libraries 
installed:

  - numpy
  - pandas
  - scipy
  - matplotlib
  - seaborn
  - tabulate

To check whether these packages are installed, run in a Unix shell:

    pip show numpy pandas scipy matplotlib seaborn tabulate

If any package is missing, install it using pip or conda. For example:

    pip install numpy pandas scipy matplotlib seaborn tabulate
    
Also ensure you have LaTeX installed on your system:
    which latex

If not installed, you can install texlive or other LaTeX distributions.:
    https://www.tug.org/texlive/

Running the Program
-------------------
The program is invoked via the command-line interface. At a minimum, you must supply the 
path to your CONNECT project folder using the `--project_path` argument. Optionally, you can 
specify an output directory via `--output_path` (if not provided, a default subfolder within 
the project folder is used (e.g., `iteration_analysis`).

### Minimal Command Example
    python plot_iterations.py --project_path "/path/to/connect/data/lcdm_example"

This command loads the training data from the specified folder and then runs all analysis 
and plotting routines using default options. The above command will attempt to create the triangle plot,
with every single parameter, which can be very memory intensive, so it might terminate on a normal laptop.
It also create the two-panel scatter plot with default cosmological pairs (H0, omega_b).

### Full Command Example

    python plot_iterations.py --project_path "/path/to/connect/data/lcdm_example" \
                     --output_path "/path/to/output" \
                        --run_univariate_analysis "True" \                      
                        --run_triangle_plot "True" \
                        --run_plot_iterations "True" \
                        --triangle_plot_cosmoparams "['H0', 'omega_b', 'tau_reio']" \
                        --plot_iter_cosmoparams "[('H0', 'omega_b'), ('H0', 'tau_reio')]" \
                        --latex_labels "{'H0': r'$H_0$', 'omega_b': r'$\\Omega_b$', 'tau_reio': r'$\\tau_{reio}$'}" \ 
                        --dataloader_args "{'save_pickle_data': True, 'pickle_path': '/path/to/pickle_file'}" \
                        --analyze_lkl_args "{'x_range': [0, 5000], 'y_range': [0, 500], 'include_inset': True, 'save_formats': ['png', 'pdf']}" \
                        --plot_iter_args "{'x_range': [0, 100], 'y_range': [0, 100], 'combine_iteration_0_and_1': False, 'plot_contours': True, 'sigma': 15}" \
                        --triangleplot_args "{'iterations_to_plot': [0, 1, 2, 3, 4, 5], 'colormap': 'tab10', 'plot_contours': True, 'sigma': 12}"

# All arguments except project_path are optional. But run_univariate_analysis, run_triangle_plot, run_plot_iterations 
# are RECOMMENDED to be set to True, if you want to run the respective analysis and plotting routines.
# And set to False, if you want to skip them.

# The triangle_plot_cosmoparams and plot_iter_cosmoparams arguments are RECOMMENDED to specify the parameters to be plotted in the triangle plot and iteration plots, respectively.
# They can also be set to "all" to include all parameters, but be aware that this can be memory-intensive for the triangle plot and excessive for the two-panel two parameter iteration plot, as it produces a plot for every pair of parameters.

# Note: latex_labels, dataloader_args, analyze_lkl_args, plot_iter_args, triangleplot_args are optional arguments.
# They are only for customization of the output plots and analysis, if the default settings are not satisfactory.
# Such as default calculated axis ranges, default data categories plotted, default color maps, etc.
                                                    
This command runs the full analysis and visualization pipeline with customized options.
For all available custmization options for the `dataloader_args`, `analyze_lkl_args`, `plot_iter_args` and `triangleplot_args` dictionaries, see the last section of this guide.

Full Program Workflow
---------------------
1. **Data Loading:**  
   - The program begins by scanning the given project folder for iteration subdirectories 
     (for example, an initial sampling folder such as "N-10000" and subsequent folders like "number_1", "number_2", etc.).
   - It then loads data from files (e.g. model parameters, derived parameters, likelihood data) and 
     organizes it into a nested dictionary under the key `"iteration_data"`.

2. **Univariate Analysis (Analyze_likelihoods):**  
   - If enabled, the program performs univariate statistical analysis on the likelihood data.
   - It produces histograms, box plots, and summary tables (in CSV, LaTeX, and Markdown formats) 
     that detail the evolution of the likelihood values and Δχ² across iterations.

3. **Iteration Plotting (PlotIterations):**  
   - Detailed two-panel scatter plots are generated for selected parameter pairs.
   - The upper panel shows data generated in a given iteration (newly accepted, discarded, etc.),
     while the lower panel shows the cumulative training data up to that iteration.
   - Optionally, contour overlays are computed and drawn on the scatter plots to highlight regions 
     of interest based on Δχ² values.
   - The plot can include a threshold line (if a delta-χ² threshold is specified in the parameter 
     configuration) to mark critical levels.

4. **Triangle Plot (TrianglePlot):**  
   - A corner (pair) plot is created to provide a broad overview of the final snapshot of the data.
   - It displays pairwise relationships between selected parameters, helping you identify interesting 
     correlations or clusters.
   - Optional overlays include filled Δχ² contours and a threshold line.

Command-Line Options and Detailed Customization
-----------------------------------------------
All non-string arguments (such as lists, dictionaries, booleans) must be supplied as Python literal 
strings (e.g., "['H0', 'omega_b']" or "{'x_range': [0, 10]}").

Below are the main CLI options available:

### Required Arguments
- **--project_path** (str)  
  Specifies the folder containing CONNECT data (must include iteration subfolders and a parameter file 
  like `log_connect.param`).  
  *Example:*  
      --project_path "/home/user/connect_data/lcdm_example"

### Optional Output Control
- **--output_path** (str)  
  Directory where all generated analysis outputs (plots, tables, reports) will be saved.  
  *Default:* `<project_path>/iteration_analysis`

### Verbosity
- **--verbose** (int, 0–5)  
  Controls the amount of console output during processing. A higher number gives more detailed logs.  
  *Example:*  
      --verbose 3

### Analysis and Plotting Flags
- **--run_univariate_analysis** (bool)  
  If set to "True", the program will perform univariate statistical analysis on the likelihood data.  
  *Example:*  
      --run_univariate_analysis "True"
  
- **--run_triangle_plot** (bool)  
  If "True", a triangle (corner) plot of the final iteration’s data will be generated.  
  *Example:*  
      --run_triangle_plot "True"
  
- **--run_plot_iterations** (bool)  
  Enables detailed iteration plots (two-panel scatter plots).  
  *Example:*  
      --run_plot_iterations "True"

### Parameter Selection Options
- **--triangle_plot_cosmoparams** (list or "all")  
  A list of parameter names to include in the triangle plot.  
  *Example:*  
      --triangle_plot_cosmoparams "['H0', 'omega_b', 'tau_reio']"  
  Use "all" to include every parameter.
  
- **--plot_iter_cosmoparams** (tuple or list of tuples)  
  Specify one or more parameter pairs for iteration plots.  
  *Example:*  
      --plot_iter_cosmoparams "[('H0', 'omega_b'), ('H0', 'tau_reio')]"

### Customization of Labels and Styles
- **--latex_labels** (dict)  
  Custom mapping of parameter names to LaTeX-formatted labels for display in plots.  
  *Example:*  
      --latex_labels "{'H0': r'$H_0$', 'omega_b': r'$\\Omega_b$'}"

- **--dataloader_args** (dict)  
  Options for the data loader. For instance, enable pickling to speed up future runs:  
      {"save_pickle_data": True, "pickle_path": "/path/to/pickle_file"}

- **--analyze_lkl_args** (dict)  
  Additional settings for univariate analysis. Options include:
    - `"x_range_hist"`, `"y_range_hist"`: Set the axis ranges for histograms.
    - `"x_range_inset_hist"`, `"y_range_inset_hist"`: Set ranges for inset histograms.
    - `"include_inset"`: (bool) Include an inset plot.
    - `"y_range_box"` : (list) Explicit axis limits for box plots.
    - `"max_delta_chi2_in_dataset"`, `"max_loglkl_in_dataset"`: Exclude extreme values from the dataset.
    - `"x_range_inset"`, `"y_range_inset"`: Set ranges for inset plots.
    - `"include_inset"`: (bool) Include an inset plot.
    - `"save_formats"`: (list) Specify output file formats (e.g., ["png", "pdf"]).

- **--plot_iter_args** (dict)  
  Customize iteration plots. Key options include:
    - **General Options:**
      - `"params_plot_iter"`: Specify parameter pairs to plot (or "all" for all combinations).
      - `"iterations_to_plot"`: List the iterations to include (or "all").
      - `"max_subplots"`: Maximum columns to display (excess iterations will be grouped).
      - `"combine_iteration_0_and_1"`: (bool) Combine iterations 0 and 1 in one subplot.
      - `"show_super_labels"`: (bool) Display super axis labels on the figure.
    - **Axis and Data Filtering:**
      - `"x_range"`, `"y_range"`: Explicit axis limits for all subplots.
      - `"ignore_iteration_0_for_axis"`: (bool) Exclude iteration 0 when calculating axis limits.
      - `"include_lkldiscard_new_for_axis_range"`: (bool) Include likelihood-filtered discarded points.
      - `"data_types_to_plot"`: (dict) Specify which data categories to plot in the upper and lower panels.
    - **Marker and Styling:**
      - `"colormap"`: Name of the colormap for iteration colors (e.g., "tab10").
      - `"marker_size"`, `"marker_edge_width"`: Adjust the size and edge width of markers.
      - `"alpha_accepted"`, `"alpha_discarded"`: Opacity settings.
      - `"legend_labels"`: Custom legend labels.
      - `"marker_styles"`: Define custom marker styles per data category.
    - **Contour Options:**
      - `"plot_contours"`: (bool) Whether to overlay Δχ² contours.
      - `"grid_size"`: Number of grid points per axis for contour interpolation.
      - `"sigma"`: Gaussian smoothing parameter for contours.
      - `"interpolation_method"`: Interpolation method ("linear", "nearest", or "cubic").
      - `"discretize_cmap"`: (bool) Use a discretized colormap.
      - `"n_discretize_cmap"`: Number of discrete colors if discretizing.
      - `"cmap_norm_type"`: Normalization type ("PercentileNorm" or "PowerNorm").
      - `"cmap_contour"`, `"alpha_contourf"`, `"alpha_contour"`: Contour colormap and opacity settings.
      - `"contour_line_overlay"`: (bool) Whether to draw contour lines over filled contours.
      - `"contour_linewidths"`: Width of contour lines.
      - **Threshold Overlay:**
        - `"plot_threshold_line"`: (bool) Whether to draw a threshold line on the contours.
        - `"delta_chi2_threshold_color"`: Color of the threshold line (e.g., "purple").
        - `"delta_chi2_threshold_linewidth"`: Line width for the threshold line.
    - **Saving Options:**
      - `"save_x_y_loglkl"`: (bool) Save the (x, y, true_loglkl) data as a pickle file.
      - `"pickle_contour_data"`: Preloaded contour data for reuse.

- **--triangleplot_args** (dict)  
  Similar to `plot_iter_args`, but for the triangle plot. Options include:
    - **General Options:**  
      `"iterations_to_plot"`, `"params_triangleplot"`, and `"colormap"` for iteration color mapping.
    - **Axis & Filtering:**  
      `"ignore_iteration_0_for_axis"`, `"include_lkldiscard_new_for_axis_range"`, and 
      `"data_types_to_plot"`.
    - **Marker & Contour Styling:**  
      Options for `"marker_size"`, `"marker_alpha"`, `"marker_edge_lw"`, as well as all contour options 
      (e.g., `"plot_contours"`, `"grid_size"`, `"sigma"`, `"interpolation_method"`, `"n_discretize_cmap"`, 
      `"cmap_contour"`, `"alpha_contourf"`, `"alpha_contour"`, `"contour_line_overlay"`, `"contour_linewidths"`).
    - **Threshold Overlay:**  
      `"plot_threshold_line"`, `"delta_chi2_threshold_color"`, and `"delta_chi2_threshold_linewidth"`.
    - **Saving Options:**  
      `"save_params_loglkls"` and `"pickle_path"` if needed.
      
For more detailed information on each option and use cases, refer to the last section of this guide.

Using the CLI – A Full Guide
-----------------------------
1. **Basic Invocation:**  
   To run the entire analysis with default settings:
   
       python plot_iterations.py --project_path "/path/to/connect/data"
   
2. **Customizing Output Location:**  
   Specify an output folder:
   
       python plot_iterations.py --project_path "/path/to/data" --output_path "/path/to/output"
   
3. **Enabling/Disabling Specific Analyses:**  
   To run only univariate analysis (and skip the triangle and iteration plots):
   
       python plot_iterations.py --project_path "/path/to/data" --run_triangle_plot "False" --run_plot_iterations "False"
   
4. **Customizing Parameter Selection for Plots:**  
   For a triangle plot using only selected parameters:
   
       python plot_iterations.py --project_path "/data" --triangle_plot_cosmoparams "['H0', 'omega_b', 'n_s']"
   
   For iteration plots focusing on specific parameter pairs:
   
       python plot_iterations.py --project_path "/data" --plot_iter_cosmoparams "[('H0', 'omega_b'), ('H0', 'tau_reio')]"
   
5. **Detailed Styling and Contour Customization:**  
   Use dictionary arguments to fine-tune the appearance of your plots. For example, to set custom 
   axis ranges, marker styles, and contour properties for the iteration plots:
   
       python plot_iterations.py --project_path "/data" \
           --plot_iter_args "{'x_range': [0, 10], 'y_range': [0, 10], 'combine_iteration_0_and_1': True, 'plot_contours': True, 'sigma': 15}"
   
   Similarly, you can pass customization options for the triangle plot using `--triangleplot_args`.
   
   See the full list of available options in the last section below.

6. **Customizing Labels with LaTeX:**  
   To use LaTeX-formatted labels for parameters in plots, which aren't supported by default:
   
       --latex_labels "{'H0': r'$H_0$', 'omega_b': r'$\\Omega_b$', 'n_s': r'$n_s$'}"
   
   These labels will appear in all plots generated by the program.

7. **Pickling Data to Save Time:**  
   If you wish to save the loaded data use:
   
       --dataloader_args "{'save_pickle_data': True, 'pickle_path': '/path/to/pickle_file'}"
       --plot_iter_args "{'save_x_y_loglkl': True}"
   
   This caches intermediate results.

Outputs
-------
After processing, the program will generate several types of output files, saved under the output directory:
  
  - **Univariate Analysis Reports:**  
    Histograms, box plots, and summary tables (CSV, LaTeX, Markdown) appear in an `likelihood_analysis` subfolder.
  
  - **Iteration Plots:**  
    Detailed two-panel scatter plots are saved in an `iteration_plots` subfolder.
  
  - **Triangle Plots:**  
    Pairplots (corner plots) are saved in a `pairplots` subfolder.
  
Each output file is saved in one or more formats as specified (e.g., PNG, PDF).

Troubleshooting and Tips
-------------------------
- **Verbose Mode:** Increase the verbosity using `--verbose` (e.g., `--verbose 3` or higher) to print detailed logs for debugging.
  
- **Data Issues:**  
  If no data appears in your plots, check that the project folder contains the correct iteration subfolders and that files like `model_params.txt`, `derived.txt`, and `likelihood_data.txt` exist.

- **Axis Ranges:**  
  If your plots appear clipped or with unexpected scales, consider manually specifying axis ranges via the `x_range` and `y_range` (or inset variants) in your customization dictionaries.
  
- **Contour Appearance:**  
  Experiment with `grid_size`, `sigma`, and `interpolation_method` if the contour overlays do not match your expectations. The threshold line for contours depends on the `delta_chi2_threshold` value defined in your parameter file (accessed via the `param_connect` object).

- **Legend Customization:**  
  Both iteration plots and triangle plots have customizable legends. You can supply your own legend labels and marker styles using `legend_labels` and `marker_styles` in the respective argument dictionaries.

Conclusion
----------
This toolkit is designed for flexibility and deep customization. By carefully setting the command-line arguments,
you can tailor the analysis and visualization to suit your specific CONNECT project and research needs. This guide 
provides detailed instructions on using the CLI to control every aspect of data processing and output generation, 
from data loading and univariate analysis to the creation of detailed iteration and triangle plots.


Author: [Your Name]
Date: [Insert Date]
License: [Insert License, e.g., MIT]
===============================================================================


Full list of options for the `dataloader_args`, `analyze_lkl_args`, `plot_iter_args` and `triangleplot_args` dictionaries:
------------------------------------------------------------------------------------------------------------------------

dataloader_args:
----------------


analyze_lkl_args:
---------------------------------------------------------------------------------------------------
--analyze_lkl_args (dict):
    "**OPTIONAL**: A dictionary of additional arguments to pass to the Analyze_likelihoods class.\n"
    "This can be used to customize the plots for the univariate analysis by overriding default settings.\n"
    "Examples could be to override the axis ranges, what data categories to plot, or to exclude the inset plot.\n\n"
    
    "Supported keys for the analyze_lkl_args dict include:\n"
    "  'max_delta_chi2_in_dataset' (float): Maximum Δχ² value to include in the dataset for the plots. (default: None)\n"
    "       Can be used to exclude extreme Δχ² values from the dataset for the plots. Is particularly useful if you want to focus on the bulk of the data and exclude extreme outliers in the display.\n"
    "       Only the data below this threshold will be included when calculating bin sizes for the histograms or creating the box plots.\n"
    "  'max_loglkl_in_dataset' (float): Maximum -log(lkl) value to include in the dataset for the plots. (default: None)\n"
    "  'x_range_hist' (list): Explicit x-axis range to display in the histogram, e.g., [0, 10].\n"
    "       This differ from the 'max_delta_chi2_in_dataset' in that it only affects the display range of the histogram, not the data included in the analysis.\n"
    "  'y_range_hist' (list): Explicit y-axis range to display in the histogram, e.g., [0, 10].\n"
    "  'x_range_inset_hist' (list): x-axis range for the inset in histogram.\n"
    "  'y_range_inset_hist' (list): y-axis range for the inset in histogram.\n"
    "       As default the ranges are calculated automatically and shared between\n"
    "       the main and inset plots, respectively.\n"
    "  'y_range_box' (list): Explicit y-axis range to display in the box plot, e.g., [0, 10].\n"
    "  'create_these_outputs' dict: Specify which outputs to create. (default: 'all' or {'histograms': True, 'boxplots': True, 'summary_tables': True, 'evolution_plot': True})\n"
    "  'save_formats' (list): List of output file formats (e.g., ['png', 'pdf']).\n"
    "  'include_inset' (bool): Include an inset plot in the analysis (default True).\n"
    "       As default the inset plot is included and shows the accumulated state\n"
    "       of accepted and discarded points.\n"
    "       But it can also be used to zoom in on specific data types or ranges,\n"
    "       if needed, by setting the 'x_range_inset' and 'y_range_inset'.\n"
    "       Or by changing 'hist_inset' below. Set to False to exclude the inset plot.\n"
    "  'data_categories' (list): List of data categories to include in the analysis.\n"
    "       Default data types:\n"
    "           [\n"
    "               'accepted_accumulated'  # The accumulated state of accepted points at the end\n"
    "                                       # of each iteration.\n"
    "               'cumulative_discarded_likelihood'  # The accumulated state of likelihood-filter\n"
    "                                                  # discarded points at the end of each iteration.\n"
    "               'accepted_new'  # The new accepted points in each iteration that survived\n"
    "                                # the likelihood filter.\n"
    "               'accepted_old'  # The accepted points from the previous iteration that were not\n"
    "                                # discarded by the likelihood filter.\n"
    "               'discarded_likelihood_new'  # The new points from MCMC chains that were\n"
    "                                           # discarded by the likelihood filter in each iteration.\n"
    "               'discarded_likelihood_old'  # The accepted points from the previous iteration that\n"
    "                                           # were discarded by the likelihood filter this iteration.\n"
    "               'discarded_likelihood'  # The total discarded points by the likelihood filter in\n"
    "                                       # each iteration. (new + old)\n"
    "               'discarded_iteration'  # Discards if the entire iteration was discarded\n"
    "                                      # (i.e., if the user chose to discard the full iteration).\n"
    "                                      # Not necessarily due to likelihood filtering.\n"
    "           ]\n"
    "  'hist_main_panel' (list): List of dicts defining histogram settings for the main panel.\n"
    "       This argument can be used to specify which data categories to plot in the\n"
    "       main panel of the histogram plot.\n"
    "       Each dict should include keys 'category', 'label', 'color', and 'plot_kws'.\n"
    "       'category' (str): Data category to plot (e.g., 'accepted_new', 'discarded_likelihood_new').\n"
    "       'label' (str): Custom label for the category (if None: fallback to self.category_labels).\n"
    "       'color' (str): Custom color for the category (if None: fallback to self.DATA_COLORS).\n"
    "       'plot_kws' (dict): Additional plotting options for the category utilized\n"
    "                         by seaborn.histplot.\n"
    "           Supported keys: 'stat', 'alpha', 'fill', 'element', 'multiple', 'log_scale', etc.\n"
    "       The categories included are the ones shown in the main panel/subplots of the histogram plot.\n"
    "       Example:\n"
    "           [{'category': 'accepted_new', 'label': 'Accepted New', 'color': 'blue',\n"
    "             'plot_kws': {'stat': 'count', 'alpha': 1, 'fill': True, 'element': 'bars',\n"
    "                          'multiple': 'stack', 'log_scale': True}},\n"
    "            ..., {next category...}, ...]\n"
    "                       Currently, only 'count' makes sense for 'stat', if one wants to compare the data across iterations.\n"
    "       If 'label' and 'color' are set to None, the default labels and colors are used\n"
    "       as defined in 'category_labels' and 'data_colors' below.\n"
    "  'hist_inset' (list): List of dicts defining histogram settings for the inset panel.\n"
    "       It follows the exact same structure as 'hist_main_panel'.\n"
    "       Used to define the data categories shown in the inset plot.\n"
    "       Example:\n"
    "           [{'category': 'accepted_accumulated', 'label': 'Accepted Accumulated',\n"
    "             'color': 'green', 'plot_kws': ...}]\n"
    "  'category_labels' (dict): Custom labels for the data categories (if None, default labels are used).\n"
    "       Example:\n"
    "           {'accepted_accumulated': 'Cumulative Accepted',\n"
    "            'cumulative_discarded_likelihood': 'Cumulative Discarded', ...}\n"
    "  'data_colors' (dict): Custom colors for the data categories.\n"
    "       Default colors are provided for the default data categories.\n"
    "       Example:\n"
    "           {'accepted_accumulated': 'tab:green',\n"
    "            'cumulative_discarded_likelihood': 'tab:red', ...}\n"
    "Example usage:\n"
    "    --analyze_lkl_args \"{'include_inset': True, 'x_range': [0, 10], 'save_formats': ['png']}\"\n"




plot_iter_args:
-----------------------------------------------------------------------------------------------------

--plot_iter_args (dict):
    A dictionary of additional arguments to pass to the PlotIterations class.
    It can be used to customize the iteration plots by overriding default settings.

    **General Plot Options**:
       'params_plot_iter' (list or 'all'):
           Parameter pairs to plot (default: ('H0','omega_n')).
           Example: [('H0', 'omega_b'), ('H0', 'tau_reio')].
           This is the same as the 'plot_iter_cosmoparams' argument. So, you can use either of them.
       'iterations_to_plot' (list or 'all'):
           Specific iterations to plot (i.e. [0,1,2]; default: 'all').
           It still uses all iterations to build the axis ranges and the contour plot, etc.
           This is useful if you want to focus on specific iterations in the plots.
           Or if you want to split your figure into multiple figures with different iterations
           to avoid too wide a figure.
           I.e. you could create one figure with [0,1,2,3,4,5] and another with [6,7,8,9,10].
       'max_subplots' (int):
           Maximum number of subplot columns (default: 10). If the number of iterations exceeds this,
           they will be grouped.
           This is useful if you have many iterations and want to avoid a too wide figure.
           It automatically groups and distributes consecutive iterations evenly over the columns.
           The grey discards get a colored edge to represent their iteration.
           But it might get cluttered with multiple iterations in one subplot.
       'save_formats' (list):
           Output file formats, e.g., ['png', 'pdf'].
       'iteration0_figure_inset' (bool):
           Insert an inset subfigure for iteration 0 data in the figure in the first column
           (default: True).
           Usually one wants to zoom in on the iterations after initial sampling, so this was
           created to display the full initial sampling while still maintaining the same axis
           ranges for the main figure.
       'show_super_legend' (bool):
           Whether to display a super legend (default True).

    **Axis & Data Filtering Options**:
       'x_range' (list):
           Override the shared x-axis range, e.g., [0, 10].
       'y_range' (list):
           Override the shared y-axis range, e.g., [0, 10].
           If not provided, the axis ranges are calculated automatically based on the data.
       'ignore_iteration_0_for_axis' (bool):
           Exclude iteration 0 when computing axis ranges (default: True).
           This is useful if iteration 0 (initial sampling) has a very different distribution than
           the other iterations.
       'include_lkldiscard_new_for_axis_range' (bool):
           Include 'discarded_likelihood_new' for axis range calculation (default: True).
           'discarded_likelihood_new' is the newly generated points from the MCMC chains that were
           discarded by the likelihood filter.
           This was added to exclude these points during testing of the likelihood filter.
           Perhaps not useful any longer.
       'include_discard_iter0_remnants_for_axis_range' (bool):
           Include remnants from iteration 0 in axis range calculation (default: False).
           With default settings, the lower panel displays the previous accepted but later discarded
           points.
           This option allows excluding these remnants from iteration 0 in the axis range calculation.
           This is useful as these remnants from iteration 0 often have a very different distribution than
           the other iterations.
       'combine_iteration_0_and_1' (bool):
           Combine iterations 0 and 1 into one subplot (default: False).
           This can be useful if you don't want a dedicated subplot for iteration 0.
       'data_types_to_plot' (dict):
           Specify which data types to include. Use a dict with keys 'upper_panel' and 'lower_panel'.
           This is useful if you want to exclude some data types from the plot to reduce clutter and
           focus on specific data types.
           The data types are as follows:
               'accepted_new'            # New points that were accepted in the iteration.
               'accepted_old'            # Points from the previous iteration that were not discarded by the likelihood filter.
               'discarded_likelihood_new'# New points from the MCMC chains that were discarded by the likelihood filter.
               'discarded_likelihood_old'# Points from the previous iteration that were discarded by the likelihood filter.
               'discarded_iteration'     # Discards if the entire iteration was discarded.
               'discarded_oversampling'  # Discards by the oversampling-filter.
               'failed_class'            # Points that failed the classification. Only from iteration > 0.
               'best_fit'                # The best fit point from the iteration. Is based on the lowest -log(lkl)
                                         # value of all accumulated accepted points.
               'accumulated_accepted_still'# The accumulated state of accepted points that are still in the training data.
               'accumulated_discarded_old'# The accumulated state of discarded points that once were accepted in previous
                                          # iterations but were discarded later.
               'accumulated_discarded_new'# The points that were accepted in the previous iteration but were discarded in the current iteration.
           To exclude a data type from the plot, set the value to False.
           The dictionary should be structured as follows:
               {
                 'upper_panel': {
                     'accepted_new': True,
                     'discarded_likelihood_new': True,
                     'discarded_likelihood_old': True,
                     'discarded_iteration': True,
                     'discarded_oversampling': True,
                     'failed_class': True,
                     'best_fit': True
                 },
                 'lower_panel': {
                     'accumulated_accepted_still': True,
                     'accumulated_discarded_old': True,
                     'accumulated_discarded_new': True,
                     'best_fit': True
                 }
               }

    **Marker & Color Styling Options**:
       'colormap' (str):
           Name of the colormap for iteration colors (default: 'tab10').
       'marker_size' (int):
           Size of markers (default: 8).
       'marker_edge_width' (float):
           Marker edge width (default: 0.1).
       'alpha_accepted' (float):
           Alpha transparency for accepted points (default: 1).
       'alpha_discarded' (float):
           Alpha transparency for discarded points (default: 1).
           Can conveniently be set to 0 to hide all discarded points.
           (Note: This does not affect the super legend.)
       'legend_labels' (dict):
           Custom label names for the legend (key: data type, value: legend label).
           Example: {'accepted_new': 'Accepted', 'discarded_likelihood': 'Discarded (Likelihood)', ...}
           All keys include: 'accepted_new', 'accepted_old', 'discarded_iteration',
                             'discarded_oversampling', 'discarded_likelihood_new',
                             'discarded_likelihood_old', 'failed_class', 'best_fit',
                             'accumulated_accepted_still', 'accumulated_discarded_old',
                             'accumulated_discarded_new'.
           (Note: To avoid duplicate legends, only one of 'accepted_new' or
           'accumulated_accepted_still' is shown in the legend.)
       'marker_styles' (dict):
           Custom marker styles for different data types.
           Example:
           {
               'accepted_new': {
                   'marker': 'o',
                   'fillstyle': 'full',
                   'color': 'iter_color',
                   'edgecolor': 'black',
                   'linewidth': self.marker_edge_width,
                   'size': self.marker_size,
                   'alpha': self.alpha_accepted
               },
               'accepted_old': ...
           }
           All NECESSARY keys: 'accepted_new', 'accepted_old', 'discarded_iteration',
                               'discarded_oversampling', 'discarded_likelihood_new',
                               'discarded_likelihood_old', 'failed_class', 'best_fit',
                               'accumulated_accepted_still', 'accumulated_discarded_old',
                               'accumulated_discarded_new'.

    **Contour Options** (only effective if 'plot_contours' is True):
       The contours are drawn based on the Δχ² values for each iteration. They are calculated
       using all samples' -log(lkl) values and the given iteration's best fit point.
       This is only relevant if the likelihood filter is used; otherwise, contours are not plotted.
       Since the samples are usually not well distributed but clustered, the contours are
       interpolated on a grid, which may be imperfect in sparse regions.
       There are methods to improve the contours by providing external data via the
       'save_x_y_loglkl' and 'pickle_path' options.
       The contour plot supports both a filled contour in the background and an overlay
       of contour lines.
       Available options:
          'plot_contours' (bool):
              Whether to display Δχ² contours (default: True).
              Useful to quickly enable/disable the contours for aesthetic purposes.
          'cmap_contour' (str):
              Colormap name for contours (default: 'viridis').
          'sigma' (int):
              Gaussian smoothing sigma for contours (default: 12).
              Higher values produce smoother contours but may reduce accuracy.
          'grid_size' (int):
              Grid resolution for contour interpolation (default: 1000).
              This is effectively the number of grid cells in each direction.
          'interpolation_method' (str):
              Interpolation method for scipy.interpolate.griddata
              ('linear', 'nearest', or 'cubic'; default: 'cubic').
          'cmap_norm_type' (str):
              Normalization type for the contour colormap; options: 'PercentileNorm' or 'PowerNorm'
              (default: 'PercentileNorm').
              'PowerNorm' applies a power-law scaling; 'PercentileNorm' sets levels at percentiles
              (each percentile is 100/n_discretize_cmap).
          'n_discretize_cmap' (int):
              Number of discrete contour levels if using PercentileNorm (default: 20).
          'gamma_powernorm' (float):
              Gamma for PowerNorm normalization (default: 0.22).
              (This option is less frequently used.)
          'contour_levels' (int):
              Number of contour levels used with the PowerNorm colormap (default: 21).
          'alpha_contourf' (float):
              Alpha transparency for filled contours (default: 1).
              Useful to disable the filled contours while still showing the threshold line.
          'contour_line_overlay' (bool):
              Whether to overlay contour lines on top of the filled contours (default: False).
              Useful for revealing the true distribution of the data.
          'alpha_contour' (float):
              Alpha transparency for contour lines (default: 1).
          'contour_linewidths' (float):
              Line width for the contour overlay lines (default: 0.5).
          'plot_threshold_line' (bool):
              Whether to plot the threshold line (default: False).
          'delta_chi2_threshold_color' (str):
              Color of the threshold line (default: 'purple').
          'delta_chi2_threshold_linewidth' (float):
              Line width of the threshold line (default: 2).

    **Saving Options**:
       'save_x_y_loglkl' (bool):
           If True, save the (x, y, true_loglkl) data as a pickle file (default: False).
           It saves the x, y, true_loglkl for all iterations and all data types in a single dataframe.
       'pickle_path' (str):
           Path to a pickle file to load extra contour data from (e.g., "path/to/pickle_file").
           Example use cases:
              - Gain extra contour data for sparse regions by loading data from a previous run
                of the same cosmological model.
              - Use LHS sampling to achieve better parameter space coverage; save the contour data,
                then load it in a subsequent run with a different sampling method.
           Note: This only affects the contour calculations; the scatter plot data always comes from
           the current run.

    **Example**:
       --plot_iter_args "{
           'x_range': [0, 10],
           'y_range': [0, 10],
           'max_subplots': 5,
           'plot_contours': True,
           'colormap': 'tab20',
           'alpha_accepted': 0.8,
           'contour_levels': 21
       }"



triangleplot_args:
--------------------------------------------------------------------------------------------------

--triangleplot_args (dict):
    A dictionary of arguments to pass to the TrianglePlot class for customizing the pairplot.

    General Plot Options:
      'iterations_to_plot' (list or 'all'):
          Iteration numbers to include in the plot (e.g. [0,1,2]; default: 'all').
          The plot will show the snapshot of the status at the final iteration included in the list.
      'params_triangleplot' (list or 'all'):
          Parameter names to include in the triangle plot (default: 'all').
          Example: ['H0', 'omega_b', 'omega_cdm'].
          This parameter is the same as the 'triangle_plot_cosmoparams' argument. So you can use either one.
      'colormap' (str):
          Name of the colormap for iteration colors in the scatter plot (default: 'tab10').

    Axis & Data Filtering Options:
      'ignore_iteration_0_for_axis' (bool):
          Whether to exclude iteration 0 when computing axis limits (default: False).
      'include_lkldiscard_new_for_axis_range' (bool):
          Include 'discarded_likelihood_new' points when determining axis ranges (default: True).
      'data_types_to_plot' (dict):
          Specify which data types to include in the triangle plot.
          Set the value to False to exclude a data type.
            The dictionary should be structured as follows:
              { 'accepted_new': True, 'discarded_likelihood_new': True, 
                'discarded_likelihood_old': True, 'discarded_iteration': True }
            The data types are as follows:
                'accepted_new'            # New points that were accepted in the iteration.
                'discarded_likelihood_new'# New points from the MCMC chains that were discarded by the likelihood filter.
                'discarded_likelihood_old'# Points from the previous iteration that were discarded by the likelihood filter.
                'discarded_iteration'     # Discards if the entire iteration was discarded.

    Marker & Styling Options:
      'marker_size' (int):
          Marker size for scatter points in the pairplot (default: 2).
      'marker_alpha' (float):
          Marker transparency (default: 1).
      'marker_edge_lw' (float):
          Marker edge linewidth (default: 0.1).

    Contour Options (only effective if 'plot_contours' is True):
      'plot_contours' (bool):
          Whether to plot filled Δχ² contours on each 2D panel (default: False).
      'grid_size' (int):
          Grid resolution for contour interpolation (default: 1000).
      'sigma' (float):
          Gaussian smoothing sigma for contours (default: 10).
      'interpolation_method' (str):
          Interpolation method ('linear', 'nearest', 'cubic'; default: 'linear').
      'n_discretize_cmap' (int):
          Number of levels for the contourmap (default: 20).
      'cmap_contour' (str):
          Colormap for contour lines and filled contours (default: 'viridis').
      'alpha_contourf' (float):
          Alpha transparency for filled contours (default: 1).
      'contour_line_overlay' (bool):
          Whether to overlay contour lines (default: False).
      'alpha_contour' (float):
          Alpha transparency for contour lines (default: 0.8).
      'contour_linewidths' (float):
          Line width for contour lines (default: 0.4).
      'plot_threshold_line' (bool):
          Whether to plot the threshold line (default: True).
      'delta_chi2_threshold_color' (str):
          Color of the threshold line (default: 'purple').
      'delta_chi2_threshold_linewidth' (float):
          Line width of the threshold line (default: 2).

    Downsampling & Saving Options:
      'downsampling_fraction' (float):
          Fraction of points to randomly downsample for plotting (default: 0).
          Note: Be careful with downsampling, as it can lead to misleading plots.
      'save_params_loglkls' (bool):
          Whether to save the parameters and -log(likelihood) to a pickle file for future use (default: False).
      'pickle_path' (str):
          Path to a pickle file to load extra contour data from.
          Can be used to provide extra contour data, e.g., from a previous run.

    Example:
      --triangleplot_args "{
          'iterations_to_plot': [0, 1, 2],
          'params_triangleplot': ['H0', 'omega_b', 'omega_cdm'],
          'plot_contours': True,
          'sigma': 8,
          'marker_size': 3
      }"

"""

# Standard Library Imports
import argparse
import sys
import os
import json
import re
import math
import warnings
import difflib as dl
from collections import defaultdict
from importlib.machinery import SourceFileLoader
import ast
import traceback

# Scientific Libraries
import numpy as np
import pandas as pd
import scipy
from scipy.stats import gaussian_kde
from scipy.interpolate import griddata
from scipy.ndimage import gaussian_filter

# Matplotlib Imports (Grouped)
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.ticker as ticker
import matplotlib.text as mtext
import matplotlib.colors as mcolors
from matplotlib import colors
from matplotlib.colors import PowerNorm, TwoSlopeNorm, Normalize, ListedColormap
from matplotlib.ticker import FuncFormatter, MaxNLocator
from matplotlib.lines import Line2D
import matplotlib.lines as mlines
from matplotlib.legend_handler import HandlerBase
import matplotlib.patches as mpatches
from matplotlib.patches import FancyBboxPatch
from matplotlib.font_manager import FontProperties
from matplotlib.transforms import Bbox

# Seaborn Imports (Grouped)
import seaborn as sns
from seaborn.distributions import _DistributionPlotter
from seaborn._stats.counting import Hist

# Matplotlib Toolkits (Grouped)
from mpl_toolkits.axes_grid1 import make_axes_locatable
from mpl_toolkits.axes_grid1.inset_locator import inset_axes


# Other Utility Imports
from tabulate import tabulate
import pickle


# ---------------------------------CLASS MAIN---------------------------------
# Main class that parses the arguments and runs the desired analysis and plotting.
class Main:
    def __init__(self):
        self.verbose = 1  # Default verbosity level

        # Setup argparse with adjusted arguments and detailed help texts.
        parser = argparse.ArgumentParser(
            description=(
                """
                Analyze and plot the training data from CONNECT iterations.
                This script automatically loads training data from the specified project folder, 
                then runs various analyses and plotting routines:  
                
                - Univariate analysis (via Analyze_likelihoods): 
                    generates histograms, box plots, etc, of the Δχ² and -log(lkl) values.  
                 
                - Triangle plot (via TrianglePlot): 
                    generates a 2D pairplot (corner plot) of parameters  
                    from the final iteration snapshot (or selected iterations), with optional Δχ² contours.  
                    
                - Plot iterations (via PlotIterations): 
                    creates a detailed two-panel multi-column figure showing the iterative process.  
                    The upper panel shows new points generated and discarded in each iteration, while the lower panel 
                    shows the accumulated state of the training data.  
                    Optional contour can be added to highlight regions of interest based on Δχ² values.  
                    
                Note: This script can be used both when CONNECT is run in with likelihood filtering and without it.   
                
                The script can be RUN with the default settings by this minimum working example:
                  
                python plot_iterations.py --project_path \"/path/to/connect/data/lcdm_example\" 
                
                Be aware that this will run all analyses and generate plots with default settings.  
                It will try to create a triangle plot for all parameters which is very memory (RAM) intensive.  
                It will also use the default parameter pairs for the iteration plots; ('H0', 'omega_b').   
            
                A slightly more detailed run could include the following options:  
                
                python plot_iterations.py --project_path \"/path/to/connect/data/lcdm_example\" --output_path \"/path/to/save/plots_and_analysis\" \\ 
                --run_univariate_analysis \"True\" --run_triangle_plot \"True\" --run_plot_iterations \"True\"  \\
                --triangle_plot_cosmoparams \"['H0', 'omega_b', 'tau_reio', 'omega_cdm']\" --plot_iter_cosmoparams \"('H0', 'omega_b')\"  \\
                --analyze_lkl_args \"{'include_inset': True, 'x_range': [0, 1000]}\"  \\
                --plot_iter_args \"{'ignore_iteration_0_for_axis' : True, 'include_discard_iter0_remnants_for_axis_range' : False, 'plot_contours': True,
                'sigma': 12, 'plot_threshold_line': True}\"  \\
                --plot_triangle_args \"{'ignore_iteration_0_for_axis' : False, 'plot_contours': True, 'sigma': 12, 'plot_threshold_line': True}\"  
                    
                    As default the axis ranges are calculated automatically, but you can override them with the 'x_range' and 'y_range' arguments.  
                    This is only a small subset of the available options. For a full list of options, 
                    see the help text below or read the docstring of the script.   
            
                If run with default settings it will output:  
                - Univariate analysis reports in the 'likelihood_analysis' subfolder. This includes histograms, box plots, etc.  
                - Iteration plots in the 'iteration_plots' subfolder. This includes detailed two-panel scatter plots.  
                - Triangle plots in the 'pairplots' subfolder. This includes pairplots (corner plots) of parameters.   
            
        
                Note: Since argparse cannot directly accept Python objects (lists, dictionaries, booleans, etc.),  
                all such arguments must be provided as strings (e.g., \"['H0', 'omega_b']\") and are parsed "
                "using ast.literal_eval method.\n"""
            ),
            formatter_class=argparse.RawTextHelpFormatter,
        )

        # Required arguments
        parser.add_argument(
            "--project_path",
            required=True,
            type=str,
            help=(
                "**REQUIRED**: Path to the CONNECT project folder containing the data.\n"
                "The folder should include all iteration subfolders.\n"
                'Example: --project_path "/path/to/connect/data/lcdm_example"'
            ),
        )

        parser.add_argument(
            "--CONNECT_path",
            required=False,
            type=str,
            default=None,
            help=(
                "**OPTIONAL**: Path to the CONNECT installation folder.\n"
                "If not provided, the script will try to find the CONNECT installation folder\n"
                "and source code based on the project_path, by assuming the folder structure:\n"
                "'CONNECT_path/data/project_path' or 'CONNECT_path/data/subfolder/project_path'.\n"
                "If your connect directory is not named 'connect' as default, you must provide the path to the connect directory.\n"
                "Example: --CONNECT_path '/path/to/connect/'"
            ),
        )

        # Other general arguments
        parser.add_argument(
            "--output_path",
            type=str,
            default=None,
            required=False,
            help=(
                "**OPTIONAL**: Folder where the generated analysis and plots will be saved.\n"
                "If not provided, defaults to 'project_path/iteration_analysis'.\n"
                "Example: --output_path '/path/to/save/plots_and_analysis'"
            ),
        )
        parser.add_argument(
            "--verbose",
            type=int,
            default=1,
            required=False,
            choices=range(0, 6),
            metavar="[0-5]",
            help=(
                "**OPTIONAL**: Set the verbosity level of output messages. Higher values produce more detailed output.\n"
                "Default is 1. (0 = quiet, 3 = detailed, 5 = maximum output)"
            ),
        )

        # Boolean flags (parsed as Python literals)
        parser.add_argument(
            "--run_univariate_analysis",
            type=self.parse_python_literal,
            default="True",
            required=False,
            help=(
                "**OPTIONAL**: Boolean flag to run univariate analysis (Analyze_likelihoods).\n"
                "If True, the script will generate histograms, box plots, and summary statistics\n"
                "for accepted and discarded likelihood data.\n"
                'Example: --run_univariate_analysis "True"'
            ),
        )
        parser.add_argument(
            "--run_triangle_plot",
            type=self.parse_python_literal,
            default="True",
            required=False,
            help=(
                "**OPTIONAL**: Boolean flag to run the triangle plot (TrianglePlot).\n"
                "If True, the script will generate a pairplot (corner plot) of the final iteration's\n"
                "accepted vs. discarded points for selected (or all) parameters.\n"
                'Example: --run_triangle_plot "True"'
            ),
        )

        parser.add_argument(
            "--run_grid_plot",
            type=self.parse_python_literal,
            default="True",
            required=False,
            help=(
                "**OPTIONAL**: Boolean flag to run the grid plot (GridPlot).\n"
                "It is the same as the triangle plot, but only plots selected parameter pairs (rows and columns specified).\n"
                "If True, the script will generate a grid plot of the final iteration's\n"
                "accepted vs. discarded points for selected (or all) parameters.\n"
                'Example: --run_grid_plot "True"'
            ),
        )

        parser.add_argument(
            "--run_plot_iterations",
            type=self.parse_python_literal,
            default="True",
            required=False,
            help=(
                "**OPTIONAL**: Boolean flag to run the detailed iteration plot (PlotIterations).\n"
                "If True, a two-panel scatter plot is generated for each selected parameter pair,\n"
                "showing both new and accumulated points (with optional contour, but with contour as default).\n"
                'Example: --run_plot_iterations "True"'
            ),
        )

        # List arguments (passed as string literals)
        parser.add_argument(
            "--triangle_plot_cosmoparams",
            type=self.parse_python_literal,
            default="all",
            required=False,
            help=(
                "**RECOMMENDED**: A list of cosmological parameter names to include in the TrianglePlot.\n"
                "Use a Python list as a string. To plot all parameters, set to 'all'.\n"
                "Warning: Plotting all parameters can be memory-intensive.\n"
                "Example: --triangle_plot_cosmoparams \"['H0', 'omega_b', 'tau_reio', 'omega_cdm']\""
            ),
        )

        parser.add_argument(
            "--grid_plot_cosmoparams",
            type=self.parse_python_literal,
            required=False,
            help=(
                "**RECOMMENDED**: A dictionary specifying the parameters to plot in the grid plot.\n"
                "The dictionary should have the following structure:\n"
                "   {'rows': ['param1', 'param2'], 'columns': ['param3', 'param4']}\n"
                "   Default: {'rows': ['H0', 'omega_b'], 'columns': ['H0', 'omega_b', 'omega_cdm', 'YHe']}\n"
                "   Example: --grid_plot_cosmoparams \"{'rows': ['H0', 'omega_b'], 'columns': ['tau_reio', 'omega_cdm']}\""
            ),
        )

        parser.add_argument(
            "--plot_iter_cosmoparams",
            type=self.parse_python_literal,
            default=("H0", "omega_b"),
            required=False,
            help=(
                "**RECOMMENDED**: A pair or a list of parameter pairs to plot in the PlotIterations class.\n"
                "Each pair should be provided as a tuple or list with tuples.\n"
                "Can also be set to 'all' to plot all parameter pairs. But be cautious with this option as it will generate a lot of plots.\n"
                "Example: --plot_iter_cosmoparams \"[('H0', 'omega_b'), ('H0', 'tau_reio')]\""
            ),
        )

        # Dictionary arguments for customization of labels and styling.
        parser.add_argument(
            "--latex_labels",
            type=self.parse_python_literal,
            default={},
            required=False,
            help=(
                "A dictionary of parameter names mapped to their LaTeX-formatted labels.\n"
                "These labels are used to customize the axis labels in plots.\n"
                "It can be used to either replace the default latex labels for selected parameters\n"
                "or to add latex labels for new parameters not present in the default labels.\n"
                "Example: --latex_labels \"{'H0': r'$H_0$', 'omega_b': r'$\\Omega_b$'}\""
            ),
        )
        parser.add_argument(
            "--dataloader_args",
            type=self.parse_python_literal,
            default={},
            required=False,
            help=(
                "A dictionary of arguments to pass to the Dataloader class.\n"
                "Supported keys include:\n"
                "  'save_pickle_data' (bool): Save loaded data as a pickle file.\n"
                "  'pickle_path' (str): Path to a pickle file to load data from (skips file loading).\n"
                "Can be used if you want to save the loaded data for faster subsequent runs \nor if you want to work with the data outside of this script.\n"
                "Example: --dataloader_args \"{'save_pickle_data': True, 'pickle_path': '/path/to/pickle_file'}\""
            ),
        )

        parser.add_argument(
            "--analyze_lkl_args",
            type=self.parse_python_literal,
            default={},
            required=False,
            help=(
                "**OPTIONAL**: A dictionary of additional arguments to pass to the Analyze_likelihoods class.\n"
                "This can be used to customize the plots for the univariate analysis by overriding default settings.\n"
                "Examples could be to override the axis ranges, what data categories to plot, or to exclude the inset plot.\n\n"
                "Supported keys for the analyze_lkl_args dict include:\n"
                "  'create_these_outputs' (dict): Specify which outputs to create (default: all).\n"
                "       Example: {'histograms': True, 'boxplots': True, 'summary_tables': True, 'evolution_plot': True} \n"
                "  'max_delta_chi2_in_dataset' (float): Maximum Δχ² value to include in the dataset for the plots. (default: None)\n"
                "  'max_loglkl_in_dataset' (float): Maximum -log(lkl) value to include in the dataset for the plots. (default: None)\n"
                "  'x_range_hist' (list): Explicit x-axis range to display for histogram, e.g., [0, 10].\n"
                "  'y_range_hist' (list): Explicit y-axis range to display for histogram, e.g., [0, 10].\n"
                "  'x_range_inset_hist' (list): x-axis range for the inset in histogram.\n"
                "  'y_range_inset_hist' (list): y-axis range for the inset in histogram.\n"
                "  'y_range_box' (list): Explicit y-axis range to display for box plot, e.g., [0, 10].\n"
                "  'save_formats' (list): List of output file formats (e.g., ['png', 'pdf']).\n"
                "  'include_inset' (bool): Include an inset plot in the analysis (default True).\n"
                "  'data_categories' (list): List of data categories to include in the analysis.\n"
                "       Refer to the end of the docstring guide for the full list of supported data categories and their descriptions.\n"
                "  'hist_main_panel' (list): List of dicts defining histogram settings for the main panel.\n"
                "       This argument can be used to specify which data categories to plot in the\n"
                "       main panel of the histogram plot. Refer to the full docstring guide for the required keys.\n"
                "  'hist_inset' (list): List of dicts defining histogram settings for the inset panel.\n"
                "  'category_labels' (dict): Custom labels for the data categories (if None, default labels are used).\n"
                "  'data_colors' (dict): Custom colors for the data categories.\n\n"
                "See the end of the docstring guide for a complete description of the supported keys and how to use them.\n\n"
                "Example usage:\n"
                "    --analyze_lkl_args \"{'include_inset': True, 'x_range': [0, 10], 'save_formats': ['png']}\"\n"
            ),
        )

        parser.add_argument(
            "--plot_iter_args",
            type=self.parse_python_literal,
            default={},
            required=False,
            help=(
                "**OPTIONAL**: A dictionary of additional arguments to pass to the PlotIterations class.\n "
                "This allows customization of iteration plots by overriding default settings.\n\n"
                "**General Plot Options**:\n"
                "   'params_plot_iter' (list or 'all'): Parameter pairs to plot (default: ('H0','omega_n')).\n"
                "       Example: [('H0', 'omega_b'), ('H0', 'tau_reio')].\n"
                "       (The same as 'plot_iter_cosmoparams'. You can use either)\n"
                "   'iterations_to_plot' (list or 'all'): Specific iterations to plot (default: 'all').\n"
                "   'max_subplots' (int): Maximum number of subplot columns (default: 5); extra iterations are grouped.\n"
                "   'fig_width' (str): Figure width in format 'width unit' (default: '440 pts').\n"
                "       possible units: 'pts', 'cm', 'in'. i.e. '440 pts', '10 cm', '5 in'.\n"
                "   'save_formats' (list): Output file formats (e.g., ['png', 'pdf']).\n"
                "   'iteration0_figure_inset' (bool): Insert an inset for iteration 0 data in the first column (default: False).\n"
                "   'show_super_legend' (bool): Display a super legend (default: True).\n\n"
                "**Axis & Data Filtering Options**:\n"
                "   'x_range' (list): Override the shared x-axis range, e.g., [0, 10].\n"
                "   'y_range' (list): Override the shared y-axis range, e.g., [0, 10].\n"
                "       (If omitted, ranges are calculated automatically.)\n"
                "   'log_x' (bool): Use logarithmic scale for x-axis (default: False).\n"
                "   'log_y' (bool): Use logarithmic scale for y-axis (default: False).\n"
                "   'ignore_iteration_0_for_axis' (bool): Exclude iteration 0 when computing axis ranges (default: True).\n"
                "   'include_lkldiscard_new_for_axis_range' (bool): Include 'discarded_likelihood_new' for axis range calculation (default: True).\n"
                "   'include_discard_iter0_remnants_for_axis_range' (bool): Include remnants from iteration 0 (default: False).\n"
                "   'combine_iteration_0_and_1' (bool): Combine iterations 0 and 1 into one subplot (default: False).\n"
                "   'exclude_iter0_discard_clutter_lower_panels' (bool): Exclude clutter from discards originating from iteration 0 'accepted' in lower panels (default: True).\n"
                "   'data_types_to_plot' (dict): Specify which data types to include (use keys 'upper_panel' and 'lower_panel').\n"
                "       (See full docstring guide for a list of valid keys and defaults.)\n\n"
                "**Marker & Color & Legend Styling Options**:\n"
                "   'colormap' (str): Colormap for iteration colors (default: 'tab10').\n"
                "   'marker_size' (int): Size of markers (default: 8).\n"
                "   'marker_edge_width' (float): Marker edge width (default: 0.1).\n"
                "   'alpha_accepted' (float): Opacity for accepted points (default: 1).\n"
                "   'alpha_discarded' (float): Opacity for discarded points (default: 1).\n"
                "   'legend_labels' (dict): Custom legend labels (default labels are used if not provided).\n"
                "   'marker_styles' (dict): Custom marker styles for each data type (see docstring guide for required keys).\n"
                "   'show_counts_in_legend' (bool): Show sample counts in the subplot-legend for each data type (default: True).\n"
                "   'use_bold_subplot_legend' (bool): Use bold font for subplot legends (readable on top of points) (default: 'auto').\n"
                "   'subplot_legend_location' (str): Location of subplot legend (default: 'upper left').\n"
                "   'tall_subplots' (bool): Make each panel taller (non-square) to fit subplot-legends without covering data. (default: True).\n\n"
                "**Contour Options** (only if 'plot_contours' is True):\n"
                "   'plot_contours' (bool): Display Δχ² contours (default: False).\n"
                "   'cmap_contour' (str): Colormap for contours (default: 'viridis').\n"
                "   'sigma' (int): Gaussian smoothing sigma for contours (default: 12).\n"
                "   'grid_size' (int): Grid resolution for contour interpolation (default: 1000).\n"
                "   'interpolation_method' (str): Interpolation method ('linear', 'nearest', or 'cubic'; default: 'cubic').\n"
                "   'cmap_norm_type' (str): Normalization type for contour colormap; options: \n'PercentileNorm' or 'PowerNorm' (default: 'PercentileNorm').\n"
                "   'n_discretize_cmap' (int): Number of discrete contour levels (default: 20).\n"
                "   'gamma_powernorm' (float): Gamma for PowerNorm normalization (default: 0.22).\n"
                "   'contour_levels' (int): Number of contour levels (default: 21).\n"
                "   'alpha_contourf' (float): Opacity for filled contours (default: 1).\n"
                "   'contour_line_overlay' (bool): Overlay contour lines (default: False).\n"
                "   'alpha_contour' (float): Opacity for contour lines (default: 0.8).\n"
                "   'contour_linewidths' (float): Line width for contour lines (default: 0.4).\n"
                "   'plot_threshold_line' (bool): Plot the threshold line (default: False).\n"
                "   'delta_chi2_threshold_color' (str): Color of the threshold line (default: 'purple').\n"
                "   'delta_chi2_threshold_linewidth' (float): Line width of the threshold line (default: 2).\n\n"
                "**Downsampling & Saving Options**:\n"
                "   'downsampling_fraction' (float): Fraction of points to downsample (default: 0).\n"
                "   'save_x_y_loglkl' (bool): Save (x, y, true_loglkl) data as a pickle file (default: False).\n"
                "   'pickle_contour_data' (str): Path to a pickle file to load extra contour data (default: None).\n\n"
                "For detailed explanations and use-case examples for each key, please refer to the end of the full docstring guide.\n\n"
                "Example:\n"
                "  --plot_iter_args \"{'x_range': [0, 10], 'y_range': [0, 10], 'max_subplots': 5, 'plot_contours': True, \n"
                "'colormap': 'tab20', 'alpha_accepted': 0.8, 'contour_levels': 21}\""
            ),
        )

        parser.add_argument(
            "--triangleplot_args",
            type=self.parse_python_literal,
            default={},
            required=False,
            help=(
                "**OPTIONAL** : A dictionary of arguments to pass to the TrianglePlot class for customizing the triangle pairplot and grid pairplot.\n\n"
                "General Plot Options:\n"
                "  'iterations_to_plot' (list or 'all'): Iteration numbers to include in the plot (eg. [0,1,2]. default: 'all').\n"
                "  'params_triangleplot' (list or 'all'): Parameter names to include in the triangle plot (default: 'all').\n"
                "       Example: ['H0', 'omega_b', 'omega_cdm'].\n"
                "       This parameter is the same as the 'triangle_plot_cosmoparams' argument. So you can use either one.\n"
                "   'grid_vars' (dict): Dictionary specifying grid parameters for the grid plot.\n"
                "       Example: {'rows': ['H0', 'omega_b'], 'columns': ['tau_reio', 'omega_cdm']}\n"
                "       (This is the same as 'grid_plot_cosmoparams'. You can use either)\n"
                "  'colormap' (str): Name of the colormap for iteration colors in the scatter plot (default: 'tab10').\n\n"
                "Axis & Data Filtering Options:\n"
                "  'only_accepted' (bool): Whether to only plot accepted points (default: False).\n"
                "  'ignore_iteration_0_for_axis' (bool): Whether to exclude iteration 0 when computing axis limits (default: False).\n"
                "  'include_lkldiscard_new_for_axis_range' (bool): Include 'discarded_likelihood_new' points\n when determining axis ranges (default: True).\n"
                "  'data_types_to_plot' (dict): Specify which data types to include in the triangle plot.\n"
                "      Example:\n"
                "      { 'accepted_new': True, 'discarded_likelihood_new': True, 'discarded_likelihood_old': True, \n"
                "        'discarded_iteration': True }\n"
                "       See full docstring guide for descriptions of the data types.\n"
                "  'custom_axis_ranges' (dict): Override axis ranges for specified parameters in the triangle plot.\n"
                "       Example: {'H0': (60, 80), 'omega_b': (0.02, 0.05)}\n"
                "  'custom_ticks' (dict): Custom ticks for specified parameters in the triangle plot.\n"
                "       Example: {'H0': [60, 70, 80], 'omega_b': [0.02, 0.04]}\n"
                "  'log_scale_params' (list): List of parameters to apply logarithmic scaling to.\n"
                "       Example: ['H0', 'omega_b']\n\n"
                "Marker & Styling Options:\n"
                "  'preferred_legend_position' (str): Preferred position for the legend (default: 'upper right').\n"
                "       Choices: 'upper right' (right top corner if space allows), 'above' (above triangle plot, but centered),\n"
                "  'marker_size' (int): Marker size for scatter points in the pairplot (default: 2).\n"
                "  'marker_alpha' (float): Marker transparency (default: 1).\n"
                "  'marker_edge_lw' (float): Marker edge linewidth (default: 0.1).\n\n"
                "Contour Options (only effective if 'plot_contours' is True):\n"
                "  'plot_contours' (bool): Whether to plot filled Δχ² contours on each 2D panel (default: False).\n"
                "  'grid_size' (int): Grid resolution for contour interpolation (default: 1000).\n"
                "  'sigma' (float): Gaussian smoothing sigma for contours (default: 10).\n"
                "  'interpolation_method' (str): Interpolation method ('linear', 'nearest', 'cubic'; default: 'linear').\n"
                "  'n_discretize_cmap' (int): Number of levels for the contourmap (default: 20).\n"
                "  'cmap_contour' (str): Colormap for contour lines and filled contours (default: 'viridis').\n"
                "  'alpha_contourf' (float): Alpha transparency for filled contours (default: 1).\n"
                "  'contour_line_overlay' (bool): Whether to overlay contour lines (default: False).\n"
                "  'alpha_contour' (float): Alpha transparency for contour lines (default: 0.8).\n"
                "  'contour_linewidths' (float): Line width for contour lines (default: 0.4).\n"
                "   'plot_threshold_line' (bool): Whether to plot the threshold line (default: False).\n"
                "   'delta_chi2_threshold_color' (str): Color of the threshold line (default: 'purple').\n"
                "   'delta_chi2_threshold_linewidth' (float): Line width of the threshold line (default: 2).\n\n"
                "Downsampling & Saving Options:\n"
                "  'downsampling_fraction' (float): Fraction of points to randomly downsample for plotting (default: 0).\n"
                "      Note: Be careful with downsampling, as it can lead to misleading plots.\n\n"
                "  'save_params_loglkls' (bool): Whether to save the parameters and -log(likelihood) to a pickle file for future use (default: False).\n"
                "  'pickle_path' (str): Path to a pickle file to load contour data from. \nCan be used to provide extra contour data, e.g., from a previous run.\n\n"
                "See the full docstring guide for detailed explanations and use-case examples for each key.\n\n"
                "Example:\n"
                "  --triangleplot_args \"{'iterations_to_plot': [0, 1, 2], 'params_triangleplot': ['H0', 'omega_b', 'omega_cdm'], 'plot_contours': True, \n"
                "                         'sigma': 8, 'marker_size': 3}\""
            ),
        )

        self.args = parser.parse_args()

        # Use verbosity from command-line argument
        self.verbose = self.args.verbose

        # Validate that the dictionary arguments are indeed dicts
        for arg_name in ["analyze_lkl_args", "plot_iter_args", "triangleplot_args"]:
            value = getattr(self.args, arg_name)
            if not isinstance(value, dict):
                raise ValueError(f"{arg_name} must be a dictionary.")

        # Set output_path to project_path/iteration_analysis if not provided
        self.output_path = self.args.output_path or os.path.join(
            self.args.project_path, "iteration_analysis"
        )
        os.makedirs(self.output_path, exist_ok=True)

        # Initialize param_connect from log file in project_path
        param_file = os.path.join(self.args.project_path, "log_connect.param")
        if not os.path.isfile(param_file):
            raise FileNotFoundError(
                f"Could not find log_connect.param file at {param_file}"
            )

        self.CONNECT_path = self.args.CONNECT_path
        if self.CONNECT_path is None:

            if os.path.isdir(
                os.path.join(os.path.dirname(self.args.project_path), "../../source/")
            ):
                self.CONNECT_path = os.path.join(
                    os.path.dirname(self.args.project_path), "../../"
                )
            elif os.path.isdir(
                os.path.join(
                    os.path.dirname(self.args.project_path), "../../../source/"
                )
            ):
                self.CONNECT_path = os.path.join(
                    os.path.dirname(self.args.project_path), "../../../"
                )
            else:
                raise FileNotFoundError(
                    f"Could not find CONNECT installation folder. Please provide the CONNECT_path argument."
                )

        default_module_path = os.path.join(
            self.CONNECT_path, "source/default_module.py"
        )
        if not os.path.isfile(default_module_path):
            raise FileNotFoundError(
                f"Could not find default_module.py file at {default_module_path}"
            )

        try:

            if os.path.isfile(default_module_path):
                self.param_connect = (
                    SourceFileLoader("Parameters", default_module_path)
                    .load_module()
                    .Parameters(param_file)
                )

            else:
                raise FileNotFoundError(
                    f"Could not find default_module.py file at {default_module_path}"
                )
            # Try to load the Parameters class from the source code located at project_path/../../source/default_module.py
            # and if it couldn't be loaded and initialized, it should raise a warning and default to the Parameters class from this file.
        except Exception as e:
            raise Exception(
                f"Error loading Parameters class from default_module.py: {e}"
            )

        # Default LaTeX labels for parameters
        self.default_latex_labels = {
            "omega_cdm": r"$\omega_{\mathrm{cdm}}$",
            "H0": r"$\mathrm{H}_{0}$",
            "omega_b": r"$\omega_{\mathrm{b}}$",
            "tau_reio": r"$\tau_{\mathrm{reio}}$",
            "n_s": r"$n_{\mathrm{s}}$",
            "ln10^{10}A_s": r"$\ln(10^{10}A_{\mathrm{s}})$",
            "z_reio": r"$z_{\mathrm{reio}}$",
            "Omega_Lambda": r"$\Omega_{\Lambda}$",
            "YHe": r"$Y_{\mathrm{He}}$",
            "A_s": r"$A_{\mathrm{s}}$",
            "sigma8": r"$\sigma_{8}$",
            "100*theta_s": r"$100\theta_{\mathrm{s}}$",
            "100theta_s": r"$100\theta_{\mathrm{s}}$",
            "omega_ini_dcdm": r"$\omega_{\mathrm{ini,cdm}}$",
            "Gamma_dcdm": r"$\Gamma_{\mathrm{dcdm}}$",
        }

        # Update LaTeX labels with any user-provided overrides
        if self.args.latex_labels:
            self.latex_labels = {**self.default_latex_labels, **self.args.latex_labels}
        else:
            self.latex_labels = self.default_latex_labels

    def run(self):
        # Load data via the Dataloader
        if self.verbose >= 1:
            print(
                "\033[1m-----------------------------LOADING TRAINING DATA FROM FILES -----------------------------\033[0m"
            )
        pickle_data_dataloader = None
        if "pickle_path" in self.args.dataloader_args:
            pickle_path = self.args.dataloader_args.get("pickle_path", None)
            if pickle_path and os.path.isfile(pickle_path):
                with open(pickle_path, "rb") as f:
                    pickle_data_dataloader = pickle.load(f)
                if self.verbose >= 2:
                    print(f"Loaded pickle data for Dataloader from {pickle_path}")
            else:
                if self.verbose >= 1:
                    print(
                        f"Pickle path {pickle_path} not found or not provided. Skipping pickle loading for Dataloader."
                    )

        try:
            dataloader_instance = Dataloader(
                project_path=self.args.project_path,
                CONNECT_path=self.CONNECT_path,
                param_connect=self.param_connect,
                verbose=self.verbose,
                save_pickle_data=self.args.dataloader_args.get(
                    "save_pickle_data", False
                ),
                pickle_data=pickle_data_dataloader,
            )
            data = dataloader_instance.run()
        except Exception as e:
            print(f"Error loading training data: {e}")
            print(traceback.format_exc())

        if self.verbose >= 1:
            print(
                "\033[1m-----------------------------LOADED TRAINING DATA SUCCESSFULLY -----------------------------\033[0m\n"
            )

        matplotlib.use("Agg")
        # Analyze likelihood data with Analyze_likelihoods

        if self.args.run_univariate_analysis:
            if self.verbose >= 1:
                print(
                    "\033[1m-----------------------------ANALYZING LIKELIHOOD DATA -----------------------------\033[0m\n"
                )

            try:
                likelihood_analyzer_instance = Analyze_likelihoods(
                    project_path=self.args.project_path,
                    output_path=self.output_path,
                    param_connect=self.param_connect,
                    data=data,
                    verbose=self.verbose,
                    include_inset=self.args.analyze_lkl_args.get("include_inset", True),
                    hist_main_panel=self.args.analyze_lkl_args.get(
                        "hist_main_panel", None
                    ),
                    hist_inset=self.args.analyze_lkl_args.get("hist_inset", None),
                    data_categories=self.args.analyze_lkl_args.get(
                        "data_categories", None
                    ),
                    category_labels=self.args.analyze_lkl_args.get(
                        "category_labels", None
                    ),
                    data_colors=self.args.analyze_lkl_args.get("data_colors", None),
                    x_range=self.args.analyze_lkl_args.get("x_range_hist", None),
                    y_range=self.args.analyze_lkl_args.get("y_range_hist", None),
                    x_range_inset=self.args.analyze_lkl_args.get(
                        "x_range_inset_hist", None
                    ),
                    y_range_inset=self.args.analyze_lkl_args.get(
                        "y_range_inset_hist", None
                    ),
                    y_range_box=self.args.analyze_lkl_args.get("y_range_box", None),
                    save_formats=self.args.analyze_lkl_args.get(
                        "save_formats", ["pdf"]
                    ),
                    max_delta_chi2_in_display=self.args.analyze_lkl_args.get(
                        "max_delta_chi2_in_dataset", None
                    ),
                    max_loglkl_in_display=self.args.analyze_lkl_args.get(
                        "max_loglkl_in_dataset", None
                    ),
                    create_these_outputs=self.args.analyze_lkl_args.get(
                        "create_these_outputs", "all"
                    ),
                )
                likelihood_analyzer_instance.run_analysis()
            except Exception as e:
                print(f"Error analyzing likelihood data: {e}")
                traceback.print_exc()
        else:
            if self.verbose >= 1:
                print(
                    f"run_univariate_analysis set to False. Skipping likelihood data analysis.\n"
                )
                print(
                    "\033[1m-----------------------------SKIPPING LIKELIHOOD DATA ANALYSIS -----------------------------\033[0m\n"
                )

        if self.verbose >= 1 and self.args.run_univariate_analysis:
            print(
                "\033[1m-----------------------------ANALYZED LIKELIHOOD DATA SUCCESSFULLY -----------------------------\033[0m\n"
            )

        # Plot detailed iteration data with PlotIterations

        if self.args.run_plot_iterations:

            if self.verbose >= 1:
                print(
                    "\033[1m-----------------------------PLOTTING ITERATIONS -----------------------------\033[0m"
                )
            pickle_data_plot_iter = None
            if "pickle_path" in self.args.plot_iter_args:
                pickle_path = self.args.plot_iter_args.get("pickle_path", None)
                if pickle_path and os.path.isfile(pickle_path):
                    with open(pickle_path, "rb") as f:
                        pickle_data_plot_iter = pickle.load(f)
                    if self.verbose >= 2:
                        print(
                            f"Loaded pickle data for PlotIterations from {pickle_path}"
                        )
                else:
                    if self.verbose >= 1:
                        print(
                            f"Pickle path {pickle_path} not found or not provided. Skipping pickle loading for PlotIterations."
                        )

            # Extract parameter pairs for PlotIterations

            if self.args.plot_iter_cosmoparams is not None:
                params = self.args.plot_iter_cosmoparams
            elif self.args.plot_iter_args.get("params_plot_iter", None) is not None:
                params = self.args.plot_iter_args.get("params_plot_iter", None)
            else:
                params = ("H0", "omega_b")

            if params == "all":
                params = data["iteration_data"][0]["accepted_accumulated"][
                    "parameters"
                ].columns.tolist()
                from itertools import combinations

                params = list(combinations(params, 2))
            elif isinstance(params, tuple) and len(params) == 2:
                params = [params]  # Convert single tuple into a list of one tuple
            elif isinstance(params, list):
                params = [
                    tuple(pair)
                    for pair in params
                    if isinstance(pair, (list, tuple)) and len(pair) == 2
                ]
            elif isinstance(params, (list, tuple)):
                params = [
                    tuple(pair)
                    for pair in params
                    if isinstance(pair, (list, tuple)) and len(pair) == 2
                ]
            else:
                raise ValueError(
                    "Invalid format for --plot_iter_cosmoparams. Must be 'all' or a list of pairs."
                )

            # Create PlotIterations for each parameter pair
            for param_x, param_y in params:
                print(f"Plotting iterations for {param_x} vs. {param_y}...")
                try:
                    plotter_instance = PlotIterations(
                        data=data,
                        param_x=param_x,
                        param_y=param_y,
                        param_connect=self.param_connect,
                        output_folder=self.output_path,
                        legend_labels=self.args.plot_iter_args.get(
                            "legend_labels", None
                        ),
                        marker_styles=self.args.plot_iter_args.get(
                            "marker_styles", None
                        ),
                        max_subplots=self.args.plot_iter_args.get("max_subplots", 5),
                        save_formats=self.args.plot_iter_args.get(
                            "save_formats", ["pdf"]
                        ),
                        marker_size=self.args.plot_iter_args.get("marker_size", 3),
                        marker_edge_width=self.args.plot_iter_args.get(
                            "marker_edge_width", 0.03
                        ),
                        combine_iteration_0_and_1=self.args.plot_iter_args.get(
                            "combine_iteration_0_and_1", False
                        ),
                        ignore_iteration_0_for_axis=self.args.plot_iter_args.get(
                            "ignore_iteration_0_for_axis", True
                        ),
                        colormap=self.args.plot_iter_args.get("colormap", "tab10"),
                        alpha_accepted=self.args.plot_iter_args.get(
                            "alpha_accepted", 1
                        ),
                        alpha_discarded=self.args.plot_iter_args.get(
                            "alpha_discarded", 1
                        ),
                        x_range=self.args.plot_iter_args.get("x_range", None),
                        y_range=self.args.plot_iter_args.get("y_range", None),
                        show_super_labels=self.args.plot_iter_args.get(
                            "show_super_labels", True
                        ),
                        verbose=self.verbose,
                        iterations_to_plot=self.args.plot_iter_args.get(
                            "iterations_to_plot", "all"
                        ),
                        draw_contours=self.args.plot_iter_args.get(
                            "plot_contours", False
                        ),
                        contour_levels=self.args.plot_iter_args.get(
                            "contour_levels", 21
                        ),
                        grid_size=self.args.plot_iter_args.get("grid_size", 1000),
                        sigma=self.args.plot_iter_args.get("sigma", 12),
                        gamma_powernorm=self.args.plot_iter_args.get(
                            "gamma_powernorm", 0.22
                        ),
                        discretize_cmap=self.args.plot_iter_args.get(
                            "discretize_cmap", False
                        ),
                        n_discretize_cmap=self.args.plot_iter_args.get(
                            "n_discretize_cmap", 20
                        ),
                        cmap_norm_type=self.args.plot_iter_args.get(
                            "cmap_norm_type", "PercentileNorm"
                        ),
                        cmap_contour=self.args.plot_iter_args.get(
                            "cmap_contour", "viridis"
                        ),
                        alpha_contourf=self.args.plot_iter_args.get(
                            "alpha_contourf", 1
                        ),
                        alpha_contour=self.args.plot_iter_args.get("alpha_contour", 1),
                        contour_line_overlay=self.args.plot_iter_args.get(
                            "contour_line_overlay", False
                        ),
                        contour_linewidths=self.args.plot_iter_args.get(
                            "contour_linewidths", 0.5
                        ),
                        show_super_legend=self.args.plot_iter_args.get(
                            "show_super_legend", True
                        ),
                        interpolation_method=self.args.plot_iter_args.get(
                            "interpolation_method", "cubic"
                        ),
                        iteration0_figure_insert=self.args.plot_iter_args.get(
                            "iteration0_figure_inset", False
                        ),
                        include_lkldiscard_new_for_axis_range=self.args.plot_iter_args.get(
                            "include_lkldiscard_new_for_axis_range", True
                        ),
                        include_discard_iter0_remnants_for_axis_range=self.args.plot_iter_args.get(
                            "include_discard_iter0_remnants_for_axis_range", False
                        ),
                        axis_labels=self.latex_labels,
                        data_types_to_plot=self.args.plot_iter_args.get(
                            "data_types_to_plot", None
                        ),
                        save_x_y_loglkl=self.args.plot_iter_args.get(
                            "save_x_y_loglkl", False
                        ),
                        pickle_contour_data=pickle_data_plot_iter,
                        plot_threshold_line=self.args.plot_iter_args.get(
                            "plot_threshold_line", False
                        ),
                        delta_chi2_threshold_color=self.args.plot_iter_args.get(
                            "delta_chi2_threshold_color", "purple"
                        ),
                        delta_chi2_threshold_linewidth=self.args.plot_iter_args.get(
                            "delta_chi2_threshold_linewidth", 2
                        ),
                        log_x=self.args.plot_iter_args.get("log_x", False),
                        log_y=self.args.plot_iter_args.get("log_y", False),
                        exclude_iter0_discard_clutter_lower_panels=self.args.plot_iter_args.get(
                            "exclude_iter0_discard_clutter_lower_panels", True
                        ),
                        use_bold_subplot_legend=self.args.plot_iter_args.get(
                            "use_bold_subplot_legend", "auto"
                        ),
                        show_counts_in_legend=self.args.plot_iter_args.get(
                            "show_counts_in_legend", True
                        ),
                        subplot_legend_location=self.args.plot_iter_args.get(
                            "subplot_legend_location", "upper left"
                        ),
                        tall_subplots=self.args.plot_iter_args.get(
                            "tall_subplots", True
                        ),
                        fig_width=self.args.plot_iter_args.get("fig_width", "440 pts"),
                    )
                    plotter_instance.plot()
                except Exception as e:
                    print(f"Error plotting iterations for {param_x} vs. {param_y}: {e}")
                    traceback.print_exc()

        else:
            print(f"run_plot_iterations set to False. Skipping iteration plotting.\n")
            print(
                "\033[1m-----------------------------SKIPPING PLOTTING ITERATIONS -----------------------------\033[0m\n"
            )

        if self.verbose >= 1 and self.args.run_plot_iterations:
            print(
                "\033[1m-----------------------------FINISHED PLOTTING ITERATIONS -----------------------------\033[0m\n"
            )

        # Plot Triangle Plot
        if self.args.run_triangle_plot or self.args.run_grid_plot:
            if self.verbose >= 1:
                print(
                    "\033[1m-----------------------------PLOTTING TRIANGLE/GRID PLOT -----------------------------\033[0m\n"
                )

            pickle_data_triangle_plot = None
            if "pickle_path" in self.args.triangleplot_args:
                pickle_path = self.args.triangleplot_args.get("pickle_path", None)
                if pickle_path and os.path.isfile(pickle_path):
                    with open(pickle_path, "rb") as f:
                        pickle_data_triangle_plot = pickle.load(f)
                    if self.verbose >= 2:
                        print(f"Loaded pickle data for TrianglePlot from {pickle_path}")
                else:
                    if self.verbose >= 1:
                        print(
                            f"Pickle path {pickle_path} not found or not provided. Skipping pickle loading for TrianglePlot."
                        )

            # Set params_triangleplot to use (self.args.triangle_plot_cosmoparams or None) or self.args.triangleplot_args.get("params_triangleplot", None)
            if (
                self.args.triangle_plot_cosmoparams != "all"
                and self.args.triangle_plot_cosmoparams is not None
            ):
                params_triangleplot = self.args.triangle_plot_cosmoparams
            elif (
                self.args.triangleplot_args.get("params_triangleplot", None) != "all"
                and self.args.triangleplot_args.get("params_triangleplot", None)
                is not None
            ):
                params_triangleplot = self.args.triangleplot_args.get(
                    "params_triangleplot", None
                )
            else:
                params_triangleplot = "all"

            if self.args.grid_plot_cosmoparams is not None:
                grid_vars = self.args.grid_plot_cosmoparams
            elif self.args.triangleplot_args.get("grid_vars", None) is not None:
                grid_vars = self.args.triangleplot_args.get("grid_vars", None)
            else:
                # Default grid_vars
                grid_vars = {
                    "rows": ["H0", "omega_b"],
                    "columns": ["H0", "omega_b", "omega_cdm", "YHe"],
                }

            try:
                triangle_plotter_instance = TrianglePlot(
                    data=data,
                    output_folder=self.output_path,
                    iterations_to_plot=self.args.triangleplot_args.get(
                        "iterations_to_plot", "all"
                    ),
                    params_triangleplot=params_triangleplot,
                    grid_vars=grid_vars,
                    verbose=self.verbose,
                    param_labels=self.latex_labels,
                    ignore_iteration_0_for_axis=self.args.triangleplot_args.get(
                        "ignore_iteration_0_for_axis", False
                    ),
                    include_lkldiscard_new_for_axis_range=self.args.triangleplot_args.get(
                        "include_lkldiscard_new_for_axis_range", True
                    ),
                    data_types_to_plot=self.args.triangleplot_args.get(
                        "data_types_to_plot", None
                    ),
                    downsampling_fraction=self.args.triangleplot_args.get(
                        "downsampling_fraction", 0
                    ),
                    preferred_legend_position=self.args.triangleplot_args.get(
                        "preferred_legend_position", "upper right"
                    ),
                    colormap=self.args.triangleplot_args.get("colormap", "tab10"),
                    save_params_loglkls=self.args.triangleplot_args.get(
                        "save_params_loglkls", False
                    ),
                    pickle_contour_data=pickle_data_triangle_plot,
                    plot_contours=self.args.triangleplot_args.get(
                        "plot_contours", False
                    ),
                    grid_size=self.args.triangleplot_args.get("grid_size", 1000),
                    sigma=self.args.triangleplot_args.get("sigma", 12),
                    interpolation_method=self.args.triangleplot_args.get(
                        "interpolation_method", "cubic"
                    ),
                    discretize_cmap=self.args.triangleplot_args.get(
                        "discretize_cmap", False
                    ),
                    n_discretize_cmap=self.args.triangleplot_args.get(
                        "n_discretize_cmap", 20
                    ),
                    cmap_contour=self.args.triangleplot_args.get(
                        "cmap_contour", "viridis"
                    ),
                    alpha_contourf=self.args.triangleplot_args.get("alpha_contourf", 1),
                    alpha_contour=self.args.triangleplot_args.get("alpha_contour", 1),
                    contour_line_overlay=self.args.triangleplot_args.get(
                        "contour_line_overlay", False
                    ),
                    contour_linewidths=self.args.triangleplot_args.get(
                        "contour_linewidths", 0.5
                    ),
                    marker_size=self.args.triangleplot_args.get("marker_size", 2),
                    marker_alpha=self.args.triangleplot_args.get("marker_alpha", 1),
                    marker_edge_lw=self.args.triangleplot_args.get(
                        "marker_edge_lw", 0.1
                    ),
                    param_connect=self.param_connect,
                    plot_threshold_line=self.args.triangleplot_args.get(
                        "plot_threshold_line", False
                    ),
                    delta_chi2_threshold_color=self.args.triangleplot_args.get(
                        "delta_chi2_threshold_color", "purple"
                    ),
                    delta_chi2_threshold_linewidth=self.args.triangleplot_args.get(
                        "delta_chi2_threshold_linewidth", 2
                    ),
                    save_formats=self.args.triangleplot_args.get(
                        "save_formats", ["pdf"]
                    ),
                    custom_axis_ranges=self.args.triangleplot_args.get(
                        "custom_axis_ranges", None
                    ),
                    log_scale_params=self.args.triangleplot_args.get(
                        "log_scale_params", None
                    ),
                    custom_ticks=self.args.triangleplot_args.get("custom_ticks", None),
                )

                if self.args.run_triangle_plot:
                    if self.verbose >= 1:
                        print(
                            "\033[1m-----------------------------PLOTTING TRIANGLE PLOT -----------------------------\033[0m\n"
                        )
                    triangle_plotter_instance.plot(
                        diag_kind="kde",
                        corner=True,
                        include_discarded=not self.args.triangleplot_args.get(
                            "only_accepted", False
                        ),
                    )
                    if self.verbose >= 1:
                        print(
                            "\033[1m-----------------------------FINISHED PLOTTING TRIANGLE PLOT-----------------------------\033[0m"
                        )

                if self.args.run_grid_plot:
                    if self.verbose >= 1:
                        print(
                            "\033[1m-----------------------------PLOTTING GRID PLOT-----------------------------\033[0m"
                        )
                    triangle_plotter_instance.plot_matrix(
                        diag_kind="kde",
                        corner=False,
                        include_discarded=not self.args.triangleplot_args.get(
                            "only_accepted", False
                        ),
                    )
                    if self.verbose >= 1:
                        print(
                            "\033[1m-----------------------------FINISHED PLOTTING GRID PLOT-----------------------------\033[0m"
                        )
            except Exception as e:
                print(f"Error plotting triangle plot or grid plot: {e}")
                traceback.print_exc()

        elif not self.args.run_triangle_plot:
            print(f"run_triangle_plot set to False. Skipping triangle plot.\n")
            print(
                "\033[1m-----------------------------SKIPPING TRIANGLE PLOT -----------------------------\033[0m\n"
            )
        elif not self.args.run_grid_plot:
            print(f"run_grid_plot set to False. Skipping grid plot.\n")
            print(
                "\033[1m-----------------------------SKIPPING GRID PLOT -----------------------------\033[0m\n"
            )

    def parse_python_literal(self, arg):
        """
        Parse a string argument into a Python literal (e.g., list, dict, bool).

        Parameters
        ----------
        arg : str
            The string to parse.

        Returns
        -------
        object
            The parsed Python object.

        Raises
        ------
        argparse.ArgumentTypeError
            If the string cannot be parsed into a valid Python literal.
        """

        if arg == "all":
            return arg

        if arg.lower() in {"true", "false"}:  # Explicitly handle boolean values
            return arg.lower() == "true"

        try:
            return ast.literal_eval(arg)
        except (ValueError, SyntaxError):
            raise argparse.ArgumentTypeError(f"Invalid Python literal: {arg}")


# ---------------------------------CLASS FOR LOADING DATA---------------------------------
# class that automatically loads the iterative data from the project folder
# To see what data is loaded and how it is stored, see the run() method.
class Dataloader:
    """
    Dataloader for CONNECT iterative data.

    This class automatically scans the given project folder for iteration subfolders,
    loads the training data from each iteration (including the initial sampling), and
    assembles the data into a structured dictionary for further analysis and plotting.

    The data are organized by iteration number. The structure of the returned data
    dictionary is as follows:

        data = {
            "iteration_data": {
                iteration_number: {
                    "accepted_accumulated": {
                        "parameters": pd.DataFrame,
                        "likelihood_data": pd.DataFrame
                    },
                    "accepted_new": {
                        "parameters": pd.DataFrame,
                        "likelihood_data": pd.DataFrame
                    },
                    "accepted_old": {
                        "parameters": pd.DataFrame,
                        "likelihood_data": pd.DataFrame
                    },
                    "discarded_iteration": {
                        "parameters": pd.DataFrame,
                        "likelihood_data": pd.DataFrame
                    },
                    "discarded_oversampling": {
                        "parameters": pd.DataFrame
                    },
                    "discarded_likelihood": {
                        "parameters": pd.DataFrame,
                        "likelihood_data": pd.DataFrame
                    },
                    "discarded_likelihood_new": {
                        "parameters": pd.DataFrame,
                        "likelihood_data": pd.DataFrame
                    },
                    "discarded_likelihood_old": {
                        "parameters": pd.DataFrame,
                        "likelihood_data": pd.DataFrame
                    },
                    "failed_class": {
                        "parameters": pd.DataFrame
                    },
                    "best_fit": {
                        "parameters": pd.DataFrame,
                        "likelihood": pd.DataFrame  # Contains a single value for 'loglkl'
                    }
                },
                ...
            }
        }

    **Data Descriptions:**

    - **accepted_accumulated:**
      Contains the points that have survived the likelihood filter across all iterations up to and including the current one.
      (Note: When a likelihood filter is applied, some points accepted in earlier iterations may be dropped in later ones.)

    - **accepted_new:**
      Points generated in the current iteration that got accepted (i.e. not discarded by the likelihood or oversampling filters).

    - **accepted_old:**
      Points accepted in previous iterations that remain accepted in the current iteration.

    - **discarded_iteration:**
      Points discarded from the entire iteration (if the whole batch is discarded).

    - **discarded_oversampling:**
      Points discarded due to oversampling; only contains the 'parameters' DataFrame.

    - **discarded_likelihood:**
      All points discarded by the likelihood filter in the current iteration (the sum of those newly generated discarded and those that were previously accepted but now discarded).

    - **discarded_likelihood_new:**
      Points newly generated in the current iteration but rejected by the likelihood filter.

    - **discarded_likelihood_old:**
      Points that were accepted in a previous iteration but are now discarded by the likelihood filter.

    - **failed_class:**
      Points that failed to run CLASS (typically due to unphysical or invalid parameter values). This key is only available for iterations greater than 0.

    - **best_fit:**
      Contains the best-fit point for the current iteration, with 'parameters' and a likelihood DataFrame (with the best-fit -log-likelihood value, although the column is named "loglkl" rather than "true_loglkl").

    The 'parameters' DataFrames contain the parameter values for each sample, including the model_params and derived_params.
    The 'likelihood_data' DataFrames contain the -log-likelihood values for each sample, it has two columns: 'true_loglkl' and 'chain_loglkl'.

    **Usage Examples:**

    To access the parameters DataFrame from a given iteration:

        df_params = data["iteration_data"][iteration_number]["accepted_accumulated"]["parameters"]
        H0_values = df_params["H0"]
        H0_value = H0_values.iloc[5]

    To access likelihood data:

        df_likelihood = data["iteration_data"][iteration_number]["accepted_accumulated"]["likelihood_data"]
        true_loglkl_values = df_likelihood["true_loglkl"]

    To obtain the best-fit point:

        best_fit_params = data["iteration_data"][iteration_number]["best_fit"]["parameters"]
        best_fit_H0 = best_fit_params["H0"].iloc[0]
        best_fit_likelihood = data["iteration_data"][iteration_number]["best_fit"]["likelihood"]["loglkl"].iloc[0]

    **Additional Notes:**

    - If a pickle file is provided via the `pickle_data` argument, this file is used instead of loading the data from disk.
    - The class automatically detects the initial sampling folder (typically starting with "N-") and numerical iteration folders (starting with "number_"),
      mapping them to iteration 0 and subsequent iteration numbers respectively.
    - Verbose levels control the amount of logging during the data-loading process.

    The main method `run()` returns the complete data dictionary described above.
    """

    def __init__(
        self,
        project_path,
        CONNECT_path,
        param_connect,
        verbose=1,
        save_pickle_data=False,
        pickle_data=None,
    ):
        self.project_path = project_path
        self.CONNECT_path = CONNECT_path
        self.param_connect = param_connect  # Store the param_connect instance
        self.verbose = verbose
        self.save_pickle_data = save_pickle_data
        self.iterations = (
            self._detect_iterations()
        )  # Automatically detect iteration folders
        self.pickle_data = pickle_data

        if self.verbose >= 2:
            print(
                f"[__init__] DataLoader initialized with {len(self.iterations)} iterations."
            )
            if self.verbose >= 3:
                print(f"[__init__] Detected iterations: {self.iterations}")

    def run(self):
        """
        This class will load the data from the project folder and store it in a dictionary.
        Returns a 'data' dictionary with the loaded data.

        The data dictionary will have the following keys:

        - iteration_data[iteration_number]: Dictionary with the following keys:
            - accepted_accumulated: Dictionary with keys "parameters" and "likelihood_data"
            - accepted_new: Dictionary with keys "parameters" and "likelihood_data"
            - accepted_old: Dictionary with keys "parameters" and "likelihood_data"
            - discarded_iteration: Dictionary with keys "parameters" and "likelihood_data"
            - discarded_oversampling: Dictionary with key "parameters"
            - discarded_likelihood: Dictionary with keys "parameters" and "likelihood_data"
            - discarded_likelihood_new: Dictionary with keys "parameters" and "likelihood_data"
            - discarded_likelihood_old: Dictionary with keys "parameters" and "likelihood_data"
            - failed_class: Dictionary with key "parameters"
            - best_fit: Dictionary with keys "parameters" and "likelihood"

            - accepted_accumulated: The current iteration's total accepted points accumulated across all iterations so far. = accepted_new + accepted_old.
                Note, this is not the same as adding all 'accepted_new' from all previous iterations and the current iteration, since a subset of those may have been discarded by the likelihood filter.
                It only contains the total points that remained accepted (survived the likelihood-filter) until and including the current iteration.
                (For example, if iteration 2 had 100 accepted points, but 10 of those were discarded in iteration 3, and 50 of those 90 points were discarded in iteration 4, then the 'accepted_accumulated' for iteration 4 would only contain 40 of the original 100 points accepted in iteration 2.)
                If no likelihood filter was used, then 'accepted_accumulated' should correspond to the sum of all 'accepted_new' from all previous iterations and the current iteration.
            - accepted_new: The newly accepted points; new points added in the current iteration as accepted.
                I.e. newly genereated points from MCMC chains that weren't discarded by the likelihood filter or oversampling filter and thus accepted.
            - accepted_old: Points that were accepted in previous iterations but are still accepted in the current iteration.
                I.e. the training data (accepted data) from the previous iteration that survived the likelihood filter and are still accepted in the current iteration.
                If no likelihood filter was used, then 'accepted_old' should correspond to the sum of all accepted points from previous iterations.
            - discarded_iteration: If the entire iteration was discarded by default (no filter was applied).
            - discarded_oversampling: Points from full chains discarded due to oversampling filter.
            - discarded_likelihood: all points discarded by the likelihood filter at this iteration. = discarded_likelihood_new + discarded_likelihood_old
            - discarded_likelihood_new: Points discarded by the likelihood filter at this iteration, but were new points generated in this iteration by the MCMC chains.
                I.e. newly generated points that didn't survive the likelihood filter and were discarded.
            - discarded_likelihood_old: Points discarded by the likelihood filter at this iteration, but were points accepted in the previous iteration but now got discarded.
            - failed_class: Points that failed to run CLASS. Points generated by the MCMC chains that failed to run CLASS, usually due to invalid parameter values (unphysical). Only available for iterations > 0.
            - best_fit: The current best-fit point from the iteration based on the "true_loglkl" likelihood value.

            The 'parameters' key in each dictionary will contain a DataFrame with columns containing the model and derived parameters.
            The 'likelihood_data' key in each dictionary will contain a DataFrame with columns containing the likelihood data, which consists of columns: 'true_loglkl', 'chain_loglkl',
            The best_fit 'likelihood' key will contain a dataframe with column 'loglkl' containing the best-fit likelihood value. (Note: This is the true_loglkl value, not the chain_loglkl value. I should have named the column 'true_loglkl' for consistency. But it's too late now. Else it has to be changed in all the code that uses this and how the likelihood filter names the column.)

            Examples of accessing data:
            df_params = data["iteration_data"][iteration_number]["accepted_accumulated"]["parameters"]              # DataFrame with model and derived parameters
            H0_values = df_params["H0"]                                                                             # DataFrame Column/array with H0 values.
            H0_value = H0_values.iloc[5]                                                                            # Single H0 value from the first row.
            df_likelihood = data["iteration_data"][iteration_number]["accepted_accumulated"]["likelihood_data"]     # DataFrame with likelihood data
            true_loglkl_values = df_likelihood["true_loglkl"]                                                       # DataFrame Column/array with true_loglkl values.
            df_best_fit_params = data["iteration_data"][iteration_number]["best_fit"]["parameters"]                 # DataFrame with best-fit parameters
            best_fit_H0 = best_fit_params["H0"][0]                                                                  # Best-fit H0 value.
            best_fit_likelihood = data["iteration_data"][iteration_number]["best_fit"]["likelihood"]["loglkl"][0]   # Best-fit likelihood value.

        """

        # ---------------------------------- LOAD PICKLE DATA ----------------------------------

        # If pickle data is provided, use that instead of loading from files.
        # Important: The pickle data should be a dictionary with the same structure as the 'data' dictionary that this function returns.
        if self.pickle_data is not None:
            return self.pickle_data

        # ---------------------------------DETERMINE ITERATION FOLDERS---------------------------------
        # Initialize the data dictionary
        data = {"iteration_data": {}}

        if self.verbose >= 1:
            print("[run] Starting data loading process...")

        # Create a mapping of folders to iteration numbers
        iteration_mapping = {}

        # Identify the initial sampling folder (e.g., N-10000)
        initial_folder = None
        for folder in self.iterations:
            if folder.startswith("N-"):
                initial_folder = folder
                break

        if initial_folder:
            iteration_mapping[initial_folder] = 0
            if self.verbose >= 2:
                print(
                    f"[run] Initial sampling folder identified as iteration 0: {initial_folder}"
                )
        else:
            if self.verbose >= 1:
                print("[run] No initial sampling folder found.")

        # Identify iteration folders like number_1, number_2, etc.
        iteration_folders = [f for f in self.iterations if f.startswith("number_")]
        for folder in iteration_folders:
            iteration_num = int(folder.split("_")[1])
            iteration_mapping[folder] = iteration_num

        if self.verbose >= 2:
            print(f"[run] Iteration folders identified: {iteration_mapping}")

        # Sort folders by iteration number
        all_iterations = sorted(iteration_mapping.items(), key=lambda x: x[1])

        if self.verbose >= 1:
            print(f"[run] Processing iterations in order: {all_iterations}")

        # Initialize the previous combined DataFrame for comparison
        prev_combined = None

        # --------------------------------- LOOP OVER ALL ITERATIONS ---------------------------------

        # Loop over all iterations, including initial (iteration_number=0)
        for folder, iteration_number in all_iterations:
            iteration_path = os.path.join(self.project_path, folder)

            # Initialize the data dictionary for the current iteration
            data["iteration_data"][iteration_number] = {
                "accepted_accumulated": {"parameters": None, "likelihood_data": None},
                "accepted_new": {"parameters": None, "likelihood_data": None},
                "accepted_old": {"parameters": None, "likelihood_data": None},
                "discarded_iteration": {"parameters": None, "likelihood_data": None},
                "discarded_oversampling": {"parameters": None},
                "discarded_likelihood": {"parameters": None, "likelihood_data": None},
                "discarded_likelihood_new": {
                    "parameters": None,
                    "likelihood_data": None,
                },
                "discarded_likelihood_old": {
                    "parameters": None,
                    "likelihood_data": None,
                },
                "failed_class": {"parameters": None},
                "best_fit": {"parameters": None, "likelihood": None},
            }

            # Reset dataframes used in the loop
            current_accumulated = None
            current_combined = None
            lkl_discarded_data = None
            lkl_discarded = None
            lkl_discarded_combined = None
            newly_accepted = None
            likelihood_newly_accepted = None
            discarded_oversampling = None
            failed_class = None
            best_fit = None
            entire_batch_discarded = False
            next_accumulated = None
            next_lkl_discarded = None

            # **Add Separator and Header Here**
            if self.verbose >= 1:
                print("\n" + "-" * 80)
                print(
                    f"[run] Processing Iteration {iteration_number} from Folder '{folder}'"
                )
                print("-" * 80)

            if self.verbose >= 2:
                print("\n[run] --- Loading Data Files ---")

            # --------------------------------- LOAD DATA FILES ---------------------------------
            # Load data for this iteration
            current_accumulated = self.load_data(iteration_path, verbose=self.verbose)

            # After loading data files
            if self.verbose >= 2:
                print("[run] Loaded data files for Iteration", iteration_number)
                if self.verbose >= 3:
                    model_cols = (
                        current_accumulated["model"].columns.tolist()
                        if current_accumulated["model"] is not None
                        else "None"
                    )
                    derived_cols = (
                        current_accumulated["derived"].columns.tolist()
                        if current_accumulated["derived"] is not None
                        else "None"
                    )
                    likelihood_cols = (
                        current_accumulated["likelihood"].columns.tolist()
                        if current_accumulated["likelihood"] is not None
                        else "None"
                    )
                    print(f"  Model Params Columns: {model_cols}")
                    print(f"  Derived Params Columns: {derived_cols}")
                    print(f"  Likelihood Data Columns: {likelihood_cols}")

            if self.verbose >= 3:
                self.debug_print_dataframe(
                    f"iteration_{iteration_number}_accumulated[model]",
                    current_accumulated["model"],
                )
                self.debug_print_dataframe(
                    f"iteration_{iteration_number}_accumulated[derived]",
                    current_accumulated["derived"],
                )
                self.debug_print_dataframe(
                    f"iteration_{iteration_number}_accumulated[likelihood]",
                    current_accumulated["likelihood"],
                )

            # --------------------------------- CHECK IF ENTIRE ITERATION WAS DISCARDED ---------------------------------
            # This is done only for the initial sampling and iteration 1 by checking whether the points in the current iteration appear in the next iteration
            entire_batch_discarded = False

            if iteration_number in [
                0,
                1,
            ]:  # Perform the check only for initial sampling or iteration 1
                if self.verbose >= 2:
                    print("\n[run] --- Checking Batch Discard Status ---")
                    print(
                        f"[run] Checking if entire iteration was discarded for iteration {iteration_number}."
                    )

                # Determine the next iteration path dynamically
                next_iteration_number = iteration_number + 1
                next_iteration_path = None
                next_accumulated = None
                next_lkl_discarded = None

                # Check if there's a next iteration in the mapping
                if next_iteration_number in iteration_mapping.values():
                    next_iteration_folder = [
                        folder
                        for folder, num in iteration_mapping.items()
                        if num == next_iteration_number
                    ][0]
                    next_iteration_path = os.path.join(
                        self.project_path, next_iteration_folder
                    )

                    if os.path.isdir(next_iteration_path):
                        next_accumulated = self.load_data(
                            next_iteration_path, verbose=self.verbose
                        )
                        if self.verbose >= 3:
                            print(
                                f"[run] Loaded data for next iteration {next_iteration_number} from {next_iteration_path}."
                            )
                    else:
                        if self.verbose >= 2:
                            print(
                                f"[run] Next iteration folder does not exist: {next_iteration_path}"
                            )

                    next_lkl_discarded_path = os.path.join(
                        next_iteration_path, "loglkl_discarded_data"
                    )
                    if os.path.isdir(next_lkl_discarded_path):
                        next_lkl_discarded = self.load_data(
                            next_lkl_discarded_path, verbose=self.verbose
                        )
                        if self.verbose >= 3:
                            print(
                                f"[run] Loaded likelihood-filter discarded data for next iteration {next_iteration_number} from {next_lkl_discarded_path}."
                            )
                    else:
                        if self.verbose >= 2:
                            print(
                                f"[run] No likelihood-filter discarded folder found for next iteration {next_iteration_number}."
                            )
                else:
                    if self.verbose >= 2:
                        print(
                            f"[run] No next iteration found in mapping for iteration {iteration_number}."
                        )

                # Add sanity checks for current_accumulated data before proceeding
                if current_accumulated["model"] is None:
                    raise ValueError(
                        f"[run] Current model data for iteration {iteration_number} is None. Cannot proceed with batch discard check."
                    )

                # Check if the entire batch for the current iteration is discarded
                entire_batch_discarded = self.check_if_entire_batch_discarded(
                    current_iter_model=current_accumulated["model"],
                    next_iter_model=(
                        next_accumulated["model"]
                        if next_accumulated and next_accumulated["model"] is not None
                        else None
                    ),
                    next_iter_lkl_discarded=(
                        next_lkl_discarded["model"]
                        if next_lkl_discarded
                        and next_lkl_discarded["model"] is not None
                        else None
                    ),
                    verbose=self.verbose,
                )

                if self.verbose >= 2:
                    if entire_batch_discarded:
                        print(
                            f"[run] The entire training data for iteration {iteration_number} was discarded."
                        )
                    else:
                        print(
                            f"[run] The entire training batch for iteration {iteration_number} was not discarded."
                        )

            # --------------------------------- COMBINE DATAFRAMES ---------------------------------

            # Combine model and derived DataFrames if derived data exists
            if current_accumulated["derived"] is not None:
                if self.verbose >= 2:
                    print("\n[run] --- Combining DataFrames ---")
                    print(
                        f"[run] Combining model_params and derived data for iteration {iteration_number}."
                    )

                current_combined = pd.concat(
                    [current_accumulated["model"], current_accumulated["derived"]],
                    axis=1,
                )
            else:
                current_combined = (
                    current_accumulated["model"].copy()
                    if current_accumulated["model"] is not None
                    else None
                )

            if self.verbose >= 3 and current_combined is not None:
                self.debug_print_dataframe(
                    f"iteration_{iteration_number}_combined", current_combined
                )

            # --------------------------------- LOAD LIKELIHOOD-FILTER DISCARDED DATA ---------------------------------
            if self.verbose >= 2:
                print("\n[run] --- Loading Likelihood-Filter Discarded Data ---")
                print(
                    f"[run] Loading likelihood-filter discarded data for iteration {iteration_number}."
                )
            # Load likelihood-filter discarded data if folder exists
            lkl_discarded_path = os.path.join(iteration_path, "loglkl_discarded_data")
            lkl_discarded = None
            lkl_discarded_data = None
            if os.path.isdir(lkl_discarded_path):
                if self.verbose >= 2:
                    print(
                        f"[run] Loading likelihood-filter discarded data from {lkl_discarded_path}"
                    )
                lkl_discarded_data = self.load_data(
                    lkl_discarded_path, verbose=self.verbose
                )
                if lkl_discarded_data["model"] is not None:
                    if lkl_discarded_data["derived"] is not None:
                        lkl_discarded_combined = pd.concat(
                            [
                                lkl_discarded_data["model"],
                                lkl_discarded_data["derived"],
                            ],
                            axis=1,
                        )
                    else:
                        lkl_discarded_combined = lkl_discarded_data["model"].copy()
                    lkl_discarded = {
                        "combined": lkl_discarded_combined,
                        "likelihood": lkl_discarded_data["likelihood"],
                    }
                    if self.verbose >= 3:
                        self.debug_print_dataframe(
                            f"iteration_{iteration_number}_lkl_discarded_combined",
                            lkl_discarded_combined,
                        )
                else:
                    if self.verbose >= 2:
                        print(
                            f"[run] Iteration {iteration_number}: Likelihood-filter discarded data directory present but no model found."
                        )
            else:
                if self.verbose >= 2:
                    print(
                        f"[run] No likelihood-filter discarded data found for iteration {iteration_number}."
                        " This is expected if the likelihood filter was not applied."
                    )

            # --------------------------------- DETERMINE NEWLY ACCEPTED POINTS ---------------------------------
            # Determine what amount of samples are new in the current iteration compared to the previous one.
            # These new samples are the ones that were accepted in the current iteration, if the entire batch was not discarded.

            # Determine newly accepted points by comparing current_combined with prev_combined
            newly_accepted = None
            likelihood_newly_accepted = None
            if current_combined is not None:
                if self.verbose >= 2:
                    print("\n[run] --- Comparing DataFrames to Find New Points ---")
                    if iteration_number > 0:
                        print(
                            f"[run] Comparing data for iteration {iteration_number} with previous iteration {iteration_number-1}."
                        )
                    else:
                        print(
                            f"[run] Initial sampling iteration; No previous iteration to compare with."
                        )

                newly_accepted, likelihood_newly_accepted = self.compare_dataframes(
                    df1=current_combined.copy(),
                    df2=prev_combined.copy() if prev_combined is not None else None,
                    df_likelihood=(
                        current_accumulated["likelihood"].copy()
                        if current_accumulated["likelihood"] is not None
                        else None
                    ),
                    comparison_type="new",
                    compare_context={
                        "context": "Finding newly accepted points",
                        "msg1": "No points are new in this iteration.",
                        "msg2": "All accepted points are new and generated in this iteration.",
                        "df1": "Current iteration's data",
                        "df2": "Previous iteration's data",
                    },
                )

            else:
                if self.verbose >= 1:
                    print(
                        f"[run] Iteration {iteration_number}: Current combined DataFrame is None."
                    )

            # --------------------------------- DETERMINE ACCEPTED OLD POINTS ---------------------------------
            # Determine the points that were accepted in previous iterations but are still accepted in the current iteration.
            # These are the points that were accepted in previous iterations but were not discarded in the current iteration.

            old_accepted = None
            likelihood_old_accepted = None
            if current_combined is not None:
                if self.verbose >= 2:
                    print("\n[run] --- Comparing DataFrames to Find Old Points ---")
                    print(
                        f"[run] Comparing data for iteration {iteration_number} with previous iterations."
                    )

                old_accepted, likelihood_old_accepted = self.compare_dataframes(
                    df1=current_combined.copy(),
                    df2=newly_accepted.copy() if newly_accepted is not None else None,
                    df_likelihood=(
                        current_accumulated["likelihood"].copy()
                        if current_accumulated["likelihood"] is not None
                        else None
                    ),
                    comparison_type="new",
                    compare_context={
                        "context": "Finding 'old' accepted points",
                        "msg1": "No points are considered accepted this iteration. Likely all points were discarded.",
                        "msg2": "The accumulated accepted points only contain old points from previous iterations. No new points were accepted this iteration.",
                        "df1": "Current iteration's data",
                        "df2": "Newly accepted data",
                    },
                )

            # --------------------------------- CHECK ACCEPTED DATA CONSISTENCY ---------------------------------

            if self.verbose >= 2:
                print("\n[run] --- Checking Accepted Data Consistency ---")

            if current_combined is not None:
                # Call the function to check the consistency of the accepted data
                self.check_data_consistency(
                    iteration_number=iteration_number,
                    combined_data=current_combined,
                    old_data=old_accepted,
                    new_data=newly_accepted,
                    combined_likelihood=(
                        current_accumulated["likelihood"]
                        if current_accumulated["likelihood"] is not None
                        else None
                    ),
                    old_likelihood=likelihood_old_accepted,
                    new_likelihood=likelihood_newly_accepted,
                    data_type="(ACCEPTED)",
                    verbose=self.verbose,
                )

            # --------------------------------- END CHECK ACCEPTED DATA CONSISTENCY ---------------------------------

            # --------------------------------- DETERMINE LIKELIHOOD-FILTER DISCARDED OLD AND NEW POINTS ---------------------------------

            # --------------------------------- DETERMINE LIKELIHOOD-FILTER DISCARDED OLD POINTS ---------------------------------
            # Determine the points that were accepted in previous iterations but are now discarded in the current iteration.
            # These are the points that were accepted in previous iterations but were discarded by the likelihood filter in the current iteration.

            old_lkl_discarded = None
            likelihood_old_lkl_discarded = None
            # Here we need to compare the previous accepted accumulated points with the discarded likelihood data
            if prev_combined is not None and lkl_discarded_combined is not None:
                if self.verbose >= 2:
                    print(
                        "\n[run] --- Comparing DataFrames to Find Old Likelihood-Discarded Points ---"
                    )
                    print(
                        f"[run] Comparing data for iteration {iteration_number} with previous iterations."
                    )

                old_lkl_discarded, likelihood_old_lkl_discarded = (
                    self.compare_dataframes(
                        df1=lkl_discarded["combined"].copy(),
                        df2=prev_combined.copy(),
                        df_likelihood=(
                            lkl_discarded["likelihood"].copy()
                            if lkl_discarded["likelihood"] is not None
                            else None
                        ),
                        comparison_type="common",
                        compare_context={
                            "context": "Finding 'old' likelihood-discarded points",
                            "msg1": "No points were discarded by the likelihood filter in this iteration.",
                            "msg2": "No previous accepted points exists. If any points were discarded by the likelihood filter, they were all new points generated in this iteration.",
                            "df1": "All Likelihood-filter discarded data this iteration",
                            "df2": "Previously accepted data",
                        },
                    )
                )

            # --------------------------------- DETERMINE LIKELIHOOD-FILTER DISCARDED NEW POINTS ---------------------------------
            # Determine the points that were newly accepted in the current iteration but are now discarded by the likelihood filter.
            # These are the points that were newly accepted in the current iteration but were discarded by the likelihood filter in the current iteration.

            new_lkl_discarded = None
            likelihood_new_lkl_discarded = None

            if lkl_discarded_combined is not None and old_lkl_discarded is not None:
                if self.verbose >= 2:
                    print(
                        "\n[run] --- Comparing DataFrames to Find New Likelihood-Discarded Points ---"
                    )
                    print(
                        f"[run] Comparing data for iteration {iteration_number} with previous iterations."
                    )

                new_lkl_discarded, likelihood_new_lkl_discarded = (
                    self.compare_dataframes(
                        df1=lkl_discarded["combined"].copy(),
                        df2=old_lkl_discarded.copy(),
                        df_likelihood=(
                            lkl_discarded["likelihood"].copy()
                            if lkl_discarded["likelihood"] is not None
                            else None
                        ),
                        comparison_type="new",
                        compare_context={
                            "context": "Finding 'new' likelihood-discarded points",
                            "msg1": "No points were discarded by the likelihood filter in this iteration.",
                            "msg2": "None of the discarded points this iteration were previously accepted. Only newly generated points were discarded, if any.",
                            "df1": "All Likelihood-filter discarded data this iteration",
                            "df2": "Previous iteration's accepted data",
                        },
                    )
                )

            if lkl_discarded_combined is not None and old_lkl_discarded is None:
                new_lkl_discarded = lkl_discarded["combined"].copy()
                likelihood_new_lkl_discarded = lkl_discarded["likelihood"].copy()

            # --------------------------------- CHECK LIKELIHOOD-DISCARDED CONSISTENCY ---------------------------------
            if self.verbose >= 2:
                print("\n[run] --- Checking Likelihood-Discarded Consistency ---")

            if lkl_discarded_combined is not None:
                # Call the function to check the consistency of the discarded likelihood-filter data
                self.check_data_consistency(
                    iteration_number=iteration_number,
                    combined_data=(
                        lkl_discarded["combined"] if lkl_discarded is not None else None
                    ),
                    old_data=old_lkl_discarded,
                    new_data=new_lkl_discarded,
                    combined_likelihood=(
                        lkl_discarded["likelihood"]
                        if lkl_discarded is not None
                        else None
                    ),
                    old_likelihood=likelihood_old_lkl_discarded,
                    new_likelihood=likelihood_new_lkl_discarded,
                    data_type="[DISCARDED (LIKELIHOOD-FILTER)]",
                    verbose=self.verbose,
                )

            # --------------------------------- CHECK FOR OVERSAMPLING DISCARDED POINTS ---------------------------------

            if self.verbose >= 2 and iteration_number > 0:
                print("\n[run] --- Checking for Oversampling Discarded Points ---")
                print(
                    f"[run] Checking for oversampling discarded points in iteration {iteration_number}."
                )
            # Load chain files:
            discarded_oversampling = None
            if iteration_number > 0:
                if self.verbose >= 2:
                    print(f"[run] Loading chain data for iteration {iteration_number}.")

                chain_data = self.load_chain_data(
                    iteration_path,
                    self.param_connect,
                    verbose=self.verbose,
                    full_chains=False,
                )

                path_full_chains = os.path.join(iteration_path, "full_chains")
                if os.path.isdir(path_full_chains):
                    chain_data_full = self.load_chain_data(
                        iteration_path,
                        self.param_connect,
                        verbose=self.verbose,
                        full_chains=True,
                    )
                else:
                    chain_data_full = None
                    if self.verbose >= 2:
                        print(
                            f"[run] No 'full_chains' directory found for iteration {iteration_number}. Skipping full chain loading."
                        )

                if self.verbose >= 4:
                    print(f"[run] Chain data for iteration {iteration_number}:")
                    print(chain_data.head())
                    if chain_data_full is not None:
                        print(
                            f"[run] Full chain data for iteration {iteration_number}:"
                        )
                        print(chain_data_full.head())
                    else:
                        print(
                            f"[run] Full chain data for iteration {iteration_number}: None"
                        )

                discarded_oversampling = self.find_oversampling_discarded(
                    reduced_chain_df=chain_data,
                    full_chain_df=chain_data_full,
                    verbose=self.verbose,
                )

            # --------------------------------- CHECK FOR FAILED CLASS RUNS ---------------------------------
            if self.verbose >= 2 and iteration_number > 0:
                print("\n[run] --- Checking for Failed CLASS Runs ---")
                print(
                    f"[run] Checking for failed CLASS runs in iteration {iteration_number}."
                )

            if iteration_number > 0:
                chain_model_df = self.load_model_chain_data(
                    iteration=iteration_number, verbose=self.verbose
                )

            failed_class = None
            if iteration_number > 0:
                failed_class = self.find_failed_class_points(
                    chain_model_df=chain_model_df,
                    model_df=current_accumulated["model"],
                    model_lkl_discarded_df=(
                        lkl_discarded_data["model"]
                        if lkl_discarded_data is not None
                        else None
                    ),
                    exact=True,
                    verbose=self.verbose,
                )

            # --------------------------------- LOAD BEST-FIT POINT ---------------------------------
            # Load best-fit point
            if self.verbose >= 2:
                print("\n[run] --- Loading Best-Fit Point ---")
                print(f"[run] Loading best-fit point for iteration {iteration_number}.")
            best_fit = None
            best_fit = self.load_best_fit(iteration_path, verbose=self.verbose)

            if self.verbose >= 2:
                print(
                    f"[run] Loaded best-fit point for iteration {iteration_number}: {True if best_fit is not None else False}"
                )

            # --------------------------------- STORE DATA IN DICTIONARY ---------------------------------

            # --------- Store accepted or discarded_iteration data in the dictionary --------

            # Store the collected info in data["iteration_data"][iteration_number]
            if entire_batch_discarded:
                data["iteration_data"][iteration_number]["discarded_iteration"] = {
                    "parameters": current_combined,
                    "likelihood_data": (
                        current_accumulated["likelihood"]
                        if current_accumulated["likelihood"] is not None
                        else None
                    ),
                }
            else:
                data["iteration_data"][iteration_number]["accepted_accumulated"] = {
                    "parameters": current_combined,
                    "likelihood_data": (
                        current_accumulated["likelihood"]
                        if current_accumulated["likelihood"] is not None
                        else None
                    ),
                }

                if newly_accepted is not None and not newly_accepted.empty:
                    data["iteration_data"][iteration_number]["accepted_new"] = {
                        "parameters": newly_accepted,
                        "likelihood_data": (
                            likelihood_newly_accepted
                            if likelihood_newly_accepted is not None
                            else None
                        ),
                    }

                if old_accepted is not None and not old_accepted.empty:
                    data["iteration_data"][iteration_number]["accepted_old"] = {
                        "parameters": old_accepted,
                        "likelihood_data": (
                            likelihood_old_accepted
                            if likelihood_old_accepted is not None
                            else None
                        ),
                    }

                else:
                    if self.verbose >= 2:
                        print(
                            f"[run] Iteration {iteration_number}: No newly accepted points found."
                        )

            # --------- Store discarded likelihood-filter data if available --------
            # Store discarded likelihood-filter data if available
            if lkl_discarded is not None:
                data["iteration_data"][iteration_number]["discarded_likelihood"] = {
                    "parameters": lkl_discarded["combined"],
                    "likelihood_data": lkl_discarded["likelihood"],
                }

            if old_lkl_discarded is not None and not old_lkl_discarded.empty:
                data["iteration_data"][iteration_number]["discarded_likelihood_old"] = {
                    "parameters": old_lkl_discarded,
                    "likelihood_data": (
                        likelihood_old_lkl_discarded
                        if likelihood_old_lkl_discarded is not None
                        else None
                    ),
                }

            if (
                (new_lkl_discarded is None)
                and (old_lkl_discarded is None)
                and (lkl_discarded is not None)
            ):
                data["iteration_data"][iteration_number]["discarded_likelihood_new"] = {
                    "parameters": lkl_discarded["combined"],
                    "likelihood_data": lkl_discarded["likelihood"],
                }

            if new_lkl_discarded is not None and not new_lkl_discarded.empty:
                data["iteration_data"][iteration_number]["discarded_likelihood_new"] = {
                    "parameters": new_lkl_discarded,
                    "likelihood_data": (
                        likelihood_new_lkl_discarded
                        if likelihood_new_lkl_discarded is not None
                        else None
                    ),
                }

            # --------- Store discarded oversampling data if available --------
            # Store discarded oversampling data if available
            if discarded_oversampling is not None:
                data["iteration_data"][iteration_number]["discarded_oversampling"] = {
                    "parameters": discarded_oversampling
                }

            # --------- Store failed CLASS runs data if available --------
            # Store failed CLASS runs data if available
            if failed_class is not None:
                data["iteration_data"][iteration_number]["failed_class"] = {
                    "parameters": failed_class
                }

            # --------- Store best-fit data if available --------
            # Store best-fit data if available
            if best_fit is not None:
                data["iteration_data"][iteration_number]["best_fit"] = {
                    "parameters": best_fit["parameters"],
                    "likelihood": best_fit["likelihood"],
                }

            # Summarizing iteration
            if self.verbose >= 2:
                print("\n[run] --- Iteration Summary ---")
                if entire_batch_discarded:
                    print(
                        f"[run] Iteration {iteration_number}: Entire batch discarded without filter."
                    )
                else:
                    num_new = (
                        len(newly_accepted)
                        if (newly_accepted is not None and not newly_accepted.empty)
                        else 0
                    )
                    print(
                        f"[run] Iteration {iteration_number}: {num_new} newly accepted points."
                    )

            if self.verbose >= 2:
                print("\n[run] --- Summary Statistics for Delta Chi2 ---")
                # Print .describe() for delta_chi2 for accumulated and accepted and discarded points
                if current_accumulated["likelihood"] is not None:
                    accumulated_chi2 = None
                    accumulated_chi2 = pd.DataFrame()
                    accumulated_chi2 = 2 * (
                        current_accumulated["likelihood"]["true_loglkl"]
                        - best_fit["likelihood"]["loglkl"][0]
                    )
                    print(
                        f"[run] Iteration {iteration_number}: Accumulated Likelihood Data Delta Chi2:"
                    )
                    print(accumulated_chi2.describe())
                if likelihood_newly_accepted is not None:
                    newly_chi2 = None
                    newly_chi2 = pd.DataFrame()
                    newly_chi2 = 2 * (
                        likelihood_newly_accepted["true_loglkl"]
                        - best_fit["likelihood"]["loglkl"][0]
                    )
                    print(
                        f"[run] Iteration {iteration_number}: Newly Accepted Points Delta Chi2:"
                    )
                    print(newly_chi2.describe())
                if lkl_discarded_data is not None:
                    discarded_chi2 = None
                    discarded_chi2 = pd.DataFrame()
                    discarded_chi2 = 2 * (
                        lkl_discarded_data["likelihood"]["true_loglkl"]
                        - best_fit["likelihood"]["loglkl"][0]
                    )
                    print(
                        f"[run] Iteration {iteration_number}: Discarded Likelihood Data Delta Chi2:"
                    )
                    print(discarded_chi2.describe())

            # Update prev_combined for the next iteration
            prev_combined = current_combined

        if self.verbose >= 1:
            print("\n" + "=" * 80)
            print("[run] Data Loading Complete.")
            print("[run] Summary:")
            print(f"[run] Total Iterations Processed: {len(data['iteration_data'])}")
            print("\n[run] Data Dictionary Overview:")

            # Iterate through the iterations to summarize their contents
            for iteration, content in data["iteration_data"].items():
                print(f"  - Iteration {iteration}:")
                for category, details in content.items():
                    if details is None:  # Explicitly state None for empty keys
                        print(f"    * {category}: None")
                    elif isinstance(details, dict):  # Nested dict
                        # add the number of samples as well next to Filled
                        sub_summary = {
                            k: ("None" if v is None else f"Filled: {len(v)} points")
                            for k, v in details.items()
                        }
                        print(f"    * {category}: {sub_summary}")
                    elif isinstance(details, pd.DataFrame):  # DataFrame summary
                        print(f"    * {category}: DataFrame with shape {details.shape}")
                    else:  # Catch-all for other types
                        print(f"    * {category}: {type(details).__name__}")

            print("=" * 80)

        if self.save_pickle_data:
            if self.verbose >= 1:
                print("\n[run] --- Pickling Data ---")
            # Create a folder in project_path/iteration_analysis if it doesn't exist
            os.makedirs(
                os.path.join(self.project_path, "iteration_analysis"), exist_ok=True
            )
            # Pickle the data dictionary
            pickle_path = os.path.join(
                self.project_path, "iteration_analysis", "iteration_data.pickle"
            )
            with open(pickle_path, "wb") as f:
                pickle.dump(data, f)
            if self.verbose >= 1:
                print(f"[run] Pickled data dictionary to {pickle_path}")

        return data

    def load_data(self, path, verbose=1):
        """
        Load model_params.txt, derived.txt, and likelihood_data.txt from a given path.
        Returns a dictionary with DataFrames for each file.
        """
        model_df = self.load_data_file(
            os.path.join(path, "model_params.txt"), verbose=verbose
        )
        derived_df = None
        derived_path = os.path.join(path, "derived.txt")
        if os.path.isfile(derived_path):
            derived_df = self.load_data_file(derived_path, verbose=verbose)

        likelihood_path = os.path.join(path, "likelihood_data.txt")
        likelihood_df = None
        if os.path.isfile(likelihood_path):
            likelihood_df = self.load_data_file(likelihood_path, verbose=verbose)

        return {"model": model_df, "derived": derived_df, "likelihood": likelihood_df}

    def load_data_file(self, file_path, verbose=1):
        """
        Load a data file that has a header line starting with '#' and returns a DataFrame.
        """
        if not os.path.isfile(file_path):
            if verbose >= 1:
                print(f"[load_data_file] File {file_path} does not exist.")
            return None

        header_line = None
        with open(file_path, "r") as f:
            for line in f:
                if line.startswith("#"):
                    header_line = line.lstrip("#").strip()
                    break

        if header_line is None:
            raise ValueError(f"No header line starting with '#' found in {file_path}")

        columns = header_line.split()
        if verbose >= 3:
            print(f"[load_data_file] Columns for {file_path}: {columns}")

        df = pd.read_csv(
            file_path,
            sep=r"\s+",
            comment="#",
            names=columns,
            index_col=False,
            dtype=np.float32,
        )

        # Optional sanity checks
        if df.empty and verbose >= 2:
            print(
                f"[load_data_file] Warning: Loaded DataFrame from {file_path} is empty."
            )

        return df

    def compare_dataframes(
        self,
        df1,
        df2,
        df_likelihood=None,
        df_likelihood2=None,
        comparison_type="new",
        verbose=1,
        compare_context=None,
    ):
        """
        Compare two DataFrames (df1, df2) to identify samples that are 'new', 'removed', or 'common',
        while preserving one-to-one matching of duplicates. Also re-aligns likelihood data if provided.

        Parameters
        ----------
        df1 : pd.DataFrame
            The first DataFrame (e.g., the "current" iteration's data).
        df2 : pd.DataFrame
            The second DataFrame (e.g., the "previous" iteration's data).
        df_likelihood : pd.DataFrame, optional
            Likelihood rows aligned with df1 (same length, same row order as df1 BEFORE sorting).
        df_likelihood2 : pd.DataFrame, optional
            Likelihood rows aligned with df2 (same length, same row order as df2 BEFORE sorting).
        comparison_type : {'new','removed','common'}, default='new'
            - 'new': return rows in df1 that are not in df2.
            - 'removed': return rows in df2 that are not in df1.
            - 'common': return rows present in both df1 and df2.
        verbose : int, optional
            If >0, prints some debugging info.

        Returns
        -------
        matched_params : pd.DataFrame
            Subset of parameter rows that match the requested relationship,
            extracted from the correct perspective (df1 or df2, or intersection).
        matched_likelihood : pd.DataFrame or None
            Subset of likelihood rows that align with matched_params. If none given,
            returns None.
        """

        # --------------------------------------------------
        # Step 1: Validate input parameters
        # --------------------------------------------------
        valid_types = ["new", "removed", "common"]
        if comparison_type not in valid_types:
            raise ValueError(
                f"comparison_type must be one of {valid_types}, got: {comparison_type}"
            )

        # --------------------------------------------------
        # Step 2: Handle edge cases (if df1 or df2 is empty)
        # --------------------------------------------------
        if df1 is None or df1.empty:
            if verbose > 0:
                print(
                    f'\n[compare_dataframes] [{compare_context["context"]}] df1 ({compare_context["df1"]}) is empty; returning trivial result:\n {compare_context["msg1"]}'
                )
            if comparison_type == "removed" and df2 is not None:
                return df2.copy().reset_index(drop=True), df_likelihood2
            return None, None

        if df2 is None or df2.empty:
            if verbose > 0:
                print(
                    f'\n[compare_dataframes] [{compare_context["context"]}] df2 ({compare_context["df2"]}) is empty; returning trivial result:\n {compare_context["msg2"]}'
                )
            if comparison_type == "new":
                return df1.copy().reset_index(drop=True), df_likelihood
            return None, None

        # --------------------------------------------------
        # Step 2b: Ensure likelihood data and dataframes match in length
        if df_likelihood is not None and len(df_likelihood) != len(df1):
            raise ValueError(
                f'\nLength mismatch: df_likelihood ({len(df_likelihood)}) and df1 ({compare_context["df1"]}) ({len(df1)}) are not equal.'
            )
        if df_likelihood2 is not None and len(df_likelihood2) != len(df2):
            raise ValueError(
                f'\nLength mismatch: df_likelihood2 ({len(df_likelihood2)}) and df2 ({compare_context["df2"]}) ({len(df2)}) are not equal.'
            )

        # --------------------------------------------------
        # Step 3: Preprocess df1 (current iteration's data)
        # --------------------------------------------------
        df1 = df1.copy()
        df1["_temp_idx1"] = df1.index  # Store original row index before sorting

        # Identify the relevant parameter columns (excluding helper columns)
        param_cols = [c for c in df1.columns if c not in ["_temp_idx1", "dup_id"]]

        # Sort df1 so that identical samples appear together
        df1_sorted = df1.sort_values(param_cols, kind="mergesort").reset_index(
            drop=True
        )

        # Assign a 'dup_id' to each duplicate row so they can be matched one-to-one
        df1_sorted["dup_id"] = df1_sorted.groupby(param_cols).cumcount()

        # Reorder the likelihood data to match this new sorted order
        df_likelihood_sorted = (
            df_likelihood.iloc[df1_sorted["_temp_idx1"]].reset_index(drop=True)
            if df_likelihood is not None
            else None
        )

        # --------------------------------------------------
        # Step 4: Preprocess df2 (previous iteration's data)
        # --------------------------------------------------
        df2 = df2.copy()
        df2["_temp_idx2"] = df2.index

        df2_sorted = df2.sort_values(param_cols, kind="mergesort").reset_index(
            drop=True
        )
        df2_sorted["dup_id"] = df2_sorted.groupby(param_cols).cumcount()

        df_likelihood2_sorted = (
            df_likelihood2.iloc[df2_sorted["_temp_idx2"]].reset_index(drop=True)
            if df_likelihood2 is not None
            else None
        )

        # --------------------------------------------------
        # Step 5: Perform Merge to Find Matches
        # --------------------------------------------------
        """
        We now compare df1_sorted and df2_sorted to determine which samples belong to which category:
        
        - 'new': Samples in df1 but not in df2 (found using a LEFT JOIN)
        - 'removed': Samples in df2 but not in df1 (found using a RIGHT JOIN)
        - 'common': Samples that exist in both df1 and df2 (found using an INNER JOIN)

        The 'merge' function combines both dataframes based on their common parameter columns + 'dup_id'.
        This ensures that duplicate rows match correctly and one-to-one.
        
        The 'how' parameter controls which type of comparison we perform:
        
        - 'left' (for 'new'): Keeps all rows from df1_sorted, adds matches from df2_sorted.
        - 'right' (for 'removed'): Keeps all rows from df2_sorted, adds matches from df1_sorted.
        - 'inner' (for 'common'): Keeps only rows that exist in BOTH df1_sorted and df2_sorted.

        The 'indicator=True' adds a new column `_merge`, which labels each row as:
        - 'left_only'  → Present only in df1 (new sample)
        - 'right_only' → Present only in df2 (removed sample)
        - 'both'       → Present in both (common sample)
        """
        if comparison_type == "new":
            merge_type = "left"
            indicator = True
        elif comparison_type == "removed":
            merge_type = "right"
            indicator = True
        else:  # 'common'
            merge_type = "inner"
            indicator = False

        merged = df1_sorted.merge(
            df2_sorted,
            on=param_cols + ["dup_id"],
            how=merge_type,
            indicator=indicator,
            suffixes=("_df1", "_df2"),
        )

        # --------------------------------------------------
        # Step 6: Extract the Matching Rows from the Merge
        # --------------------------------------------------
        """
        Now that we have merged df1_sorted and df2_sorted, we extract the rows based on `_merge`:

        - For 'new': We filter only rows labeled as 'left_only' (i.e., samples that appear in df1 but not df2).
        - For 'removed': We filter only rows labeled as 'right_only' (samples in df2 but not df1).
        - For 'common': We take all merged rows, since they exist in both dataframes.
        """
        if comparison_type == "new":
            matched_df = merged[merged["_merge"] == "left_only"].drop(
                columns=["_merge"]
            )
        elif comparison_type == "removed":
            matched_df = merged[merged["_merge"] == "right_only"].drop(
                columns=["_merge"]
            )
        else:  # 'common'
            matched_df = merged

        # Extract the original rows from df1 or df2
        matched_params = (
            df1.iloc[matched_df["_temp_idx1"]].copy()
            if comparison_type != "removed"
            else df2.iloc[matched_df["_temp_idx2"]].copy()
        )

        # Remove helper columns
        matched_params.drop(
            columns=["dup_id", "_temp_idx1", "_temp_idx2"],
            inplace=True,
            errors="ignore",
        )
        matched_params.reset_index(drop=True, inplace=True)

        # --------------------------------------------------
        # Step 7: Extract Aligned Likelihood Data
        # --------------------------------------------------
        matched_likelihood = None
        if comparison_type in ["new", "common"] and df_likelihood_sorted is not None:
            matched_likelihood = (
                df_likelihood.iloc[matched_df["_temp_idx1"]]
                .copy()
                .reset_index(drop=True)
            )
        elif comparison_type == "removed" and df_likelihood2_sorted is not None:
            matched_likelihood = (
                df_likelihood2.iloc[matched_df["_temp_idx2"]]
                .copy()
                .reset_index(drop=True)
            )

        return matched_params, matched_likelihood

    def check_if_entire_batch_discarded(
        self, current_iter_model, next_iter_model, next_iter_lkl_discarded, verbose=1
    ):
        """
        Determine if entire batch was discarded without any filter.
        current_iter_model: DataFrame containing model parameters for the current iteration.
        next_iter_model: DataFrame containing model parameters for the next iteration.
        next_iter_lkl_discarded: DataFrame containing likelihood-filter discarded model parameters for the next iteration.
        """
        if current_iter_model is None or current_iter_model.empty:
            # If there's no current_iter_model at all, this is unexpected.
            # This might be a critical error since we cannot proceed.
            if verbose >= 1:
                print(
                    "[check_if_entire_batch_discarded] current_iter_model is None or empty, unexpected scenario."
                )
            # Raise an assertion as this scenario indicates something else went wrong
            assert not (
                current_iter_model is None or current_iter_model.empty
            ), "No current_iter_model found, cannot determine discard status."

        if (
            next_iter_model is None
            or (next_iter_model.empty if next_iter_model is not None else True)
        ) and (
            next_iter_lkl_discarded is None
            or (
                next_iter_lkl_discarded.empty
                if next_iter_lkl_discarded is not None
                else True
            )
        ):
            if verbose >= 2:
                print(
                    "[check_if_entire_batch_discarded] No next iteration data found; assuming entire batch discarded."
                )
            return True

        merge_columns = current_iter_model.columns.tolist()
        next_iter_match = 0
        if next_iter_model is not None and not next_iter_model.empty:
            merged_next = current_iter_model.merge(
                next_iter_model, on=merge_columns, how="inner"
            )
            next_iter_match = len(merged_next)

        lkl_discarded_match = 0
        if next_iter_lkl_discarded is not None and not next_iter_lkl_discarded.empty:
            merged_lkl = current_iter_model.merge(
                next_iter_lkl_discarded, on=merge_columns, how="inner"
            )
            lkl_discarded_match = len(merged_lkl)

        if next_iter_match > 0 or lkl_discarded_match > 0:
            if verbose >= 2:
                print(
                    "[check_if_entire_batch_discarded] Found matches in next iteration data. Not entirely discarded."
                )
            return False
        else:
            if verbose >= 2:
                print(
                    "[check_if_entire_batch_discarded] No matches found. Entire batch discarded without filter."
                )
            return True

    def load_chain_data(self, file_path, param_connect, verbose=1, full_chains=False):
        """
        Load and process chain data from the given iteration folder.

        Parameters:
        - file_path (str): Path to the iteration folder (e.g., '/path/to/jobname/number_1')
        - param_connect: The param object similar to self.param, containing parameter info.
        - verbose (int): Verbosity level.
        - full_chains (bool): If True, load chains from the 'full_chains' subfolder.

        Returns:
        - df (pd.DataFrame): DataFrame containing model + derived parameters (no nuisance), scaled and converted as needed.
        """

        # Identify chain directory
        chain_dir = os.path.join(file_path, "full_chains") if full_chains else file_path

        # Find the chain files
        chain_files = [
            f for f in os.listdir(chain_dir) if f.endswith(".txt") and "__" in f
        ]
        chain_files.sort()  # Ensure consistent order (e.g., __1.txt, __2.txt, ...)

        if verbose >= 2:
            print(
                f"[load_chain_data] Found {len(chain_files)} chain files in {chain_dir}: {chain_files}"
            )

        # Locate .paramnames file in the parent folder (not in full_chains)
        paramnames_file = None
        for f in os.listdir(file_path):
            if f.endswith(".paramnames"):
                paramnames_file = os.path.join(file_path, f)
                break
        if paramnames_file is None:
            raise FileNotFoundError(
                f"[load_chain_data] No .paramnames file found in {file_path}"
            )

        # Locate log.param file
        log_param_file = os.path.join(file_path, "log.param")
        if not os.path.isfile(log_param_file):
            raise FileNotFoundError(
                f"[load_chain_data] log.param not found in {file_path}"
            )

        # Parse paramnames file to get parameter order
        with open(paramnames_file, "r") as f:
            paramnames_lines = [line.strip() for line in f if line.strip()]

        # paramnames_lines now contains lines like:
        # omega_b     10^{-2}\omega{}_{b }
        # ...
        # Extract the parameter names (the first token in each line):
        all_chain_params = []
        for line in paramnames_lines:
            parts = line.split(None, 1)  # split into two parts max
            if len(parts) > 0:
                p_name = parts[0].strip()
                all_chain_params.append(p_name)

        # Handle parameter name normalization (e.g., 100theta_s -> 100*theta_s)
        name_mapping = {
            "100theta_s": "100*theta_s",
            # Add more mappings if needed
        }
        normalized_chain_params = [name_mapping.get(p, p) for p in all_chain_params]

        # Now we have a list normalized_chains_params that matches the chain columns after the first column (with fake loglkl).
        # Parse log.param to identify parameter scaling and types
        param_info = self.parse_log_param(log_param_file, verbose=verbose)

        # param_info could be a dictionary: { 'omega_b': {'type': 'cosmo', 'scale': 1.0, 'log_prior':False}, ... }
        # We'll write a helper function parse_log_param below

        # Identify which parameters from normalized_chains_params are model, nuisance, derived:
        # According to param_info, each has a 'type' field that can be 'cosmo', 'nuisance', 'derived'.
        # Also, param_connect.parameters keys give us model param ranges.

        # Separate parameters by type (model, nuisance, derived)
        model_params = []
        nuisance_params = []
        derived_params = []
        for p in normalized_chain_params:
            p_type = param_info.get(p, {}).get("type", None)
            if p_type == "cosmo":
                model_params.append(p)
            elif p_type == "nuisance":
                nuisance_params.append(p)
            elif p_type == "derived":
                derived_params.append(p)
            else:
                # If type is unknown, assume model and warn
                if verbose >= 2:
                    print(
                        f"[load_chain_data] Warning: Parameter '{p}' not found in log.param info. Assuming model."
                    )
                model_params.append(p)

        # Determine scaling factors and log-prior conversion
        # param_info[p]['scale'] gives scaling factor
        # If p in param_connect.log_priors: need to do 10^(value)

        # Handle log-prior conversions
        log_priors = getattr(param_connect, "log_priors", [])

        # Read chain files and construct DataFrame
        all_data = []
        for cf in chain_files:
            chain_path = os.path.join(chain_dir, cf)
            if verbose >= 3:
                print(f"[load_chain_data] Reading chain file {chain_path}")
            with open(chain_path, "r") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith("#"):
                        continue
                    # The first column has two values separated by two spaces.
                    # Let's split by whitespace at first. The first two numbers are step count and fake loglkl.
                    # Use split with maxsplit=2 to isolate the remainder
                    # The remainder should be tab-separated parameters afterwards.
                    parts = line.split(None, 2)
                    if len(parts) < 3:
                        # Malformed line
                        if verbose >= 2:
                            print(f"[load_chain_data] Warning: Malformed line in {cf}.")
                        continue
                    # parts[0] = step_count, parts[1] = fake_loglkl, parts[2] = rest of params separated by tabs
                    param_values_str = parts[2]
                    param_values = np.array(
                        param_values_str.split("\t"), dtype=np.float32
                    )

                    if len(param_values) != len(all_chain_params):
                        # Length mismatch -> skip or warn
                        if verbose >= 2:
                            print(
                                f"[load_chain_data] Warning: Parameter count mismatch in line of {cf}."
                            )
                        continue

                    # Handle log-priors
                    for p_name in normalized_chain_params:
                        # if p_name in log_priors:
                        if (
                            p_name[-12:] == "_log10_prior"
                            and p_name[:-12] in log_priors
                        ):
                            idx = normalized_chain_params.index(p_name)
                            # Convert from log10 to linear scale
                            param_values[idx] = np.power(10.0, param_values[idx])

                    # Apply scaling factors
                    for p_name in normalized_chain_params:
                        scale = param_info.get(p_name, {}).get("scale", 1.0)
                        idx = normalized_chain_params.index(p_name)
                        param_values[idx] *= np.float32(scale)
                    all_data.append(param_values)

        for p_name in normalized_chain_params:
            if p_name[-12:] == "_log10_prior" and p_name[:-12] in log_priors:
                idx = normalized_chain_params.index(p_name)
                normalized_chain_params[idx] = normalized_chain_params[idx][:-12]

        # Create a DataFrame with normalized_chain_params as columns
        df = pd.DataFrame(all_data, columns=normalized_chain_params, dtype=np.float32)
        # Drop nuisance parameters
        df.drop(columns=nuisance_params, inplace=True, errors="ignore")

        # The final DataFrame should have model + derived parameters in some sensible order
        # Enforce correct order of columns (model + derived, in that order)
        final_columns = model_params + derived_params

        for p_name in final_columns:
            if p_name[-12:] == "_log10_prior" and p_name[:-12] in log_priors:
                idx = final_columns.index(p_name)
                final_columns[idx] = final_columns[idx][:-12]

        final_columns = [col for col in final_columns if col in df.columns]
        df = df[final_columns]

        if (
            hasattr(param_connect, "custom_parameters")
            and param_connect.custom_parameters
        ):
            data_np = df.to_numpy(dtype=np.float32)

            transform_custom_parameters = (
                SourceFileLoader(
                    "transform_custom_parameters",
                    f"{self.CONNECT_path}/source/tools.py",
                )
                .load_module()
                .transform_custom_parameters
            )

            model_params_and_derived = {
                **param_connect.parameters,
                **{key: None for key in param_connect.output_derived},
            }

            final_data_np, final_names = transform_custom_parameters(
                data=data_np,
                all_param_names=final_columns,
                native_params=model_params_and_derived,
                custom_parameters=param_connect.custom_parameters,
            )

            df = pd.DataFrame(final_data_np, columns=final_names, dtype=np.float32)

        if verbose >= 2:
            print(
                f"[load_chain_data] Loaded {len(df)} samples with {len(df.columns)} parameters (model+derived) from {file_path}."
            )

        return df

    def parse_log_param(self, log_param_file, verbose=1):
        """
        Parse the log.param file to extract information about parameters:
        - Parameter type: cosmo, nuisance, derived
        - Scaling factors
        - This follows the pattern data.parameters['param'] = [init, min, max, step, scale, 'type']

        Returns:
        A dict: { param_name: {'type': str, 'scale': float}, ... }
        """
        param_info = {}
        with open(log_param_file, "r") as f:
            for line in f:
                line = line.strip()
                if "data.parameters['" in line and "=" in line:
                    # Example line:
                    # data.parameters['omega_b'] = [2.65, 1.4, 3.9, 0.05, 0.01, 'cosmo']
                    # Let's parse it:
                    # Extract param_name between data.parameters['...']
                    start = line.find("data.parameters['") + len("data.parameters['")
                    end = line.find("']", start)
                    p_name = line[start:end]

                    # Extract the array part
                    arr_str = line.split("=", 1)[1].strip()
                    arr_str = arr_str.strip("[] ")
                    parts = arr_str.split(",")
                    # parts might look like:
                    # ["2.65", "1.4", "3.9", "0.05", "0.01", "'cosmo'"]
                    parts = [p.strip() for p in parts]

                    # last element is the type string (either 'cosmo', 'nuisance', 'derived')
                    p_type = parts[-1].strip().strip("'")
                    # Scale is the second last element before type
                    # The pattern given: [init, min, max, step, scale, 'type']
                    # scale should be at index 4
                    scale = 1.0
                    if len(parts) >= 6:
                        try:
                            scale = float(parts[4])
                        except:
                            scale = 1.0
                    param_info[p_name] = {"type": p_type, "scale": scale}

        return param_info

    def find_oversampling_discarded(self, full_chain_df, reduced_chain_df, verbose=1):
        """
        Identify samples that were discarded by the oversampling filter.
        These are samples present in full_chain_df but not in reduced_chain_df.

        Parameters:
        - full_chain_df (pd.DataFrame): DataFrame containing the full chain data.
        - reduced_chain_df (pd.DataFrame): DataFrame containing the reduced chain data after filtering.
        - verbose (int): Verbosity level for logging.

        Returns:
        - discard_oversampled_df (pd.DataFrame or None): DataFrame of discarded samples or None.
        """

        if verbose >= 2:
            print(
                "[find_oversampling_discarded] Starting comparison between full and reduced chains."
            )

        # 1. Check if full_chain_df is None
        if full_chain_df is None:
            if verbose >= 2:
                print(
                    "[find_oversampling_discarded] full_chain_df is None. Returning None."
                )
            return None

        # 2. If reduced_chain_df is None, something is wrong
        if reduced_chain_df is None:
            raise ValueError(
                "[find_oversampling_discarded] reduced_chain_df is None. Cannot proceed."
            )

        # 3. Check if both DataFrames are identical
        if full_chain_df.equals(reduced_chain_df):
            if verbose >= 2:
                print(
                    "[find_oversampling_discarded] full_chain_df and reduced_chain_df are identical. No samples discarded."
                )
            return None

        # 4. Ensure consistent data types and sorting for accurate comparison
        if verbose >= 3:
            print(
                "[find_oversampling_discarded] Ensuring consistent data types and sorting for comparison."
            )

        # Sort both DataFrames by all columns to align rows
        sorted_full = full_chain_df.sort_values(
            by=full_chain_df.columns.tolist()
        ).reset_index(drop=True)
        sorted_reduced = reduced_chain_df.sort_values(
            by=reduced_chain_df.columns.tolist()
        ).reset_index(drop=True)

        # Reset indices to ensure they align
        sorted_full = sorted_full.reset_index(drop=True)
        sorted_reduced = sorted_reduced.reset_index(drop=True)

        # 5. Perform the left merge to identify rows in full_chain_df not in reduced_chain_df
        if verbose >= 3:
            print(
                "[find_oversampling_discarded] Performing left merge to find discarded samples."
            )

        merged_df = sorted_full.merge(sorted_reduced, how="left", indicator=True)

        # Select rows that are only in full_chain_df
        discarded_oversampled_df = merged_df[merged_df["_merge"] == "left_only"].drop(
            columns=["_merge"]
        )

        # 6. Check if any samples were discarded
        if discarded_oversampled_df.empty:
            if verbose >= 2:
                print(
                    "[find_oversampling_discarded] No samples were discarded by the oversampling filter."
                )
            return None

        if verbose >= 2:
            num_discarded = len(discarded_oversampled_df)
            print(
                f"[find_oversampling_discarded] Identified {num_discarded}/{len(full_chain_df)} discarded samples by the oversampling filter."
            )

        return discarded_oversampled_df

    def load_model_chain_data(self, iteration, verbose=1):
        # More or less identical to import_points_from_chains() from montepython.py

        paramnames = list(self.param_connect.parameters.keys())
        paramnames_custom = []
        try:
            paramnames_custom = list(self.param_connect.custom_parameters.keys())
        except:
            # Not implemented
            pass
        if len(paramnames_custom) > 0:
            paramnames = paramnames + paramnames_custom

        paramnames = ["100theta_s" if s == "100*theta_s" else s for s in paramnames]
        model_param_scales = []
        with open(f"{self.project_path}/number_{iteration}/log.param", "r") as f:
            lines = list(f)
        for i, name in enumerate(paramnames):
            for line in lines:
                if name in self.param_connect.log_priors:
                    if (
                        line.startswith(f"data.parameters['{name}_log10_prior']")
                        and line.split("'")[-2] == "cosmo"
                    ):
                        model_param_scales.append(
                            np.float32(
                                line.split("=")[-1].replace(" ", "").split(",")[4]
                            )
                        )
                        break
                else:
                    if (
                        line.startswith(f"data.parameters['{name}']")
                        and line.split("'")[-2] == "cosmo"
                    ):
                        model_param_scales.append(
                            np.float32(
                                line.split("=")[-1].replace(" ", "").split(",")[4]
                            )
                        )
                        break
        model_param_scales = np.array(model_param_scales)
        mp_names = []
        paramnames_file = [
            f
            for f in os.listdir(f"{self.project_path}/number_{iteration}")
            if f.endswith(".paramnames")
        ][0]
        with open(
            f"{self.project_path}/number_{iteration}/" + paramnames_file, "r"
        ) as f:
            list_f = list(f)
        for line in list_f[0 : len(paramnames)]:
            name = line.replace(" ", "").split("\t")[0]
            if (
                name[-12:] == "_log10_prior"
                and name[:-12] in self.param_connect.log_priors
            ):
                name = name[:-12]
            mp_names.append(name)

        lines = []
        i = 0
        data = []
        files = sorted(
            [
                f
                for f in os.listdir(f"{self.project_path}/number_{iteration}")
                if f.endswith(".txt") and "__" in f
            ]
        )

        for filename in files:
            with open(f"{self.project_path}/number_{iteration}/" + filename, "r") as f:
                list_f = list(f)
            lines += list_f
            for line in list_f:
                if line[0] != "#":
                    params = np.float32(
                        line.replace("\n", "").split("\t")[1 : len(paramnames) + 1]
                    )
                    for name in self.param_connect.log_priors:
                        k = mp_names.index(name)
                        params[k] = np.power(10.0, params[k])
                    params *= model_param_scales
                    data.append(
                        [params[j] for j in [mp_names.index(n) for n in paramnames]]
                    )
                    i += 1

        data = np.array(data, dtype=np.float32)

        if len(paramnames_custom) > 0:
            try:
                # Load transform_custom_parameters from tools.py in CONNECT_path/source/tools.py
                transform_custom_parameters = (
                    SourceFileLoader(
                        "transform_custom_parameters",
                        f"{self.CONNECT_path}/source/tools.py",
                    )
                    .load_module()
                    .transform_custom_parameters
                )
                data, param_names = transform_custom_parameters(
                    data,
                    paramnames,
                    self.param_connect.parameters,
                    self.param_connect.custom_parameters,
                )
            except:
                raise ImportError(
                    "transform_custom_parameters not found in CONNECT_path/source/tools.py"
                )

        else:
            param_names = list(self.param_connect.parameters.keys())

        return pd.DataFrame(data, columns=param_names)

    def find_failed_class_points(
        self, chain_model_df, model_df, model_lkl_discarded_df, exact=True, verbose=1
    ):
        """
        Identify samples that failed CLASS calculations.
        These are samples present in chain_model_df but not in model_df nor in model_lkl_discarded_df.

        Parameters:
        - chain_model_df (pd.DataFrame): DataFrame containing all chain samples (model parameters only).
        - model_df (pd.DataFrame): DataFrame containing combined accepted samples (model parameters only).
        - model_lkl_discarded_df (pd.DataFrame or None): DataFrame containing samples discarded by likelihood filter (model parameters only).
        - exact (bool): If True, perform exact matching. If False, perform approximate matching based on tolerances.
        - verbose (int): Verbosity level for logging.

        Returns:
        - failed_class_df (pd.DataFrame or None): DataFrame containing failed CLASS samples, or None if none found.
        """

        if verbose >= 2:
            print(
                "[find_failed_class_points] Starting identification of failed CLASS samples."
            )

        try:
            if exact:
                if verbose >= 2:
                    print("[find_failed_class_points] Performing exact matching.")

                # **1. Validate Essential DataFrames**
                if chain_model_df is None or model_df is None:
                    raise ValueError("chain_model_df and model_df must not be None.")

                # **2. Combine Accepted and Discarded Samples into data_iteration**
                # If model_lkl_discarded_df is None, only model_df is used
                data_iteration = (
                    pd.concat([model_df, model_lkl_discarded_df], ignore_index=True)
                    if model_lkl_discarded_df is not None
                    else model_df.copy()
                )

                if data_iteration.empty:
                    if verbose >= 2:
                        print(
                            "[find_failed_class_points] data_iteration is empty. All chain samples are considered failed."
                        )
                    return chain_model_df.copy()

                if (
                    hasattr(self.param_connect, "custom_parameters")
                    and len(list(self.param_connect.custom_parameters)) > 0
                ):
                    # mapped_to = custom_parameters[param_name]["maps_to"]
                    mapped_to_names = []
                    for param_name in self.param_connect.custom_parameters:
                        mapped_to_names += [
                            self.param_connect.custom_parameters[param_name]["maps_to"]
                        ]

                    # Copy columns from data_iteration that are mapped_to_names into a new DataFrame
                    # data_iteration_mappedto = data_iteration[mapped_to_names].copy()
                    # drop the mapped_to_names columns from data_iteration
                    data_iteration.drop(columns=mapped_to_names, inplace=True)

                    # Copy columns from chain_model_df that are mapped_to_names into a new DataFrame
                    chain_model_df_mappedto = chain_model_df[mapped_to_names].copy()
                    # drop the mapped_to_names columns from chain_model_df
                    chain_model_df.drop(columns=mapped_to_names, inplace=True)

                    # Perform the comparison
                    failed_class_df, failed_class_df_mappedto = self.compare_dataframes(
                        df1=chain_model_df,
                        df2=data_iteration,
                        df_likelihood=chain_model_df_mappedto,
                        df_likelihood2=None,
                        comparison_type="new",
                        compare_context={
                            "context": "find_failed_class_points",
                            "df1": "chain_model_df",
                            "df2": "all accepted and discarded samples",
                            "msg1": "No chain samples were provided.",
                            "msg2": "No accepted or discarded samples were provided.",
                        },
                    )

                    # Combine the failed_class_df and failed_class_df_mappedto
                    failed_class_df = pd.concat(
                        [failed_class_df, failed_class_df_mappedto], axis=1
                    )
                else:

                    failed_class_df, _ = self.compare_dataframes(
                        df1=chain_model_df,
                        df2=data_iteration,
                        df_likelihood=None,
                        df_likelihood2=None,
                        comparison_type="new",
                        compare_context={
                            "context": "find_failed_class_points",
                            "df1": "chain_model_df",
                            "df2": "all accepted and discarded samples",
                            "msg1": "No chain samples were provided.",
                            "msg2": "No accepted or discarded samples were provided.",
                        },
                    )

                # # **3. Perform Exact Matching Using Merge with Indicator**
                # # Drop duplicates to ensure accurate matching
                # merged = chain_model_df.merge(data_iteration, how="left", indicator=True)

                # # **4. Identify Failed Samples**
                # failed_class_df = merged[merged["_merge"] == "left_only"].drop(
                #     columns=["_merge"]
                # )

                if verbose >= 2:
                    print(
                        f"[find_failed_class_points] Identified {len(failed_class_df)}/{len(chain_model_df)} failed CLASS samples via exact matching."
                    )

                # **5. Return Results**
                if len(failed_class_df) == 0:
                    if verbose >= 2:
                        print(
                            "[find_failed_class_points] No failed CLASS samples found via exact matching."
                        )
                    return None

                return failed_class_df

        except Exception as e:
            if verbose >= 1:
                print(f"[find_failed_class_points] Error: {e}")
            return None

    def load_best_fit(self, iteration_path, verbose=1):
        """
        Load the best fit point data from 'best_fit_point.txt' in the given iteration path.

        Returns:
        - best_fit_data (dict): Dictionary containing:
            - "parameters": pd.DataFrame with parameter names as columns and their best fit values.
            - "likelihood": pd.DataFrame with a single column "loglkl" and its best fit value.
        If 'best_fit_point.txt' does not exist, returns {"parameters": None, "likelihood": None}
        """
        best_fit_file = os.path.join(iteration_path, "best_fit_point.txt")

        if verbose >= 2:
            print(
                f"[load_best_fit] Attempting to load best fit data from '{best_fit_file}'."
            )

        # Check if the file exists
        if not os.path.isfile(best_fit_file):
            if verbose >= 2:
                print(
                    f"[load_best_fit] File '{best_fit_file}' does not exist. This is expected if the likelihood filter was not applied."
                )
            return None

        try:
            # Use the existing load_data_file method to read the best_fit_point.txt
            df = self.load_data_file(best_fit_file, verbose=verbose)

            if df is None or df.empty:
                if verbose >= 2:
                    print(f"[load_best_fit] No data found in '{best_fit_file}'.")
                return {"parameters": None, "likelihood": None}

            if verbose >= 3:
                print(f"[load_best_fit] Loaded DataFrame:\n{df}")

            # Check if 'loglkl' column exists
            if "loglkl" not in df.columns:
                if verbose >= 2:
                    print(
                        f"[load_best_fit] 'loglkl' column not found in '{best_fit_file}'."
                    )
                return {"parameters": None, "likelihood": None}

            # Assuming there's only one best fit point, extract the first row
            best_fit_row = df.iloc[0]

            if verbose >= 3:
                print(f"[load_best_fit] Best fit row:\n{best_fit_row}")

            # Extract 'loglkl' as a separate DataFrame
            likelihood_value = best_fit_row["loglkl"]
            likelihood_df = pd.DataFrame(
                {"loglkl": [likelihood_value]}, dtype=np.float32
            )

            if verbose >= 3:
                print(f"[load_best_fit] Extracted likelihood:\n{likelihood_df}")

            # Extract parameter values by dropping 'loglkl'
            parameter_series = best_fit_row.drop(labels=["loglkl"])
            parameter_df = parameter_series.to_frame().T.astype(np.float32)

            if verbose >= 3:
                print(f"[load_best_fit] Extracted parameters:\n{parameter_df}")

            return {"parameters": parameter_df, "likelihood": likelihood_df}

        except Exception as e:
            if verbose >= 1:
                print(f"[load_best_fit] Error loading best fit data: {e}")
            return {"parameters": None, "likelihood": None}

    def debug_print_dataframe(self, name, df):
        """
        Print information about DataFrame df based on verbosity.
        """
        if self.verbose >= 3 and df is not None:
            print(
                f"[debug_print_dataframe] {name}: shape={df.shape}, columns={df.columns.tolist()}"
            )
            if self.verbose >= 4:
                # Print a few sample rows
                print(df.head())
        elif self.verbose >= 3 and df is None:
            print(f"[debug_print_dataframe] {name} is None.")

    def _detect_iterations(self):
        """
        Automatically detect iteration folders (including initial sampling) in the project path.

        Returns:
        - dict: A mapping of folder names to iteration numbers, where:
          - The initial sampling folder (e.g., "N-10000") is assigned iteration 0.
          - Subsequent folders (e.g., "number_1", "number_2") are assigned incrementing numbers.
        """
        iterations = {}
        try:
            # List all directories in the project path
            folders = [
                f
                for f in os.listdir(self.project_path)
                if os.path.isdir(os.path.join(self.project_path, f))
            ]

            # Identify the initial sampling folder (e.g., "N-10000")
            for folder in folders:
                if folder.startswith("N-"):

                    # Check if folder is complete
                    if os.path.exists(
                        os.path.join(self.project_path, folder, "model_params.txt")
                    ):
                        iterations[folder] = 0  # Initial sampling is iteration 0
                        break

            # Identify numerical iteration folders (e.g., "number_1", "number_2")
            numeric_folders = [f for f in folders if f.startswith("number_")]
            numeric_folders.sort(key=lambda x: int(x.split("_")[1]))  # Sort numerically

            # Add these to the iterations dictionary with consecutive numbers
            for i, folder in enumerate(numeric_folders, start=1):
                if os.path.exists(
                    os.path.join(self.project_path, folder, "model_params.txt")
                ):
                    iterations[folder] = i

        except Exception as e:
            raise RuntimeError(f"[__init__] Failed to detect iteration folders: {e}")

        return iterations

    def check_data_consistency(
        self,
        iteration_number,
        combined_data,
        old_data,
        new_data,
        combined_likelihood=None,
        old_likelihood=None,
        new_likelihood=None,
        data_type="DATA",
        verbose=1,
    ):
        """
        General function to check consistency between combined data, old data, and newly added data.

        Args:
            iteration_number (int): Current iteration number.
            combined_data (pd.DataFrame): Combined DataFrame of all data points.
            old_data (pd.DataFrame): DataFrame of old data points.
            new_data (pd.DataFrame): DataFrame of newly added data points.
            combined_likelihood (pd.DataFrame, optional): Combined likelihood DataFrame.
            old_likelihood (pd.DataFrame, optional): Old likelihood DataFrame.
            new_likelihood (pd.DataFrame, optional): New likelihood DataFrame.
            data_type (str): Type of data being checked (e.g., "ACCEPTED", "DISCARDED").
            verbose (int): Verbosity level.

        Returns:
            None
        """
        if verbose >= 2:
            print(f"\n[run] --- Checking {data_type} Data Consistency ---")

        # Short-circuit if no combined data
        if combined_data is None:
            if verbose >= 1:
                print(
                    f"[run] Iteration {iteration_number}: No combined data found for {data_type}; skipping checks."
                )
            return

        # 1) Basic length checks
        length_combined = len(combined_data) if combined_data is not None else 0
        length_old = len(old_data) if old_data is not None else 0
        length_new = len(new_data) if new_data is not None else 0

        # Combine old & new into a single DataFrame
        frames = []
        if old_data is not None:
            frames.append(old_data)
        if new_data is not None:
            frames.append(new_data)

        combined_df = pd.concat(frames, ignore_index=True) if frames else None

        # 2) Check if lengths match
        if (length_old + length_new) != length_combined:
            if verbose >= 1:
                print(
                    f"[run] Iteration {iteration_number}: WARNING - mismatch in total # of {data_type} rows!"
                )
                print(f"    Combined: {length_combined}")
                print(f"    Old:      {length_old}")
                print(f"    New:      {length_new}")
                print(f"    Difference: {length_combined - (length_old + length_new)}")
        else:
            # 3) If counts match, check row-by-row equality (unordered)
            if combined_df is not None and not combined_df.empty:
                # Reorder columns for a fair comparison
                if set(combined_df.columns) == set(combined_data.columns):
                    sorted_cols = sorted(combined_df.columns)
                    dfA = (
                        combined_df[sorted_cols]
                        .sort_values(by=sorted_cols)
                        .reset_index(drop=True)
                    )
                    dfB = (
                        combined_data[sorted_cols]
                        .sort_values(by=sorted_cols)
                        .reset_index(drop=True)
                    )

                    if not len(dfA) == len(dfB):
                        if verbose >= 1:
                            print(
                                f"[run] Iteration {iteration_number}: WARNING - mismatch in row counts between old+new vs. combined {data_type}."
                            )
                    else:
                        if not dfA.equals(dfB):
                            if verbose >= 1:
                                print(
                                    f"[run] Iteration {iteration_number}: WARNING - old+new {data_type} points differ from the combined data."
                                )
                                print(
                                    f"This is completely unexpected and may indicate a problem with this data loader class."
                                )

                            # Find mismatched rows
                            mismatched = pd.merge(dfA, dfB, how="outer", indicator=True)
                            only_in_A = mismatched[mismatched["_merge"] == "left_only"]
                            only_in_B = mismatched[mismatched["_merge"] == "right_only"]

                            # Summarize mismatches
                            print(f"    Rows only in old+new: {len(only_in_A)}")
                            print(f"    Rows only in combined: {len(only_in_B)}")

                            if verbose >= 2:
                                print("\n    Examples of mismatched rows (old+new):")
                                print(only_in_A.head())
                                print("\n    Examples of mismatched rows (combined):")
                                print(only_in_B.head())
                        else:
                            if verbose >= 2:
                                print(
                                    f"[run] Iteration {iteration_number}: old+new {data_type} points perfectly match combined data."
                                )
                else:
                    if verbose >= 1:
                        print(
                            f"[run] Iteration {iteration_number}: WARNING - Column sets differ between old+new vs. combined {data_type}."
                        )
                        print(f"    Old+New columns: {sorted(combined_df.columns)}")
                        print(f"    Combined columns: {sorted(combined_data.columns)}")
            else:
                if length_combined > 0 and (length_old > 0 or length_new > 0):
                    if verbose >= 1:
                        print(
                            f"[run] Iteration {iteration_number}: WARNING - combined old+new is empty but we expected rows."
                        )

        # 4) Check likelihood data counts if present
        if combined_likelihood is not None:
            length_combined_likelihood = len(combined_likelihood)
            length_old_likelihood = (
                len(old_likelihood) if old_likelihood is not None else 0
            )
            length_new_likelihood = (
                len(new_likelihood) if new_likelihood is not None else 0
            )

            # Combine old & new likelihood into a single DataFrame
            frames_likelihood = []
            if old_likelihood is not None:
                frames_likelihood.append(old_likelihood)
            if new_likelihood is not None:
                frames_likelihood.append(new_likelihood)

            combined_likelihood_df = (
                pd.concat(frames_likelihood, ignore_index=True)
                if frames_likelihood
                else None
            )

            if length_combined_likelihood != (
                length_old_likelihood + length_new_likelihood
            ):
                if verbose >= 1:
                    print(
                        f"[run] Iteration {iteration_number}: WARNING - mismatch in likelihood {data_type} counts!"
                    )
                    print(f"    Combined: {length_combined_likelihood}")
                    print(f"    Old:      {length_old_likelihood}")
                    print(f"    New:      {length_new_likelihood}")
                    print(
                        f"    Difference: {length_combined_likelihood - (length_old_likelihood + length_new_likelihood)}"
                    )
            else:
                if (
                    combined_likelihood_df is not None
                    and not combined_likelihood_df.empty
                ):
                    if set(combined_likelihood_df.columns) == set(
                        combined_likelihood.columns
                    ):
                        sorted_cols_lkl = sorted(combined_likelihood_df.columns)
                        dfA_lkl = (
                            combined_likelihood_df[sorted_cols_lkl]
                            .sort_values(by=sorted_cols_lkl)
                            .reset_index(drop=True)
                        )
                        dfB_lkl = (
                            combined_likelihood[sorted_cols_lkl]
                            .sort_values(by=sorted_cols_lkl)
                            .reset_index(drop=True)
                        )

                        if not len(dfA_lkl) == len(dfB_lkl):
                            if verbose >= 1:
                                print(
                                    f"[run] Iteration {iteration_number}: WARNING - mismatch in row counts between old+new likelihood values vs. combined likelihood values."
                                )

                        elif not dfA_lkl.equals(dfB_lkl):
                            if verbose >= 1:
                                print(
                                    f"[run] Iteration {iteration_number}: WARNING - old+new likelihood {data_type} points differ from the combined likelihood."
                                )
                                print(
                                    f"This is completely unexpected and may indicate a problem with this data loader class."
                                )

                            mismatched_lkl = pd.merge(
                                dfA_lkl, dfB_lkl, how="outer", indicator=True
                            )
                            only_in_A_lkl = mismatched_lkl[
                                mismatched_lkl["_merge"] == "left_only"
                            ]
                            only_in_B_lkl = mismatched_lkl[
                                mismatched_lkl["_merge"] == "right_only"
                            ]

                            print(
                                f"    Rows only in old+new likelihood: {len(only_in_A_lkl)}"
                            )
                            print(
                                f"    Rows only in combined likelihood: {len(only_in_B_lkl)}"
                            )

                            if verbose >= 1:
                                print(
                                    "\n    Examples of mismatched rows (old+new likelihood):"
                                )
                                print(only_in_A_lkl.head())
                                print(
                                    "\n    Examples of mismatched rows (combined likelihood):"
                                )
                                print(only_in_B_lkl.head())
                        else:
                            if verbose >= 2:
                                print(
                                    f"[run] Iteration {iteration_number}: old+new likelihood {data_type} points perfectly match combined likelihood."
                                )
                    else:
                        if verbose >= 1:
                            print(
                                f"[run] Iteration {iteration_number}: WARNING - Column sets differ between old+new likelihood vs. combined likelihood."
                            )
                            print(
                                f"    Old+New likelihood columns: {sorted(combined_likelihood_df.columns)}"
                            )
                            print(
                                f"    Combined likelihood columns: {sorted(combined_likelihood.columns)}"
                            )

        # 5) Check overlap between old and new data samples
        if old_data is not None and new_data is not None:
            if set(old_data.columns) == set(new_data.columns):
                sorted_cols_overlap = sorted(old_data.columns)
                old_sorted = (
                    old_data[sorted_cols_overlap]
                    .sort_values(by=sorted_cols_overlap)
                    .reset_index(drop=True)
                )
                new_sorted = (
                    new_data[sorted_cols_overlap]
                    .sort_values(by=sorted_cols_overlap)
                    .reset_index(drop=True)
                )

                # Find overlapping rows
                overlap = pd.merge(old_sorted, new_sorted, how="inner")
                num_overlap = len(overlap)

                if num_overlap > 0:
                    if verbose >= 1:
                        print(
                            f"[run] Iteration {iteration_number}: WARNING - overlap detected between old and new {data_type} samples.\nThis happens if two different samples happen to be identical. But it should NOT happen and may indicate an error."
                        )
                        (
                            print(
                                f"This could theoretically happen if a newly generated accepted sample from the MCMC chain happens to be identical to an old sample still considered accepted. But the oversampling-filter should prevent this."
                            )
                            if data_type == "(ACCEPTED)"
                            else None
                        )
                        (
                            print(
                                f"This could theoretically happen if a newly generated sample from the MCMC chain that was discarded by the likelihood-filter happens to be identical to an previous accepted sample also discarded by the likelihood-filter. But the oversampling-filter should prevent this."
                            )
                            if data_type == "[DISCARDED (LIKELIHOOD-FILTER)]"
                            else None
                        )
                        print(f"    Number of overlapping samples: {num_overlap}")
                        if verbose >= 1:
                            print("\n    Examples of overlapping samples:")
                            pd.set_option(
                                "display.max_rows", None
                            )  # None means no limit
                            pd.set_option(
                                "display.max_columns", None
                            )  # Show all columns
                            pd.set_option("display.width", None)  # Don't wrap lines
                            pd.set_option("display.max_colwidth", None)
                            print(overlap.head())
                else:
                    if verbose >= 2:
                        print(
                            f"[run] Iteration {iteration_number}: No overlap found between old and new {data_type} samples."
                        )
            else:
                if verbose >= 1:
                    print(
                        f"[run] Iteration {iteration_number}: WARNING - Column sets differ between old and new {data_type} samples; overlap check skipped."
                    )
                    print(f"    Old columns: {sorted(old_data.columns)}")
                    print(f"    New columns: {sorted(new_data.columns)}")

        # 6) Check likelihood values for overlap between old and new data
        if old_likelihood is not None and new_likelihood is not None:
            if set(old_likelihood.columns) == set(new_likelihood.columns):
                sorted_cols_lkl_overlap = sorted(old_likelihood.columns)
                old_lkl_sorted = (
                    old_likelihood[sorted_cols_lkl_overlap]
                    .sort_values(by=sorted_cols_lkl_overlap)
                    .reset_index(drop=True)
                )
                new_lkl_sorted = (
                    new_likelihood[sorted_cols_lkl_overlap]
                    .sort_values(by=sorted_cols_lkl_overlap)
                    .reset_index(drop=True)
                )

                # Find overlapping likelihood rows
                likelihood_overlap = pd.merge(
                    old_lkl_sorted, new_lkl_sorted, how="inner"
                )
                num_likelihood_overlap = len(likelihood_overlap)

                if num_likelihood_overlap > 0:
                    if verbose >= 1:
                        print(
                            f"[run] Iteration {iteration_number}: (WARNING) - overlap detected between old and new {data_type} likelihood values. \n This can theoretically happen if two different samples happen to have the same likelihood value.\n But if many samples overlap, it may be because of an underlying error in this Dataloader class or the likelihood calculation."
                        )
                        print(
                            f"    Number of overlapping likelihood values: {num_likelihood_overlap}"
                        )
                        if verbose >= 1:
                            print("\n    Some of the overlapping likelihood values:")
                            pd.set_option(
                                "display.max_rows", None
                            )  # None means no limit
                            pd.set_option(
                                "display.max_columns", None
                            )  # Show all columns
                            pd.set_option("display.width", None)  # Don't wrap lines
                            pd.set_option("display.max_colwidth", None)
                            print(likelihood_overlap.head())
                            print(
                                f"    (Note: Only showing the first few overlapping rows.)"
                            )
                            print(
                                f"    (Note: If the -log(lkl) values are 1e30, this is likely due to the likelihood calculator returning 1e30 for failed calculations. This is fine if the number of such samples is small.)"
                            )
                            print(
                                f"\n    The cosmological parameters for these samples are:"
                            )
                            # Find indices in old_likelihood and new_likelihood that has the same likelihood value as the first rows in likelihood_overlap
                            # Define which columns hold the likelihood values.

                            # (Assuming all columns in old_likelihood/new_likelihood represent the likelihoods)
                            likelihood_cols = list(old_likelihood.columns)

                            # Use up to the first 5 overlapping samples
                            overlap_samples = likelihood_overlap.head(5)

                            for i, sample in overlap_samples.iterrows():
                                # Build a boolean mask for rows in old_likelihood that match all likelihood values in the sample
                                mask_old = (
                                    old_likelihood[likelihood_cols]
                                    == sample[likelihood_cols]
                                ).all(axis=1)
                                # Do the same for new_likelihood
                                mask_new = (
                                    new_likelihood[likelihood_cols]
                                    == sample[likelihood_cols]
                                ).all(axis=1)

                                # Get the indices in the original unsorted likelihood DataFrames
                                indices_old = old_likelihood.index[mask_old].tolist()
                                indices_new = new_likelihood.index[mask_new].tolist()

                                # Use these indices to extract the corresponding rows (cosmological parameters)
                                cosmo_old = old_data.loc[indices_old]
                                cosmo_new = new_data.loc[indices_new]

                                print("Corresponding rows in old_data:")
                                print(
                                    cosmo_old.head()
                                )  # head() in case there are multiple matches
                                print("Corresponding rows in new_data:")
                                print(cosmo_new.head())

                else:
                    if verbose >= 2:
                        print(
                            f"[run] Iteration {iteration_number}: No overlap found between old and new {data_type} likelihood values."
                        )
            else:
                if verbose >= 1:
                    print(
                        f"[run] Iteration {iteration_number}: WARNING - Column sets differ between old and new {data_type} likelihood values; overlap check skipped."
                    )
                    print(
                        f"    Old likelihood columns: {sorted(old_likelihood.columns)}"
                    )
                    print(
                        f"    New likelihood columns: {sorted(new_likelihood.columns)}"
                    )

        # Check that the lengths of the likelihoods match the lengths of the data
        if combined_likelihood is not None and combined_data is not None:
            if len(combined_likelihood) != len(combined_data):
                if verbose >= 1:
                    print(
                        f"[run] Iteration {iteration_number}: WARNING - mismatch in number of {data_type} likelihood values and {data_type} data points."
                    )
                    print(
                        f"    Number of {data_type} likelihood values: {len(combined_likelihood)}"
                    )
                    print(
                        f"    Number of {data_type} data points: {len(combined_data)}"
                    )
        if old_likelihood is not None and old_data is not None:
            if len(old_likelihood) != len(old_data):
                if verbose >= 1:
                    print(
                        f"[run] Iteration {iteration_number}: WARNING - mismatch in number of old {data_type} likelihood values and old {data_type} data points."
                    )
                    print(
                        f"    Number of old {data_type} likelihood values: {len(old_likelihood)}"
                    )
                    print(f"    Number of old {data_type} data points: {len(old_data)}")

        if new_likelihood is not None and new_data is not None:
            if len(new_likelihood) != len(new_data):
                if verbose >= 1:
                    print(
                        f"[run] Iteration {iteration_number}: WARNING - mismatch in number of new {data_type} likelihood values and new {data_type} data points."
                    )
                    print(
                        f"    Number of new {data_type} likelihood values: {len(new_likelihood)}"
                    )
                    print(f"    Number of new {data_type} data points: {len(new_data)}")

        if verbose >= 2:
            print(f"[run] --- Finished Checking {data_type} Data Consistency ---\n")


# ---------------------------------CLASS Analyze_likelihoods---------------------------------#
# This class is used to create a univariate analysis of the likelihood values
# and delta chi-squared values for the accepted and discarded points in each iteration.
# It creates outputs such as histograms, box plots, and summary statistics for each iteration.


class Analyze_likelihoods:
    """
    Perform univariate analysis of likelihood and delta chi² values across iterations.

    This class processes the training data produced by CONNECT and computes a series of
    univariate statistics and visualizations for each iteration. In particular, it:

      - Extracts likelihood values (typically the 'true_loglkl') from various data categories,
        such as accepted and discarded points.
      - Computes the delta chi² values for each data category using the best-fit log-likelihood
        from the corresponding iteration.
      - Aggregates cumulative quantities (e.g. cumulative discarded likelihood) across iterations.
      - Computes extended summary statistics (minimum, maximum, mean, median, standard deviation,
        and specified quantiles) for each iteration and category.
      - Generates visual outputs including:
          * Histograms (using Seaborn’s histplot) and boxplots (with overlaid strip plots) for both
            the likelihood and delta chi² distributions.
          * A plot showing the evolution of key metrics (e.g. best-fit likelihood, average likelihood
            for newly accepted points, the ratio of discarded to accepted points, and optionally the
            fraction of newly accepted points above a given delta chi² threshold) across iterations.
      - Saves the analysis outputs in multiple file formats (e.g., PDF and PNG) as well as summary tables
        in CSV, LaTeX, and Markdown formats.

    Parameters
    ----------
    project_path : str
        Path to the CONNECT project directory containing the iteration data.
    output_path : str
        Folder where the generated analysis outputs (plots and summary files) will be saved.
    param_connect : object
        An instance containing model parameter settings and thresholds (e.g., delta chi² threshold).
    data : dict
        A dictionary with key "iteration_data" holding the data for each iteration. Each iteration’s
        data should itself be a dictionary with keys such as:
            - 'accepted_accumulated'
            - 'cumulative_discarded_likelihood'
            - 'accepted_new'
            - 'accepted_old'
            - 'discarded_likelihood_new'
            - 'discarded_likelihood_old'
            - 'discarded_likelihood'
            - 'discarded_iteration'

        Each of these sub-dictionaries is expected to have keys "parameters" (a DataFrame of model and derived
        parameters) and, where applicable, "likelihood_data" (a DataFrame containing likelihood values).
    data_categories : list, optional
        List of data keys (categories) to analyze. If None, defaults to:
            ['accepted_accumulated', 'cumulative_discarded_likelihood', 'accepted_new', 'accepted_old',
                'discarded_likelihood_new', 'discarded_likelihood_old', 'discarded_likelihood', 'discarded_iteration']
    category_labels : dict, optional
        A dictionary mapping data category keys to human‐readable labels for use in plot legends.
        If None, default labels are used.
    verbose : int, optional
        Verbosity level (e.g., 0 for quiet up to higher levels for more detailed logs).
    data_colors : dict, optional
        A dictionary mapping data category keys to color names. If not provided, a default palette is used.
    include_inset : bool, optional
        If True, includes an inset histogram (typically showing cumulative data) within each plot.
    hist_main_panel : list, optional
        A list of dictionaries specifying configuration options for the histograms in the main panel.
        Each dictionary should include:
          - "category": the data key to plot,
          - "label": custom label (or None to use the default),
          - "color": custom color (or None to use the default),
          - "plot_kws": a dictionary of plotting keyword arguments.
    hist_inset : list, optional
        Similar to `hist_main_panel` but for the inset panel plots.
    x_range, y_range : list or tuple, optional
        Explicit axis ranges for the main histograms (e.g., [min, max]). If not provided, the ranges
        are computed from the data.
    x_range_inset, y_range_inset : list or tuple, optional
        Explicit axis ranges for the inset histograms.
    save_formats : list, optional
        A list of strings specifying the file formats in which to save the generated plots (e.g., ['pdf', 'png']).

    Returns
    -------
    None
        All output files (plots and summary tables) are saved to the directory specified by output_path.

    Usage
    -----
    >>> analyzer = Analyze_likelihoods(
            project_path="/path/to/project",
            output_path="/path/to/save/analysis",
            param_connect=my_param_connect_instance,
            data=loaded_data,
            verbose=2
        )
    >>> analyzer.run_analysis()

    Additional Details
    ------------------
    - The class first validates the input parameters (e.g., checking that the project and output paths exist,
      that data is provided in the expected dictionary format, and that numeric ranges are specified correctly).
    - It computes the delta chi² values for each data category as:
          delta_chi_sq = 2.0 * (true_loglkl - best_fit_loglkl)
      where best_fit_loglkl is obtained from the 'best_fit' key for each iteration.
    - The method _gather_iteration_data() dynamically constructs a dictionary of arrays containing likelihood
      and delta chi² values for each iteration and category.
    - The generated plots include both per-iteration histograms/boxplots and a summary figure showing the evolution
      of key metrics across iterations.
    - Summary statistics (including specified quantiles) are computed for each iteration and saved in CSV, LaTeX,
      and Markdown formats.
    """

    def __init__(
        self,
        project_path,
        output_path,
        param_connect,
        data,
        data_categories=None,
        category_labels=None,
        verbose=1,
        data_colors=None,
        include_inset=True,
        hist_main_panel=None,
        hist_inset=None,
        x_range=None,
        y_range=None,
        x_range_inset=None,
        y_range_inset=None,
        y_range_box=None,
        save_formats=["pdf", "png"],
        max_delta_chi2_in_display=None,
        max_loglkl_in_display=None,
        # What would be a good parameter for choosing what to analyze, i.e. what plots to generate?
        # E.g. a list of booleans for each type of plot? choose a good parameter name:
        create_these_outputs="all",
    ):
        """
        Parameters
        ----------
        project_path : str
            Path to the CONNECT project directory.
        output_path : str
            Path to the directory where analysis outputs will be saved.
        param_connect : object
            The param_connect instance (for threshold, etc.).
        data : dict
            The dictionary containing 'iteration_data' for each iteration.
        data_categories : list or None
            A list of data keys to analyze, e.g.:
                [
                  'accepted_accumulated', 'accepted_new', 'accepted_old',
                  'discarded_iteration', 'discarded_oversampling',
                  'discarded_likelihood', 'discarded_likelihood_new', 'discarded_likelihood_old',
                  'total'
                ]
            If None, defaults to all the above categories.
        verbose : int
            Verbosity level.
        """

        # Check that project_path is a string and that the directory exists
        if not isinstance(project_path, str):
            raise TypeError("project_path must be a string.")
        if not os.path.isdir(project_path):
            raise ValueError(
                f"project_path '{project_path}' does not exist or is not a directory."
            )

        # Check that output_path is a string
        if not isinstance(output_path, str):
            raise TypeError("output_path must be a string.")

        # Check param_connect: here we only check for non-None; you might want to check for a specific attribute
        if param_connect is None:
            raise ValueError("param_connect cannot be None.")

        # Check that data is a dictionary and contains 'iteration_data'
        if not isinstance(data, dict):
            raise TypeError("data must be a dictionary.")
        if "iteration_data" not in data:
            raise ValueError("data dictionary must contain the key 'iteration_data'.")

        # Optionally, check that data['iteration_data'] is a dict
        if not isinstance(data.get("iteration_data"), dict):
            raise TypeError("data['iteration_data'] must be a dictionary.")

        # Check that verbose is an int
        if not isinstance(verbose, int):
            raise TypeError("verbose must be an integer.")

        # Check save_formats is a list of strings
        if not isinstance(save_formats, list):
            raise TypeError("save_formats must be a list of strings.")
        for fmt in save_formats:
            if not isinstance(fmt, str):
                raise TypeError("Each element in save_formats must be a string.")

        # Validate x_range, y_range, x_range_inset, and y_range_inset if provided
        for rng, name in zip(
            [x_range, y_range, x_range_inset, y_range_inset],
            ["x_range", "y_range", "x_range_inset", "y_range_inset"],
        ):
            if rng is not None:
                if not (isinstance(rng, (list, tuple)) and len(rng) == 2):
                    raise ValueError(
                        f"{name} must be a list or tuple of two numbers (min and max)."
                    )
                # Optionally, check numeric types:
                if not all(isinstance(x, (int, float)) for x in rng):
                    raise TypeError(f"Both values in {name} must be numeric.")

        # If data_categories is provided, ensure it is a list
        if data_categories is not None and not isinstance(data_categories, list):
            raise TypeError("data_categories must be a list if provided.")

        # If category_labels is provided, ensure it is a dict
        if category_labels is not None and not isinstance(category_labels, dict):
            raise TypeError("category_labels must be a dictionary if provided.")

        # If data_colors is provided, ensure it is a dict
        if data_colors is not None and not isinstance(data_colors, dict):
            raise TypeError("data_colors must be a dictionary if provided.")

        self.project_path = project_path
        self.output_path = output_path
        self.param_connect = param_connect
        self.data = data
        self.verbose = verbose
        self.save_formats = save_formats
        self.x_range = x_range
        self.y_range = y_range
        self.x_range_inset = x_range_inset
        self.y_range_inset = y_range_inset
        self.y_range_box = y_range_box
        self.max_delta_chi2_in_display = max_delta_chi2_in_display
        self.max_loglkl_in_display = max_loglkl_in_display
        self.create_these_outputs = create_these_outputs

        # Create a directory to store analysis reports
        self.analysis_dir = os.path.join(self.output_path, "likelihood_analysis")
        os.makedirs(self.analysis_dir, exist_ok=True)

        # Default categories if user doesn't supply them
        if data_categories is None:
            self.data_categories = [
                # Cumulative Overview
                "accepted_accumulated",
                "cumulative_discarded_likelihood",
                # Iteration-Specific Insights
                "accepted_new",
                "accepted_old",
                "discarded_likelihood_new",
                "discarded_likelihood_old",
                "discarded_likelihood",
                # Non-Likelihood-filter-Based Discards
                "discarded_iteration",
            ]
        else:
            self.data_categories = data_categories

        if category_labels is None:

            self.category_labels = {
                "accepted_accumulated": "Cumulative Accepted",  # All points accepted across all iterations.
                "cumulative_discarded_likelihood": "Cumulative Discarded",  # All points discarded by the likelihood filter across all iterations.
                "accepted_new": "Newly Accepted",  # Points accepted in the current iteration.
                "accepted_old": "Previously Accepted",  # Points accepted in earlier iterations.
                "discarded_likelihood_new": "Newly generated & Discarded",  # Points generated this iteration and discarded immediately by the likelihood filter.
                "discarded_likelihood_old": "Previously accepted, now discarded",  # Points previously accepted but discarded this iteration by the likelihood filter.
                "discarded_likelihood": "Total Discarded (current iter)",  # All points discarded this iteration by the likelihood filter.
                "discarded_iteration": "Discarded (Iteration)",  # Points discarded for non-likelihood-related reasons in the current iteration.
            }

            """
            Cumulative Categories:
            - accepted_accumulated: Current iterations training data. It is all the points that have survived the likelihood filter up to this iteration and remains accepted.
            - cumulative_discarded_likelihood: Points discarded by the likelihood filter across all iterations, giving a cumulative view of how the filter behaves over time.
                It is thus all points that have been discarded by the likelihood filter up to this iteration.
            
            Iteration-Specific Categories:
            - accepted_new: Newly generated points by the MCMC smampler, that got accepted and survived all filters. Useful for understanding what gets added to the accepted pool each iteration.
            - accepted_old: Points that were considered accepted in the previous iteration and remain accepted in the current iteration.
                I.e. accepted points from the previous iteration that survived the likelihood filter again and remain accepted in the current iteration.
        
            Likelihood-Based Discards:
            - discarded_likelihood_new: Points generated this iteration and immediately discarded by the likelihood filter in the current iteration.
            - discarded_likelihood_old: Points that were accepted in the previous iteration, but got discarded by the likelihood filter in the current iteration.
            - discarded_likelihood: Total points discarded by the likelihood filter in the current iteration (sum of discarded_likelihood_new and discarded_likelihood_old).
                Not to be confused with cumulative_discarded_likelihood, which is the sum of all discarded points across all iterations.
            
            Non-Likelihood-filter Discards:
            - discarded_iteration: Points discarded for other reasons in the current iteration. If the entire batch was discarded by default (keep_first_iteration=False), this will be the entire batch.
                    
            """

        else:
            self.category_labels = category_labels

        # Define quantiles for extended statistics
        self.quantiles = [0.05, 0.25, 0.5, 0.75, 0.95]

        # Define colors for each data category (tweak to your taste):
        # - Greens for accepted, Reds/Oranges for discards, Blue for total, etc.

        lightred = sns.color_palette("husl", 9, desat=0.8)[0]

        if data_colors is not None:
            self.DATA_COLORS = data_colors
        else:
            self.DATA_COLORS = {
                "accepted_accumulated": "tab:green",  # Cumulative Accepted Points (base green)
                "cumulative_discarded_likelihood": "tab:red",  # Cumulative Discarded by Likelihood-filter (base red)
                "accepted_new": "lightgreen",  # Lighter green for newly accepted
                "accepted_old": "darkgreen",  # Darker green for previously accepted
                "discarded_likelihood_new": lightred,  # Lighter red for newly generated and discarded
                "discarded_likelihood_old": "crimson",  # Darker red for previously accepted but discarded
                "discarded_likelihood": "darkred",  # Strong red for total discards in this iteration
                "discarded_iteration": "tab:blue",  # Blue for iteration-based discards
            }

        # -- NEW: Store the histogram config parameters --
        self.include_inset = include_inset

        global_plot_kws = {
            "stat": "count",
            "alpha": 1,
            "fill": True,
            "element": "bars",
            "multiple": "stack",
            "log_scale": True,
            "common_bins": True,
            "common_norm": True,
        }

        # Default main panel config if none provided
        if hist_main_panel is None:
            #  Typically want to see iteration-level acceptance vs. discards
            hist_main_panel = [
                {
                    "category": "accepted_old",
                    "label": None,
                    "color": None,
                    "plot_kws": (
                        global_plot_kws
                        if global_plot_kws is not None
                        else {
                            "stat": "count",
                            "alpha": 1,
                            "fill": True,
                            "element": "bars",
                            "multiple": "stack",
                            "log_scale": True,
                            "common_bins": True,
                            "common_norm": True,
                        }
                    ),
                },
                {
                    "category": "accepted_new",
                    "label": None,  # fallback to self.category_labels
                    "color": None,  # fallback to self.DATA_COLORS
                    "plot_kws": (
                        global_plot_kws
                        if global_plot_kws is not None
                        else {
                            "stat": "count",
                            "alpha": 1,
                            "fill": True,
                            "element": "bars",
                            "multiple": "stack",
                            "log_scale": True,
                            "common_bins": True,
                            "common_norm": True,
                        }
                    ),
                },
                {
                    "category": "discarded_likelihood_new",
                    "label": None,
                    "color": None,
                    "plot_kws": (
                        global_plot_kws
                        if global_plot_kws is not None
                        else {
                            "stat": "count",
                            "alpha": 1,
                            "fill": True,
                            "element": "bars",
                            "multiple": "stack",
                            "log_scale": True,
                            "common_bins": True,
                            "common_norm": True,
                        }
                    ),
                },
                {
                    "category": "discarded_likelihood_old",
                    "label": None,
                    "color": None,
                    "plot_kws": (
                        global_plot_kws
                        if global_plot_kws is not None
                        else {
                            "stat": "count",
                            "alpha": 1,
                            "fill": True,
                            "element": "bars",
                            "multiple": "stack",
                            "log_scale": True,
                            "common_bins": True,
                            "common_norm": True,
                        }
                    ),
                },
                {
                    "category": "discarded_iteration",
                    "label": None,
                    "color": None,
                    "plot_kws": (
                        global_plot_kws
                        if global_plot_kws is not None
                        else {
                            "stat": "count",
                            "alpha": 1,
                            "fill": True,
                            "element": "bars",
                            "multiple": "stack",
                            "log_scale": True,
                            "common_bins": True,
                            "common_norm": True,
                        }
                    ),
                },
            ]
        self.hist_main_panel = hist_main_panel

        # Default inset config if none provided
        if hist_inset is None:
            # Typically want to see the cumulative sets in the inset
            hist_inset = [
                {
                    "category": "accepted_accumulated",
                    "label": None,
                    "color": None,
                    "plot_kws": (
                        global_plot_kws
                        if global_plot_kws is not None
                        else {
                            "stat": "count",
                            "alpha": 1,
                            "fill": True,
                            "element": "bars",
                            "multiple": "stack",
                            "log_scale": True,
                            "common_bins": True,
                            "common_norm": True,
                        }
                    ),
                },
                {
                    "category": "cumulative_discarded_likelihood",
                    "label": None,
                    "color": None,
                    "plot_kws": (
                        global_plot_kws
                        if global_plot_kws is not None
                        else {
                            "stat": "count",
                            "alpha": 1,
                            "fill": True,
                            "element": "bars",
                            "multiple": "stack",
                            "log_scale": True,
                            "common_bins": True,
                        }
                    ),
                },
            ]
        self.hist_inset = hist_inset

    def run_analysis(self):
        """Orchestrate all analysis steps."""
        if self.verbose >= 1:
            print("[run_analysis] Starting likelihood analysis.")

        if not self._check_likelihood_availability():
            if self.verbose >= 1:
                print("[run_analysis] No likelihood data found. Skipping analysis.")
            return

        # Gather iteration-wise data (loglkl and Δχ²)
        iteration_data = self._gather_iteration_data()
        iteration_data_chain = self._gather_iteration_data_chain()

        # Check if create_these_outputs is set to 'all' or if the 'summary' key is True in create_these_outputs
        if (
            self.create_these_outputs == "all"
            or self.create_these_outputs["summary_tables"] == True
        ):
            # Generate iteration-wise summaries and plots
            self._save_iteration_summaries(iteration_data)

        matplotlib.use("Agg")
        # Boxplots for delta_chi_sq and loglkl
        if (
            self.create_these_outputs == "all"
            or self.create_these_outputs["boxplots"] == True
        ):
            self._plot_per_iteration_boxplot(iteration_data, metric="delta_chi_sq")
            self._plot_per_iteration_boxplot(iteration_data, metric="loglkl")

        matplotlib.use("Agg")
        # Histograms
        if (
            self.create_these_outputs == "all"
            or self.create_these_outputs["histograms"] == True
        ):
            for stat_type in ["count"]:  # , 'density']: not supported anymore

                self._plot_per_iteration_hist(
                    iteration_data,
                    iteration_data_chain=iteration_data_chain,
                    metric="delta_chi_sq",
                    stat=stat_type,
                )

                self._plot_per_iteration_hist(
                    iteration_data,
                    iteration_data_chain=iteration_data_chain,
                    metric="loglkl",
                    stat=stat_type,
                )

        matplotlib.use("Agg")
        # Likelihood evolution plots
        if (
            self.create_these_outputs == "all"
            or self.create_these_outputs["evolution_plot"] == True
        ):
            self._plot_likelihood_evolution(iteration_data)

        if self.verbose >= 1:
            print(
                "[run_analysis] Likelihood analysis complete. Check 'likelihood_analysis' directory for outputs."
            )

    def _check_likelihood_availability(self):
        """Check if any iteration has any recognized likelihood data for analysis."""
        for iteration, content in self.data["iteration_data"].items():
            for cat in self.data_categories:
                lkl_df = content.get(cat, {}).get("likelihood_data", None)
                if lkl_df is not None and not lkl_df.empty:
                    return True
        return False

    def _gather_iteration_data(self):
        """
        Dynamically gather loglkl arrays for each category in self.data_categories,
        and compute delta_chi_sq with the iteration's best-fit loglkl (if available).
        Includes cumulative discarded likelihood and delta_chi_sq calculations.

        Returns
        -------
        iteration_data : dict
            iteration_data[iteration][category] = array of loglkl
            iteration_data[iteration]['delta_chi_sq_{category}'] = array of delta chi^2
            iteration_data[iteration]['best_fit_loglkl'] = float or None
        """

        last_completed_it = self._find_last_complete_iteration()
        self.count_points = {}  # Count the orginal number of points in each category
        # Initialize the count_points dictionary
        for iteration in self.data["iteration_data"]:
            self.count_points[iteration] = {}
            for cat in self.data_categories:
                self.count_points[iteration][cat] = {}
                self.count_points[iteration][cat]["original"] = 0
                self.count_points[iteration][cat]["display"] = None
                self.count_points[iteration][f"delta_chi_sq_{cat}"] = {}
                self.count_points[iteration][f"delta_chi_sq_{cat}"]["original"] = 0
                self.count_points[iteration][f"delta_chi_sq_{cat}"]["display"] = None
            self.count_points[iteration]["cumulative_discarded_likelihood"] = {}
            self.count_points[iteration]["cumulative_discarded_likelihood"][
                "original"
            ] = 0
            self.count_points[iteration]["cumulative_discarded_likelihood"][
                "display"
            ] = None
            self.count_points[iteration][
                "delta_chi_sq_cumulative_discarded_likelihood"
            ] = {}
            self.count_points[iteration][
                "delta_chi_sq_cumulative_discarded_likelihood"
            ]["original"] = 0
            self.count_points[iteration][
                "delta_chi_sq_cumulative_discarded_likelihood"
            ]["display"] = None

        iteration_data = {}
        non_empty_categories = {
            cat: False for cat in self.data_categories
        }  # Track whether each category is non-empty

        # Initialize cumulative containers
        cumulative_discarded_likelihood = np.array([], dtype=np.float32)

        # Loop over each iteration in sorted order
        for iteration, content in sorted(self.data["iteration_data"].items()):

            # Skip incomplete iterations
            if iteration > last_completed_it:
                continue
            # 1) Identify the best-fit loglkl
            best_fit_lkl_df = content.get("best_fit", {}).get("likelihood", None)
            if (
                best_fit_lkl_df is not None
                and not best_fit_lkl_df.empty
                and "loglkl" in best_fit_lkl_df.columns
            ):
                best_fit_loglkl = best_fit_lkl_df["loglkl"].iloc[0]
            else:
                best_fit_loglkl = None

                # Add bestfit manually if keep_initial_data or keep_first_iteration is set to None with likelihood filter,
                # so that the delta_chi_sq can be calculated and plotted even for iterations where the entire batch is discarded.
                # if iteration == 0 or iteration == 1:
                #     if not (
                #         getattr(self.param_connect, "keep_initial_data", False)
                #         or getattr(self.param_connect, "keep_first_iteration", False)
                #     ) and getattr(self.param_connect, "use_likelihood_filter", False):

                if getattr(self.param_connect, "use_likelihood_filter", False):
                    if (
                        iteration == 0
                        and not getattr(self.param_connect, "keep_initial_data", False)
                    ) or (
                        iteration == 1
                        and not getattr(
                            self.param_connect, "keep_first_iteration", False
                        )
                    ):
                        # Calculate the minimum loglkl from the discarded_iteration
                        discarded_iteration_df = content.get(
                            "discarded_iteration", {}
                        ).get("likelihood_data", None)
                        if (
                            discarded_iteration_df is not None
                            and not discarded_iteration_df.empty
                            and "true_loglkl" in discarded_iteration_df.columns
                        ):
                            best_fit_loglkl = discarded_iteration_df[
                                "true_loglkl"
                            ].min()

            iteration_data[iteration] = {"best_fit_loglkl": best_fit_loglkl}

            # 2) For each category in self.data_categories, gather loglkl data if present
            for cat in self.data_categories:
                cat_dict = content.get(cat, {})
                lkl_df = cat_dict.get("likelihood_data", None)
                if (
                    lkl_df is not None
                    and not lkl_df.empty
                    and "true_loglkl" in lkl_df.columns
                ):
                    arr = lkl_df["true_loglkl"].values.astype(np.float32)
                    # Apply filtering if max_loglkl_in_display is set
                    self.count_points[iteration][cat]["original"] = len(arr)
                    if self.max_loglkl_in_display is not None:
                        arr = arr[arr <= self.max_loglkl_in_display]
                        self.count_points[iteration][cat]["display"] = len(arr)
                    non_empty_categories[cat] = True  # Mark the category as non-empty

                    if self.x_range is not None:
                        self.count_points[iteration][cat]["display"] = len(
                            arr[(arr >= self.x_range[0]) & (arr <= self.x_range[1])]
                        )

                else:
                    arr = np.array([], dtype=np.float32)
                iteration_data[iteration][cat] = arr

                # 3) Compute delta_chi^2 if best_fit_loglkl is defined
                delta_key = f"delta_chi_sq_{cat}"
                if best_fit_loglkl is not None and len(arr) > 0:
                    delta_chi_sq = 2.0 * (arr - best_fit_loglkl)

                    self.count_points[iteration][delta_key]["original"] = len(
                        delta_chi_sq
                    )

                    # Apply filtering if max_delta_chi2_in_display is set
                    if self.max_delta_chi2_in_display is not None:
                        delta_chi_sq = delta_chi_sq[
                            delta_chi_sq <= self.max_delta_chi2_in_display
                        ]
                        self.count_points[iteration][delta_key]["display"] = len(
                            delta_chi_sq
                        )

                    iteration_data[iteration][delta_key] = delta_chi_sq

                    if self.x_range is not None:
                        self.count_points[iteration][delta_key]["display"] = len(
                            delta_chi_sq[
                                (delta_chi_sq >= self.x_range[0])
                                & (delta_chi_sq <= self.x_range[1])
                            ]
                        )

                    # iteration_data[iteration][delta_key] = 2.0 * (arr - best_fit_loglkl)
                else:
                    iteration_data[iteration][delta_key] = np.array(
                        [], dtype=np.float32
                    )

            # 4) Handle cumulative discarded likelihood
            discarded_new = iteration_data[iteration].get(
                "discarded_likelihood_new", np.array([], dtype=np.float32)
            )
            discarded_old = iteration_data[iteration].get(
                "discarded_likelihood_old", np.array([], dtype=np.float32)
            )

            # Combine current discarded likelihoods
            current_discarded_likelihoods = np.concatenate(
                [discarded_new, discarded_old]
            )

            # Update cumulative discarded likelihood
            cumulative_discarded_likelihood = np.concatenate(
                [cumulative_discarded_likelihood, current_discarded_likelihoods]
            )

            # Store cumulative discarded likelihood in the same format as other categories
            iteration_data[iteration][
                "cumulative_discarded_likelihood"
            ] = cumulative_discarded_likelihood.copy()

            self.count_points[iteration]["cumulative_discarded_likelihood"][
                "original"
            ] = len(cumulative_discarded_likelihood)

            # Apply filtering if max_loglkl_in_display is set
            if self.max_loglkl_in_display is not None:
                cumulative_discarded_likelihood = cumulative_discarded_likelihood[
                    cumulative_discarded_likelihood <= self.max_loglkl_in_display
                ]

                self.count_points[iteration]["cumulative_discarded_likelihood"][
                    "display"
                ] = len(cumulative_discarded_likelihood)

            if self.x_range is not None:
                self.count_points[iteration]["cumulative_discarded_likelihood"][
                    "display"
                ] = len(
                    cumulative_discarded_likelihood[
                        (cumulative_discarded_likelihood >= self.x_range[0])
                        & (cumulative_discarded_likelihood <= self.x_range[1])
                    ]
                )

            # Add to non-empty categories if cumulative discarded likelihood is non-empty
            if len(cumulative_discarded_likelihood) > 0:
                non_empty_categories["cumulative_discarded_likelihood"] = True

            # 5) Compute delta_chi^2 for cumulative discarded likelihood
            delta_key = "delta_chi_sq_cumulative_discarded_likelihood"
            if best_fit_loglkl is not None and len(cumulative_discarded_likelihood) > 0:

                delta_chi_sq_cumulative = 2.0 * (
                    cumulative_discarded_likelihood - best_fit_loglkl
                )

                self.count_points[iteration][delta_key]["original"] = len(
                    delta_chi_sq_cumulative
                )
                # Apply filtering if max_delta_chi2_in_display is set
                if self.max_delta_chi2_in_display is not None:
                    delta_chi_sq_cumulative = delta_chi_sq_cumulative[
                        delta_chi_sq_cumulative <= self.max_delta_chi2_in_display
                    ]
                    self.count_points[iteration][delta_key]["display"] = len(
                        delta_chi_sq_cumulative
                    )

                iteration_data[iteration][delta_key] = delta_chi_sq_cumulative

                if self.x_range is not None:
                    self.count_points[iteration][delta_key]["display"] = len(
                        delta_chi_sq_cumulative[
                            (delta_chi_sq_cumulative >= self.x_range[0])
                            & (delta_chi_sq_cumulative <= self.x_range[1])
                        ]
                    )
            else:
                iteration_data[iteration][delta_key] = np.array([], dtype=np.float32)

        # Retain only non-empty categories
        self.data_categories = [
            cat for cat, is_non_empty in non_empty_categories.items() if is_non_empty
        ]

        return iteration_data

    # ------------------------------------------------------------------
    #  NEW  – collect likelihoods that live in the "chain_loglkl" column
    # ------------------------------------------------------------------
    def _gather_iteration_data_chain(self, anchor_on_true_bestfit=True):
        """
        Build a per-iteration container identical in shape to the one
        returned by `_gather_iteration_data`, but using `chain_loglkl`
        instead of `true_loglkl`.

        Parameters
        ----------
        anchor_on_true_bestfit : bool, default=True
            If True  -> use the chain-likelihood value evaluated at the
                        *true* best-fit point in that iteration as the
                        reference for Δχ².
            If False -> use the minimum chain_loglkl **seen so far**
                        (across all categories in all iterations processed
                        up to the current one).

        Returns
        -------
        iteration_data_chain : dict
            Same layout as the original method, but all arrays are built
            from `chain_loglkl` and keys are `delta_chi_sq_*` (chain).
        """

        last_completed_it = self._find_last_complete_iteration()

        # ---------- 1) bookkeeping containers (mirrors original) ----------
        self.count_points_chain = {}  # <<< NEW
        for iteration in self.data["iteration_data"]:
            self.count_points_chain[iteration] = {}
            for cat in self.data_categories:
                for key in (cat, f"delta_chi_sq_{cat}"):
                    self.count_points_chain[iteration][key] = {
                        "original": 0,
                        "display": None,
                    }
            # cumulative bucket
            for key in (
                "cumulative_discarded_likelihood",
                "delta_chi_sq_cumulative_discarded_likelihood",
            ):
                self.count_points_chain[iteration][key] = {
                    "original": 0,
                    "display": None,
                }

        iteration_data = {}
        non_empty_categories = {cat: False for cat in self.data_categories}
        cumulative_discarded_lkl = np.array([], dtype=np.float32)

        # global running best (only used if anchor_on_true_bestfit=False)
        running_min_chain = np.inf  # <<< NEW

        # ---------- 2) iterate over iterations ----------
        for it, content in sorted(self.data["iteration_data"].items()):

            if it > last_completed_it:
                continue

            # ---------- 2a) choose best-fit reference ----------
            if anchor_on_true_bestfit:
                # Find the row with the *minimum true_loglkl* first
                acc_df = content.get("accepted_accumulated", {}).get(
                    "likelihood_data", None
                )
                if acc_df is not None and not acc_df.empty:
                    idx = acc_df["true_loglkl"].idxmin()
                    best_fit_chain = acc_df.loc[idx, "chain_loglkl"]
                else:
                    best_fit_chain = None
                    if getattr(self.param_connect, "use_likelihood_filter", False):
                        if (
                            it == 0
                            and not getattr(
                                self.param_connect, "keep_initial_data", False
                            )
                        ) or (
                            it == 1
                            and not getattr(
                                self.param_connect, "keep_first_iteration", False
                            )
                        ):

                            # fall-back for iterations 0/1 when everything may be discarded
                            disc_df = content.get("discarded_iteration", {}).get(
                                "likelihood_data", None
                            )
                            if disc_df is not None and not disc_df.empty:
                                idx = disc_df["true_loglkl"].idxmin()
                                best_fit_chain = disc_df.loc[idx, "chain_loglkl"]
                            else:
                                best_fit_chain = None
            else:
                # absolute minimum of chain_loglkl across *all* samples seen so far
                best_fit_chain = None  # not implemented yet                                            # <<< NEW
            iteration_data[it] = {"best_fit_loglkl": best_fit_chain}  # <<< MOD

            # ---------- 2b) loop over categories ----------
            for cat in self.data_categories:
                lkl_df = content.get(cat, {}).get("likelihood_data", None)

                if lkl_df is not None and not lkl_df.empty and "chain_loglkl" in lkl_df:
                    arr = lkl_df["chain_loglkl"].values.astype(np.float32)

                    # running global min for option 2
                    if not anchor_on_true_bestfit:
                        running_min_chain = min(running_min_chain, arr.min())
                        iteration_data[it][
                            "best_fit_loglkl"
                        ] = running_min_chain  # <<< NEW

                    # normal display / clipping logic (identical to original)
                    self.count_points_chain[it][cat]["original"] = len(arr)
                    if self.max_loglkl_in_display is not None:
                        arr = arr[arr <= self.max_loglkl_in_display]
                        self.count_points_chain[it][cat]["display"] = len(arr)
                    if self.x_range is not None:
                        self.count_points_chain[it][cat]["display"] = len(
                            arr[(arr >= self.x_range[0]) & (arr <= self.x_range[1])]
                        )
                    non_empty_categories[cat] = True
                else:
                    arr = np.array([], dtype=np.float32)

                iteration_data[it][cat] = arr

                # ---------- 2c) Δχ² with chain reference ----------
                delta_key = f"delta_chi_sq_{cat}"
                ref = iteration_data[it]["best_fit_loglkl"]
                if ref is not None and len(arr) > 0:
                    delta = 2.0 * (arr - ref)
                    self.count_points_chain[it][delta_key]["original"] = len(delta)
                    if self.max_delta_chi2_in_display is not None:
                        delta = delta[delta <= self.max_delta_chi2_in_display]
                        self.count_points_chain[it][delta_key]["display"] = len(delta)
                    if self.x_range is not None:
                        self.count_points_chain[it][delta_key]["display"] = len(
                            delta[
                                (delta >= self.x_range[0]) & (delta <= self.x_range[1])
                            ]
                        )
                    iteration_data[it][delta_key] = delta
                else:
                    iteration_data[it][delta_key] = np.array([], dtype=np.float32)

            # ---------- 2d) cumulative discarded ----------
            disc_new = iteration_data[it].get(
                "discarded_likelihood_new", np.array([], dtype=np.float32)
            )
            disc_old = iteration_data[it].get(
                "discarded_likelihood_old", np.array([], dtype=np.float32)
            )
            current_disc = np.concatenate([disc_new, disc_old])
            cumulative_discarded_lkl = np.concatenate(
                [cumulative_discarded_lkl, current_disc]
            )

            iteration_data[it][
                "cumulative_discarded_likelihood"
            ] = cumulative_discarded_lkl.copy()
            self.count_points_chain[it]["cumulative_discarded_likelihood"][
                "original"
            ] = len(cumulative_discarded_lkl)

            if self.max_loglkl_in_display is not None:
                cumulative_discarded_lkl = cumulative_discarded_lkl[
                    cumulative_discarded_lkl <= self.max_loglkl_in_display
                ]
                self.count_points_chain[it]["cumulative_discarded_likelihood"][
                    "display"
                ] = len(cumulative_discarded_lkl)
            if self.x_range is not None:
                self.count_points_chain[it]["cumulative_discarded_likelihood"][
                    "display"
                ] = len(
                    cumulative_discarded_lkl[
                        (cumulative_discarded_lkl >= self.x_range[0])
                        & (cumulative_discarded_lkl <= self.x_range[1])
                    ]
                )

            if len(cumulative_discarded_lkl) > 0:
                non_empty_categories["cumulative_discarded_likelihood"] = True
            # Δχ² for cumulative discarded
            delta_key = "delta_chi_sq_cumulative_discarded_likelihood"
            ref = iteration_data[it]["best_fit_loglkl"]
            if ref is not None and len(cumulative_discarded_lkl) > 0:
                delta_cum = 2.0 * (cumulative_discarded_lkl - ref)
                self.count_points_chain[it][delta_key]["original"] = len(delta_cum)
                if self.max_delta_chi2_in_display is not None:
                    delta_cum = delta_cum[delta_cum <= self.max_delta_chi2_in_display]
                    self.count_points_chain[it][delta_key]["display"] = len(delta_cum)
                iteration_data[it][delta_key] = delta_cum

                if self.x_range is not None:
                    self.count_points_chain[it][delta_key]["display"] = len(
                        delta_cum[
                            (delta_cum >= self.x_range[0])
                            & (delta_cum <= self.x_range[1])
                        ]
                    )
            else:
                iteration_data[it][delta_key] = np.array([], dtype=np.float32)

        # drop empty categories (mirrors original)
        self.data_categories_chain = [
            c for c, non_empty in non_empty_categories.items() if non_empty
        ]  # <<< NEW
        return iteration_data

    def _plot_per_iteration_boxplot(self, iteration_data, metric="delta_chi_sq"):
        """
        Plot distributions per iteration (boxplot + stripplot) for each data category in self.data_categories.
        Maintains log-scale on the y-axis for both 'delta_chi_sq' and 'loglkl'.
        """
        if self.verbose >= 2:
            print(
                f"[_plot_per_iteration_boxplot] Plotting boxplots for {metric} per iteration."
            )
        matplotlib.rcParams.update(matplotlib.rcParamsDefault)
        sns.reset_defaults()
        sns.set_style("ticks")

        matplotlib.use("Agg")

        # Construct a DataFrame for plotting
        plot_rows = []
        for iteration, i_data in iteration_data.items():

            # Determine plot label & threshold logic
            if metric == "delta_chi_sq":
                y_label = r"$\Delta\chi^2$"
                global_threshold_value = getattr(
                    self.param_connect, "delta_chi2_threshold", None
                )
                # Check if global_threshold_value is a list
                if isinstance(global_threshold_value, list):
                    if iteration < len(global_threshold_value):
                        global_threshold_value = global_threshold_value[iteration]
                    else:
                        global_threshold_value = global_threshold_value[-1]

                threshold_line_label = (
                    f"$\Delta\chi^2{{\mathrm{{-Threshold}}}} = {global_threshold_value:.1f}$"
                    if global_threshold_value < 10000
                    else (
                        r"$\Delta\chi^2\mathrm{{-Threshold}}="
                        + self.sci_notation_latex(global_threshold_value)
                        + "$"
                    )
                )

            elif metric == "loglkl":
                y_label = r"$-\log(\mathcal{L})$"
                threshold_line_label = r"\mathrm{{Best-Fit:}}\; -\log(\mathcal{{L}})"
                global_threshold_value = None  # We'll get best-fit loglkl per iteration
            else:
                raise ValueError("Invalid metric. Use 'delta_chi_sq' or 'loglkl'.")

            best_fit_loglkl = i_data.get("best_fit_loglkl", None)
            for cat in self.data_categories:
                if metric == "delta_chi_sq":
                    arr = i_data.get(
                        f"delta_chi_sq_{cat}", np.array([], dtype=np.float32)
                    )
                else:  # loglkl
                    arr = i_data.get(cat, np.array([], dtype=np.float32))

                for val in arr:
                    plot_rows.append((iteration, cat, val, best_fit_loglkl))

        if not plot_rows:
            if self.verbose >= 2:
                print(
                    f"[_plot_per_iteration_boxplot] No data available for {metric} boxplots."
                )
            return

        df = pd.DataFrame(
            plot_rows, columns=["iteration", "category", metric, "best_fit_loglkl"]
        )
        df["category"] = pd.Categorical(
            df["category"], categories=self.data_categories, ordered=True
        )
        df["category_label"] = df["category"].map(self.category_labels)

        if self.y_range_box is None and metric == "delta_chi_sq":
            # 1) Compute robust min and max across all data
            y_min = df[metric].quantile(
                0.001
            )  # 0.1st percentile (Removing the worst outliers)
            y_max = df[metric].quantile(
                0.999
            )  # 99.9th percentile (Removing the best outliers)

        # Figure layout
        iterations = sorted(df["iteration"].unique())
        num_iterations = len(iterations)
        max_cols = 3  # Set a max number of columns to avoid too wide plots
        cols = min(max_cols, num_iterations)
        rows = int(math.ceil(num_iterations / cols))

        fig, axes = plt.subplots(
            rows, cols, figsize=(3.5 * cols, 4 * rows), sharey=True  # (width, height)
        )
        axes = axes.flatten() if rows * cols > 1 else [axes]

        # Prepare color palette (for each category)
        # We'll rely on self.DATA_COLORS if the category is recognized
        # otherwise fallback to a default color
        def get_cat_color(cat):
            return self.DATA_COLORS.get(cat, "gray")

        # Because Seaborn expects a palette dict with category->color
        # We'll build that on the fly
        # Create a palette dictionary for Seaborn
        palette_box = {
            self.category_labels[cat]: self.DATA_COLORS[cat]
            for cat in self.data_categories
            if cat in self.DATA_COLORS
        }

        # Now plot iteration by iteration
        for idx, it in enumerate(iterations):
            ax = axes[idx]
            subdf = df[df["iteration"] == it].copy()
            # For boxplot, we rely on Seaborn's 'x="category"' to group categories
            sns.boxplot(
                data=subdf,
                x="category_label",
                y=metric,
                hue="category_label",
                ax=ax,
                palette=palette_box,
                dodge=False,
                showfliers=False,
                log_scale=True,
            )
            # Overplot strip
            # We'll define a matching list for the strip palette in the same order
            # But we can pass the same dict as 'palette=palette_box'
            # if we specify "hue='category'" again
            sns.stripplot(
                data=subdf,
                x="category_label",
                y=metric,
                hue="category_label",
                ax=ax,
                palette=palette_box,
                dodge=True,
                size=0.8,
                alpha=1,
                jitter=0.5,
                linewidth=0.05,
                edgecolor="black",
                log_scale=True,
            )

            # Remove repeated legends
            handles, labels = ax.get_legend_handles_labels()
            by_label = dict(zip(labels, handles))
            ax.legend(by_label.values(), by_label.keys(), loc="upper right", fontsize=7)

            # Threshold line (for delta_chi_sq) or best-fit loglkl line (for loglkl)
            if metric == "delta_chi_sq":
                if global_threshold_value is not None:
                    ax.axhline(
                        global_threshold_value,
                        color="purple",
                        linestyle="--",
                        label=threshold_line_label,
                    )
                    # Update legend
                    h2, l2 = ax.get_legend_handles_labels()
                    by_label_2 = dict(zip(l2, h2))
                    ax.legend(
                        by_label_2.values(),
                        by_label_2.keys(),
                        loc="upper right",
                        fontsize=6,
                    )
            else:  # loglkl
                # We can draw a line at iteration's best-fit if it exists
                bf = subdf["best_fit_loglkl"].iloc[0]
                if not pd.isnull(bf):
                    ax.axhline(
                        bf,
                        color="purple",
                        linestyle="--",
                        label=(
                            rf"${threshold_line_label} = {bf:.2f}$"
                            if bf < 10000
                            else rf"${threshold_line_label} = {self.sci_notation_latex(bf)}$"
                        ),
                    )
                    h3, l3 = ax.get_legend_handles_labels()
                    by_label_3 = dict(zip(l3, h3))
                    ax.legend(
                        by_label_3.values(),
                        by_label_3.keys(),
                        loc="upper right",
                        fontsize=6,
                    )

            if self.y_range_box is not None:
                ax.set_ylim(*self.y_range_box)
            elif metric == "delta_chi_sq":
                ax.set_ylim(y_min, y_max)

            # Add iteration label
            ax.text(
                0.03,
                0.97,
                f"i = {it}",
                transform=ax.transAxes,
                fontsize=6,
                verticalalignment="top",
                bbox=dict(
                    facecolor="white",
                    edgecolor="grey",
                    boxstyle="round,pad=0.4",
                    alpha=0.5,
                ),
            )

            # X labels
            ax.set_xlabel("")
            ax.set_ylabel("")

            # Remove x-axis labels except for the last row
            if it < len(iterations) - cols:
                ax.tick_params(
                    axis="x", which="both", bottom=False, top=False, labelbottom=False
                )
            if it >= len(iterations) - cols:
                ax.set_xticklabels(
                    ax.get_xticklabels(), rotation=90, ha="right", fontsize=8
                )

            ax.grid(axis="y", linestyle="--", alpha=0.7)

        # Hide unused axes
        for j in range(idx + 1, len(axes)):
            axes[j].axis("off")

        fig.supylabel(y_label, x=0.06, fontsize=12)
        plt.subplots_adjust(wspace=0, hspace=0)

        # Save
        for fmt in self.save_formats:
            plot_filename = f"per_iteration_{metric}_boxplots.{fmt}"
            plot_path = os.path.join(self.analysis_dir, plot_filename)
            plt.savefig(plot_path, dpi=1000, bbox_inches="tight")

        plt.close(fig)
        if self.verbose >= 1:
            print(
                f"[_plot_per_iteration_boxplot] Saved {metric} boxplots to {plot_path}"
            )

    def _plot_per_iteration_hist_original(
        self, iteration_data, metric="delta_chi_sq", stat="count"
    ):
        """
        Plot histograms per iteration for each data category in self.data_categories.
        Maintains log-scale on the x-axis for both 'delta_chi_sq' and 'loglkl'.
        """

        matplotlib.rcParams.update(matplotlib.rcParamsDefault)
        if self.verbose >= 2:
            print(
                f"[_plot_per_iteration_hist] Plotting histograms with stat={stat} for {metric}."
            )

        matplotlib.use("Agg")
        dummy_df = self._create_dummy_df(iteration_data, metric=metric)
        if dummy_df is None:  # Handle cases where no data is available
            if self.verbose >= 2:
                print(f"[_plot_per_iteration_hist] No data available for {metric}.")
            return

        global_bin_edges = self.compute_global_bin_edges_seaborn_internal(
            data=dummy_df,
            x="value",
            hue="category_iteration",
            stat="count",
            bins="auto",
            log_scale=(True, False),  # Logarithmic x-axis
            palette=sns.color_palette("tab20", n_colors=80),  # Use a large palette
            alpha=1,
            fill=True,
            element="bars",
            common_norm=True,
            common_bins=True,
            multiple="layer",
        )

        # ------------------------------
        # 1) Gather data for main/inset
        # ------------------------------
        plot_data = []
        for iteration, i_data in iteration_data.items():
            best_fit_loglkl = i_data.get("best_fit_loglkl", None)
            inset_arrays = []
            main_arrays = []

            # Loop over hist_main_panel
            for item in self.hist_main_panel:
                cat = item["category"]
                if metric == "delta_chi_sq":
                    arr = i_data.get(
                        f"delta_chi_sq_{cat}", np.array([], dtype=np.float32)
                    )
                else:
                    arr = i_data.get(cat, np.array([], dtype=np.float32))

                if len(arr) > 0:
                    main_arrays.append((cat, arr, item))

            # Loop over hist_inset if self.include_inset
            if self.include_inset:
                for item in self.hist_inset:
                    cat = item["category"]
                    if metric == "delta_chi_sq":
                        arr = i_data.get(
                            f"delta_chi_sq_{cat}", np.array([], dtype=np.float32)
                        )
                    else:
                        arr = i_data.get(cat, np.array([], dtype=np.float32))

                    if len(arr) > 0:
                        inset_arrays.append((cat, arr, item))

            # If both are empty, skip
            if not main_arrays and not inset_arrays:
                continue

            plot_data.append(
                {
                    "iteration": iteration,
                    "best_fit_loglkl": best_fit_loglkl,
                    "main_arrays": main_arrays,
                    "inset_arrays": inset_arrays,
                }
            )

        if not plot_data:
            if self.verbose >= 2:
                print("[_plot_per_iteration_hist] No data to plot for histograms.")
            return

        # -------------------------------------------------------------------
        # 2) *Collect all* the inset data from all iterations in "plot_data"
        #    so we can compute the global x & y axis range for the insets.
        # -------------------------------------------------------------------
        all_inset_plots = []  # Will store (arr, item_cfg) from each iteration
        if self.include_inset:
            for entry in plot_data:
                for cat, arr, item_cfg in entry["inset_arrays"]:
                    all_inset_plots.append((arr, item_cfg))

        # -------------------------------------------------------------------
        # 3) Compute global inset ranges *once*, if insets are used
        # -------------------------------------------------------------------
        x_min_inset, x_max_inset, y_min_inset, y_max_inset = None, None, None, None
        if self.include_inset and len(all_inset_plots) > 0:
            (x_min_inset, x_max_inset, y_min_inset, y_max_inset) = (
                self._compute_global_inset_ranges(
                    all_inset_plots, metric=metric, stat=stat, bins=global_bin_edges
                )
            )

        # -------------------------------------------------------------------
        # 4) Create subplots and do the real plotting
        # -------------------------------------------------------------------
        cols = max(3, int(round(math.sqrt(len(plot_data)))))
        rows = int(math.ceil(len(plot_data) / cols))
        fig, axes = plt.subplots(
            rows, cols, figsize=(6 * cols, 3 * rows), sharex=True, sharey=True
        )

        axes = axes.T.flatten() if rows * cols > 1 else [axes]

        # Identify the bottom row and rightmost column axes based on actual number of plots
        used_axes_indices = list(range(len(plot_data)))

        # Bottom row: the last 'cols' or fewer axes
        bottom_row_indices = used_axes_indices[-cols:]

        # Rightmost column: every axis where (index + 1) % cols == 0
        rightmost_col_indices = [
            i
            for i in used_axes_indices
            if (i + 1) % cols == 0
            or (i == len(used_axes_indices) - 1 and len(used_axes_indices) % cols != 0)
        ]

        all_handles_labels = {}
        all_handles_labels_inset = {}
        first_inset_ax = None  # Initialize

        all_legends_info_main = {}
        all_legends_info_inset = {}
        threshold_label_added = False
        threshold_label_added_inset = False

        lowest_iteration = min([entry["iteration"] for entry in plot_data])
        lowest_iteration_inset = min(
            [entry["iteration"] for entry in plot_data if entry["inset_arrays"]]
        )
        # Now fill each subplot
        for i, entry in enumerate(plot_data):
            iteration = entry["iteration"]
            delta_threshold = getattr(self.param_connect, "delta_chi2_threshold", None)
            if isinstance(delta_threshold, list):
                if iteration < len(delta_threshold):
                    delta_threshold = delta_threshold[iteration]
                else:
                    delta_threshold = delta_threshold[-1]

            best_fit = entry["best_fit_loglkl"]
            main_arrays = entry["main_arrays"]
            inset_arrays = entry["inset_arrays"]
            all_legends_info_main[iteration] = {}

            ax = axes[i]
            ax.set_axisbelow(True)
            ax.grid(True, which="major", linestyle="-", linewidth=0.5, alpha=0.5)

            # --- Plot main panel categories ---
            for cat, arr, item_cfg in main_arrays:
                color = item_cfg.get("color", None)
                label = item_cfg.get("label", None)
                plot_kws = item_cfg.get("plot_kws", {}).copy()
                plot_kws["bins"] = global_bin_edges

                # fallback
                if color is None:
                    color = self.DATA_COLORS.get(cat, "gray")
                if label is None:
                    label = self.category_labels.get(cat, cat)

                # add the label to the legend
                display = self.count_points[iteration][cat]["display"]
                original = self.count_points[iteration][cat]["original"]
                if display is None or display == original:
                    counts_str = f"{original} points"
                else:
                    counts_str = f"showing $\\frac{{{display}}}{{{original}}}$ points"

                all_legends_info_main[iteration][label] = {}
                all_legends_info_main[iteration][label]["label"] = (
                    f"{label} ({counts_str})"
                    if iteration == lowest_iteration
                    else f"{counts_str}"
                )

                all_legends_info_main[iteration][label] = {}
                all_legends_info_main[iteration][label]["label"] = (
                    f"{label} ({counts_str})"
                    if iteration == lowest_iteration
                    else f"{counts_str}"
                )

                final_plot_kws = dict(plot_kws)
                final_plot_kws.setdefault("color", color)
                final_plot_kws.setdefault("label", label)
                final_plot_kws.setdefault("stat", stat)

                if len(arr) == 0:
                    continue  # Skip empty categories

                sns.histplot(x=arr, ax=ax, **final_plot_kws)

            # Optional vertical line
            if metric == "delta_chi_sq" and delta_threshold is not None:
                delta_chi2_threshold_label = (
                    f"$\Delta\chi^2\mathrm{{-Threshold}}={delta_threshold:.1f}$"
                    if delta_threshold < 10000
                    else (
                        r"$\Delta\chi^2\mathrm{-Threshold}="
                        + self.sci_notation_latex(delta_threshold)
                        + "$"
                    )
                )

                delta_chi2_value = (
                    f"{delta_threshold:.1f}"
                    if delta_threshold < 10000
                    else f"${self.sci_notation_latex(delta_threshold)}$"
                )
                ax.axvline(
                    delta_threshold,
                    color="purple",
                    linestyle="--",
                    label=delta_chi2_threshold_label,
                )
                all_legends_info_main[iteration][delta_chi2_threshold_label] = {}
                all_legends_info_main[iteration][delta_chi2_threshold_label][
                    "label"
                ] = (
                    delta_chi2_threshold_label
                    if iteration == lowest_iteration
                    else delta_chi2_value
                )

            elif metric == "loglkl" and best_fit is not None and not np.isnan(best_fit):
                best_fit_label = (
                    f"$\mathrm{{Best-Fit:}}\; -\log(\mathcal{{L}}) = {best_fit:.1f}$"
                    if best_fit < 10000
                    else (
                        r"$\mathrm{Best-Fit:}\; -\log(\mathcal{L}) = "
                        + self.sci_notation_latex(best_fit)
                        + "$"
                    )
                )
                # f"$\mathrm{{Best-Fit:}}\; -\log(\mathcal{{L}}) = {best_fit:.0e}$"

                best_fit_value = (
                    f"{best_fit:.1f}"
                    if best_fit < 10000
                    else self.sci_notation_latex(best_fit)
                )
                ax.axvline(
                    best_fit,
                    color="purple",
                    linestyle="--",
                    label=best_fit_label,
                )
                all_legends_info_main[iteration][best_fit_label] = {}
                all_legends_info_main[iteration][best_fit_label]["label"] = (
                    best_fit_label if iteration == lowest_iteration else best_fit_value
                )

            # --- Plot the inset if needed ---
            if self.include_inset and inset_arrays:
                ax_inset = inset_axes(
                    ax, width="40%", height="40%", loc="upper right", borderpad=1
                )
                ax_inset.set_axisbelow(True)
                ax_inset.grid(
                    True, which="major", linestyle="-", linewidth=0.5, alpha=0.5
                )

                all_legends_info_inset[iteration] = {}

                if first_inset_ax is None:
                    first_inset_ax = ax_inset

                for cat, arr, item_cfg in inset_arrays:
                    color = item_cfg.get("color", None)
                    label = item_cfg.get("label", None)
                    plot_kws = item_cfg.get("plot_kws", {}).copy()
                    plot_kws["bins"] = global_bin_edges

                    if color is None:
                        color = self.DATA_COLORS.get(cat, "gray")
                    if label is None:
                        label = self.category_labels.get(cat, cat)
                    final_plot_kws = dict(plot_kws)
                    final_plot_kws.setdefault("color", color)
                    final_plot_kws.setdefault("label", label)
                    final_plot_kws.setdefault("stat", stat)

                    if len(arr) == 0:
                        continue  # Skip empty categories

                    sns.histplot(x=arr, ax=ax_inset, **final_plot_kws)

                    display = self.count_points[iteration][cat]["display"]
                    original = self.count_points[iteration][cat]["original"]

                    if display is None or display == original:
                        counts_str = f"{original} points"
                    else:
                        counts_str = (
                            f"showing $\\frac{{{display}}}{{{original}}}$ points"
                        )

                    all_legends_info_inset[iteration][label] = {}
                    all_legends_info_inset[iteration][label]["label"] = (
                        f"{label} ({counts_str})"
                        if iteration == lowest_iteration_inset
                        else f"{counts_str}"
                    )

                # *Now* we unify the axis ranges for the inset
                if x_min_inset is not None and x_max_inset is not None:
                    ax_inset.set_xlim(x_min_inset, x_max_inset)
                if y_min_inset is not None and y_max_inset is not None:
                    ax_inset.set_ylim(y_min_inset, y_max_inset)

                # Optional threshold lines in the inset
                if metric == "delta_chi_sq" and delta_threshold is not None:
                    ax_inset.axvline(
                        delta_threshold,
                        color="purple",
                        linestyle="--",
                        linewidth=1,
                        label=delta_chi2_threshold_label,
                    )

                    all_legends_info_inset[iteration][delta_chi2_threshold_label] = {}
                    all_legends_info_inset[iteration][delta_chi2_threshold_label][
                        "label"
                    ] = (
                        delta_chi2_threshold_label
                        if iteration == lowest_iteration_inset
                        else delta_chi2_value
                    )

                elif (
                    metric == "loglkl"
                    and best_fit is not None
                    and not np.isnan(best_fit)
                ):
                    ax_inset.axvline(
                        best_fit,
                        color="purple",
                        linestyle="--",
                        linewidth=1,
                        label=best_fit_label,
                    )
                    all_legends_info_inset[iteration][best_fit_label] = {}
                    all_legends_info_inset[iteration][best_fit_label]["label"] = (
                        best_fit_label
                        if iteration == lowest_iteration_inset
                        else best_fit_value
                    )

                # ax_inset.legend(fontsize=5)
                ax_inset.tick_params(axis="both", labelsize=6)
                ax_inset.set_xlabel("")
                ax_inset.set_ylabel("")
                ax_inset.minorticks_on()

                handles, labels = ax_inset.get_legend_handles_labels()
                for handle, label in zip(handles, labels):

                    all_legends_info_inset[iteration][label]["handle"] = handle

                    if r"\Delta\chi^2\mathrm{-Threshold}=" in label:
                        if not threshold_label_added_inset:
                            all_handles_labels_inset[label] = handle
                            threshold_label_added_inset = True
                        continue  # skip adding any other threshold labels
                    if r"\mathrm{Best-Fit" in label:
                        if not threshold_label_added_inset:
                            all_handles_labels_inset[label] = handle
                            threshold_label_added_inset = True
                        continue  # skip adding any other threshold labels
                    if label not in all_handles_labels_inset:
                        all_handles_labels_inset[label] = handle

                # Set axis ranges for inset based on self.x_range_inset, self.y_range_inset
                if self.x_range_inset is not None:
                    ax_inset.set_xlim(self.x_range_inset)
                if self.y_range_inset is not None:
                    ax_inset.set_ylim(self.y_range_inset)

                # if iteration > lowest_iteration_inset:
                # Create legend for the subplot using all_legends_info_inset
                legend_labels = []
                legend_handles = []
                for label in all_legends_info_inset[iteration].keys():
                    legend_labels.append(
                        all_legends_info_inset[iteration][label]["label"]
                    )
                    legend_handles.append(
                        all_legends_info_inset[iteration][label]["handle"]
                    )
                legend_inset = ax_inset.legend(
                    legend_handles,
                    legend_labels,
                    loc="upper left",
                    fontsize=6,
                    frameon=True,
                )

            # --- End of plotting inset ---

            if i in bottom_row_indices and i in rightmost_col_indices:
                ax.tick_params(axis="x", which="both", labelbottom=True)

            from matplotlib.ticker import LogLocator

            ax.xaxis.set_minor_locator(
                LogLocator(base=10.0, subs="auto", numticks=None)
            )

            ax.minorticks_on()

            ax.set_xlabel("")
            ax.set_ylabel("")

            # Collect handles and labels from each iteration

            handles, labels = ax.get_legend_handles_labels()
            for handle, label in zip(handles, labels):

                all_legends_info_main[iteration][label]["handle"] = handle

                if r"\Delta\chi^2\mathrm{-Threshold}=" in label:
                    if not threshold_label_added:
                        all_handles_labels[label] = handle
                        threshold_label_added = True
                    continue  # skip adding any other threshold labels
                if r"\mathrm{Best-Fit" in label:
                    if not threshold_label_added:
                        all_handles_labels[label] = handle
                        threshold_label_added = True
                    continue  # skip adding any other threshold labels
                if label not in all_handles_labels:
                    all_handles_labels[label] = handle

            # Owerwrite axis ranges based on self.x_range, self.y_range if not None
            if self.x_range is not None:
                ax.set_xlim(self.x_range)
            if self.y_range is not None:
                ax.set_ylim(self.y_range)

        for label, handle in list(all_handles_labels.items()):
            # Overwrite the legend labels with the counts
            new_label = label
            if label in all_legends_info_main[lowest_iteration]:
                new_label = all_legends_info_main[lowest_iteration][label]["label"]

            # reset the 'label' key to the new new_label key, but reuse the same handle as value
            # Only rename if label changed
            if new_label != label:
                all_handles_labels[new_label] = handle
                del all_handles_labels[label]

        # Add a shared legend to the first subplot
        fig.canvas.draw()

        legend1 = axes[0].legend(
            all_handles_labels.values(),
            all_handles_labels.keys(),
            loc="upper left",
            # bbox_to_anchor=(0.01, 0.91),  # Place it outside the subplot to the right
            fontsize=7,
            frameon=True,
            # add legend title
            title="Shared Legend for all Iterations",
            title_fontsize="8",
            # bbox_transform=axes[0].transAxes,
        )

        fig.canvas.draw()

        ax.annotate(
            f"i = {lowest_iteration}",
            xy=(0, 0),  # anchor to the bottom-left corner of the legend
            xycoords=legend1,  # interpret (0,0) as fraction of the legend's bbox
            xytext=(2, -6),  # shift by –10 points in y
            textcoords="offset points",  # interpret xytext in points
            ha="left",
            va="top",  # "top" means the text's top is at the anchor
            bbox=dict(
                facecolor="white",
                edgecolor="grey",
                boxstyle="round,pad=0.4",
                alpha=0.5,
            ),
            fontsize=7,
        )

        for iteration in range(len(plot_data)):
            ax = axes[iteration]
            if iteration > lowest_iteration:
                # Create legend for the subplot using all_legends_info_main
                legend_labels = []
                legend_handles = []
                for label in all_legends_info_main[iteration].keys():
                    legend_labels.append(
                        all_legends_info_main[iteration][label]["label"]
                    )
                    legend_handles.append(
                        all_legends_info_main[iteration][label]["handle"]
                    )
                legend_main = ax.legend(
                    legend_handles,
                    legend_labels,
                    loc="upper left",
                    fontsize=7,
                    frameon=True,
                )

                fig.canvas.draw()  # So legend_main has a realized bounding box

                ax.annotate(
                    f"i = {iteration}",
                    xy=(0, 0),  # anchor to the bottom-left corner of the legend
                    xycoords=legend_main,  # interpret (0,0) as fraction of the legend's bbox
                    xytext=(2, -6),  # shift by –10 points in y
                    textcoords="offset points",  # interpret xytext in points
                    ha="left",
                    va="top",  # "top" means the text's top is at the anchor
                    bbox=dict(
                        facecolor="white",
                        edgecolor="grey",
                        boxstyle="round,pad=0.4",
                        alpha=0.5,
                    ),
                    fontsize=7,
                )

        for label, handle in list(all_handles_labels_inset.items()):
            # Overwrite the legend labels with the counts
            new_label = label
            if label in all_legends_info_inset[lowest_iteration_inset]:
                new_label = all_legends_info_inset[lowest_iteration_inset][label][
                    "label"
                ]

            # reset the 'label' key to the new new_label key, but reuse the same handle as value
            # Only rename if label changed
            if new_label != label:
                all_handles_labels_inset[new_label] = handle
                del all_handles_labels_inset[label]

        # Add legend to the first inset:
        if self.include_inset:
            first_inset_ax.legend(
                all_handles_labels_inset.values(),  #
                all_handles_labels_inset.keys(),
                loc="upper left",
                fontsize=5,
                frameon=True,
                title="Shared Legend for all Insets",
                title_fontsize="6",
            )

        # Hide extra subplots

        for j in range(i + 1, len(axes)):
            axes[j].axis("off")

        # Add super labels
        if metric == "delta_chi_sq":
            fig.supxlabel(r"$\Delta\chi^2$", fontsize=12, y=0.05)
        else:
            fig.supxlabel(r"$-\log(\mathcal{L})$", fontsize=12, y=0.05)
        fig.supylabel("Counts" if stat == "count" else "Density", fontsize=12, x=0.08)

        plt.subplots_adjust(wspace=0, hspace=0)

        for fmt in self.save_formats:
            plot_filename = f"per_iteration_{metric}_hist_{stat}.{fmt}"
            save_path = os.path.join(self.analysis_dir, plot_filename)
            plt.savefig(save_path, bbox_inches="tight", dpi=1000)

        plt.close(fig)

        if self.verbose >= 1:
            print(
                f"[_plot_per_iteration_hist] Saved histograms ({metric}, {stat}) to {save_path}"
            )

    # ================================================================
    #  _plot_per_iteration_hist  (fully backward-compatible)
    # ================================================================
    def _plot_per_iteration_hist(
        self,
        iteration_data,
        iteration_data_chain=None,  # <<< NEW (optional)
        metric="delta_chi_sq",
        stat="count",
    ):
        """
        Produce per-iteration histograms.

        • With only `iteration_data` ➜ identical figure as before.
        • With BOTH containers      ➜ each iteration-cell is split in two
        stacked panels: upper = CHAIN (“fake”), lower = CLASS (“true”).
        """

        matplotlib.rcParams.update(matplotlib.rcParamsDefault)
        if self.verbose >= 2:
            extra = " +CHAIN" if iteration_data_chain is not None else ""
            print(f"[_plot_per_iteration_hist] stat={stat}, metric={metric}{extra}")

        matplotlib.use("Agg")

        # ------------------------------------------------------------------
        # 0)  GLOBAL BIN EDGES  (build a dummy-DF from *both* containers)
        # ------------------------------------------------------------------
        dummy_df = self._create_dummy_df(  # <<< MOD (extra arg)
            iteration_data,
            iteration_data_chain,  # <<< NEW
            metric=metric,
        )
        if dummy_df is None:
            if self.verbose >= 2:
                print("[_plot_per_iteration_hist] Nothing to plot.")
            return

        global_bin_edges = self.compute_global_bin_edges_seaborn_internal(
            data=dummy_df,
            x="value",
            hue="category_iteration",
            stat="count",
            bins="auto",
            log_scale=(True, False),
            palette=sns.color_palette("tab20", n_colors=80),
            alpha=1,
            fill=True,
            element="bars",
            common_norm=True,
            common_bins=True,
            multiple="layer",
        )

        # ------------------------------------------------------------------
        # 1)  GATHER DATA FOR EACH ITERATION
        # ------------------------------------------------------------------
        plot_data = []
        # A shared one‐element list to hold the *first* inset‐axes we create:
        first_inset_ax_container = [None]

        # Shared flags so we only add the Δχ²‐threshold (or Best‐Fit) line _once_
        threshold_label_added_container = [False]
        threshold_label_added_inset_container = [False]

        for iteration, i_data_true in iteration_data.items():

            # --- CLASS / TRUE ---
            best_fit_true = i_data_true.get("best_fit_loglkl", None)
            main_true, inset_true = [], []
            # Loop over hist_main_panel

            for item in self.hist_main_panel:
                cat = item["category"]
                arr = (
                    i_data_true.get(
                        f"delta_chi_sq_{cat}", np.array([], dtype=np.float32)
                    )
                    if metric == "delta_chi_sq"
                    else i_data_true.get(cat, np.array([], dtype=np.float32))
                )
                if len(arr) > 0:
                    main_true.append((cat, arr, item))
            if self.include_inset:
                for item in self.hist_inset:
                    cat = item["category"]
                    arr = (
                        i_data_true.get(
                            f"delta_chi_sq_{cat}", np.array([], dtype=np.float32)
                        )
                        if metric == "delta_chi_sq"
                        else i_data_true.get(cat, np.array([], dtype=np.float32))
                    )
                    if len(arr) > 0:
                        inset_true.append((cat, arr, item))

            # --- CHAIN / FAKE (optional) --------------------------------
            main_fake, inset_fake = [], []  # <<< NEW
            best_fit_fake = None  # <<< NEW
            if iteration_data_chain is not None and iteration in iteration_data_chain:
                i_data_fake = iteration_data_chain[iteration]
                best_fit_fake = i_data_fake.get("best_fit_loglkl", None)

                for item in self.hist_main_panel:
                    cat = item["category"]
                    arr = (
                        i_data_fake.get(
                            f"delta_chi_sq_{cat}", np.array([], dtype=np.float32)
                        )
                        if metric == "delta_chi_sq"
                        else i_data_fake.get(cat, np.array([], dtype=np.float32))
                    )
                    if len(arr) > 0:
                        main_fake.append((cat, arr, item))
                if self.include_inset:
                    for item in self.hist_inset:
                        cat = item["category"]
                        arr = (
                            i_data_fake.get(
                                f"delta_chi_sq_{cat}", np.array([], dtype=np.float32)
                            )
                            if metric == "delta_chi_sq"
                            else i_data_fake.get(cat, np.array([], dtype=np.float32))
                        )
                        if len(arr) > 0:
                            inset_fake.append((cat, arr, item))

            # --- skip empty iterations completely -----------------------
            if not main_true and not inset_true and not main_fake and not inset_fake:
                continue

            plot_data.append(
                dict(
                    iteration=iteration,
                    # CLASS
                    best_fit_loglkl=best_fit_true,
                    main_arrays=main_true,
                    inset_arrays=inset_true,
                    # CHAIN
                    best_fit_loglkl_fake=best_fit_fake,  # <<< NEW
                    main_arrays_fake=main_fake,  # <<< NEW
                    inset_arrays_fake=inset_fake,  # <<< NEW
                )
            )

        if not plot_data:
            if self.verbose >= 2:
                print("[_plot_per_iteration_hist] No data to plot for histograms.")
            return

        # ------------------------------------------------------------------
        # 2)  GLOBAL INSET RANGES  (true + fake combined)
        # ------------------------------------------------------------------
        all_inset_plots = []
        if self.include_inset:
            for entry in plot_data:
                all_inset_plots += [(arr, cfg) for _, arr, cfg in entry["inset_arrays"]]
                all_inset_plots += [  # <<< NEW
                    (arr, cfg) for _, arr, cfg in entry["inset_arrays_fake"]
                ]
        x_min_in, x_max_in, y_min_in, y_max_in = (None, None, None, None)
        if self.include_inset and all_inset_plots:
            x_min_in, x_max_in, y_min_in, y_max_in = self._compute_global_inset_ranges(
                all_inset_plots, metric=metric, stat=stat, bins=global_bin_edges
            )

        # ------------------------------------------------------------------
        # 3)  CREATE THE OUTER GRID (same as before)
        # ------------------------------------------------------------------
        cols = max(3, int(round(math.sqrt(len(plot_data)))))
        rows = int(math.ceil(len(plot_data) / cols))
        fig, axes = plt.subplots(
            rows, cols, figsize=(6 * cols, 3 * rows), sharex=True, sharey=True
        )
        axes = axes.T.flatten() if rows * cols > 1 else [axes]

        first_true_ax, first_fake_ax = None, None  # <<< NEW
        master_ax = None  # <- one axis that everybody will share
        panel_axes = []
        main_axes = []

        # bookkeeping dictionaries (identical names)

        # Identify the bottom row and rightmost column axes based on actual number of plots
        used_axes_indices = list(range(len(plot_data)))

        # Bottom row: the last 'cols' or fewer axes
        bottom_row_indices = used_axes_indices[-cols:]

        # Rightmost column: every axis where (index + 1) % cols == 0
        rightmost_col_indices = [
            i
            for i in used_axes_indices
            if (i + 1) % cols == 0
            or (i == len(used_axes_indices) - 1 and len(used_axes_indices) % cols != 0)
        ]

        all_handles_labels = {}
        all_handles_labels_inset = {}
        first_inset_ax = None
        all_legends_info_main = {}
        all_legends_info_inset = {}
        # threshold_label_added = False
        # threshold_label_added_inset = False

        lowest_iteration = min(e["iteration"] for e in plot_data)
        if self.include_inset:
            try:
                lowest_iteration_inset = min(
                    e["iteration"]
                    for e in plot_data
                    if e["inset_arrays"] or e["inset_arrays_fake"]
                )
            except ValueError:  # ← no inset arrays at all
                self.include_inset = False
                lowest_iteration_inset = None
        else:
            lowest_iteration_inset = None

        # ------------------------------------------------------------------
        # 4)  FILL EACH OUTER CELL  (one GridSpec per iteration)
        # ------------------------------------------------------------------
        from matplotlib.gridspec import GridSpecFromSubplotSpec

        for axis_index, (ax_cell, entry) in enumerate(zip(axes, plot_data)):
            iteration = entry["iteration"]

            # ----------------------------------------------------------------
            # 4A)  Build a nested 2×1 GridSpec if we have CHAIN data
            # ----------------------------------------------------------------
            gs_inner = GridSpecFromSubplotSpec(
                2 if iteration_data_chain is not None else 1,
                1,
                height_ratios=[1, 1] if iteration_data_chain is not None else [1],
                hspace=0,
                subplot_spec=ax_cell.get_subplotspec(),
            )
            ax_cell.set_axis_off()  # outer axis just a holder

            # helper to create / fetch a real axis object
            def _get_inner(r):
                if iteration_data_chain is None and r == 0:
                    return ax_cell
                ax = plt.Subplot(fig, gs_inner[r])
                fig.add_subplot(ax)
                return ax

            # ---------- (i) CHAIN / fake – upper panel (if any) ----------
            if iteration_data_chain is not None:
                ax_fake = _get_inner(0)
                # share axis with very first fake panel so Δχ² line lines up
                if master_ax is None:
                    master_ax = ax_fake
                else:
                    ax_fake.sharex(master_ax)
                    ax_fake.sharey(master_ax)

            if metric == "delta_chi_sq":
                # if any of the CHAIN delta_chi_sq arrays go negative, switch to symlog
                # if any(arr.min() < 0 for _, arr, _ in entry["main_arrays_fake"]):
                ax_fake.set_xscale("symlog", linthresh=1e-2)
                ax_true.set_xscale("symlog", linthresh=1e-2)

                self._plot_one_panel(  # <<< NEW (wrapper)
                    ax=ax_fake,
                    iteration=iteration,
                    lowest_iteration=lowest_iteration,
                    lowest_iteration_inset=lowest_iteration_inset,
                    metric=metric,
                    stat=stat,
                    global_bin_edges=global_bin_edges,
                    x_min_inset=x_min_in,
                    x_max_inset=x_max_in,
                    y_min_inset=y_min_in,
                    y_max_inset=y_max_in,
                    main_arrays=entry["main_arrays_fake"],
                    inset_arrays=entry["inset_arrays_fake"],
                    best_fit_loglkl=entry["best_fit_loglkl_fake"],
                    is_fake=True,
                    # legend book-keeping dicts
                    all_handles_labels=all_handles_labels,
                    all_handles_labels_inset=all_handles_labels_inset,
                    all_legends_info_main=all_legends_info_main,
                    all_legends_info_inset=all_legends_info_inset,
                    first_inset_ax_container=first_inset_ax_container,
                    threshold_label_added_container=threshold_label_added_container,
                    threshold_label_added_inset_container=threshold_label_added_inset_container,
                )
                # first_inset_ax_fake = first_inset_ax_fake or ax_fake  # update link
                panel_axes.append((ax_fake, axis_index, True))  # True  → is_fake

            # ---------- (ii) CLASS / true – lower (or only) panel --------
            ax_true = _get_inner(1 if iteration_data_chain is not None else 0)

            if master_ax is None:
                master_ax = ax_true  # could happen if first CHAIN was empty
            else:
                ax_true.sharex(master_ax)
                ax_true.sharey(master_ax)

            self._plot_one_panel(  # <<< NEW
                ax=ax_true,
                iteration=iteration,
                lowest_iteration=lowest_iteration,
                lowest_iteration_inset=lowest_iteration_inset,
                metric=metric,
                stat=stat,
                global_bin_edges=global_bin_edges,
                x_min_inset=x_min_in,
                x_max_inset=x_max_in,
                y_min_inset=y_min_in,
                y_max_inset=y_max_in,
                main_arrays=entry["main_arrays"],
                inset_arrays=entry["inset_arrays"],
                best_fit_loglkl=entry["best_fit_loglkl"],
                is_fake=False,
                # legend book-keeping dicts
                all_handles_labels=all_handles_labels,
                all_handles_labels_inset=all_handles_labels_inset,
                all_legends_info_main=all_legends_info_main,
                all_legends_info_inset=all_legends_info_inset,
                first_inset_ax_container=first_inset_ax_container,
                threshold_label_added_container=threshold_label_added_container,
                threshold_label_added_inset_container=threshold_label_added_inset_container,
            )
            # first_inset_ax_true = first_inset_ax_true or first_inset_ax_container[0]
            panel_axes.append((ax_true, axis_index, False))  # False → CLASS

            main_axes.append(ax_true)

            # --------------------------------------------------------------
            #  after both panels exist → decide whether to show x-tick labels
            # --------------------------------------------------------------
            # if axis_index in bottom_row_indices and axis_index in rightmost_col_indices:
            #     ax_true.tick_params(axis="x", which="both", labelbottom=True)
            #     if iteration_data_chain is not None:
            #         ax_fake.tick_params(axis="x", which="both", labelbottom=True)

            def last_outer_row_idx(col, n_cells, n_cols):
                """
                highest outer-cell index that belongs to this column
                (works even when the grid is ragged)
                """
                last = n_cells - 1
                while last % n_cols != col:
                    last -= 1
                return last

            # ------------- X / Y tick visibility -----------------------------
            col = axis_index % cols
            last_idx = last_outer_row_idx(col, len(plot_data), cols)

            # y-labels only on first column
            ax_true.tick_params(labelleft=(col == 0))
            if iteration_data_chain is not None:
                ax_fake.tick_params(labelleft=(col == 0))

            # x-labels on the CLASS panel of the last row in *this* column
            if axis_index == last_idx:
                ax_true.tick_params(axis="x", which="both", labelbottom=True)
            else:
                ax_true.tick_params(axis="x", which="both", labelbottom=False)

            # the CHAIN panel never shows x-labels
            if iteration_data_chain is not None:
                ax_fake.tick_params(axis="x", which="both", labelbottom=False)

            # if iteration_data_chain is not None:
            #     ax_fake.text(
            #         0.02,
            #         0.96,
            #         "CHAIN",
            #         transform=ax_fake.transAxes,
            #         fontsize=7,
            #         fontweight="bold",
            #         va="top",
            #     )

            # ax_true.text(
            #     0.02,
            #     0.96,
            #     "CLASS",
            #     transform=ax_true.transAxes,
            #     fontsize=7,
            #     fontweight="bold",
            #     va="top",
            # )

            # row_y = np.linspace(0.5 + 1 / rows, 0.5 / rows, rows)  # centre of each row
            # for k, y in enumerate(row_y):
            #     fig.text(
            #         0.995,
            #         y,
            #         "CHAIN" if k % 2 == 0 else "CLASS",
            #         ha="right",
            #         va="center",
            #         fontsize=8,
            #         fontweight="bold",
            #     )

            # if iteration_data_chain is not None:  # <<< NEW
            #     total_panel_rows = rows * 2
            #     for r in range(total_panel_rows):
            #         y = 1 - (r + 0.5) / total_panel_rows
            #         fig.text(
            #             1.005,
            #             y,
            #             "CHAIN" if r % 2 == 0 else "CLASS",
            #             ha="left",
            #             va="center",
            #             fontsize=8,
            #             fontweight="bold",
            #             transform=fig.transFigure,
            #         )

            if iteration_data_chain is not None and axis_index % cols == 0:

                # ---- find the *right-most* outer cell in this row -------------
                row_end_idx = min(axis_index + cols - 1, len(plot_data) - 1)
                last_ax_cell = axes[row_end_idx]  # outer “holder” axis
                bb = last_ax_cell.get_position(fig)

                # -- y-centres of the two stacked panels ------------------------
                bb_fake = ax_fake.get_position(fig)
                bb_true = ax_true.get_position(fig)
                y_chain = 0.5 * (bb_fake.y0 + bb_fake.y1)
                y_class = 0.5 * (bb_true.y0 + bb_true.y1)

                x_txt = bb.x1 + 0.01  # a whisker to the right

                fig.text(
                    x_txt,
                    y_chain,
                    "CHAIN",
                    ha="left",
                    va="center",
                    fontsize=8,
                    fontweight="bold",
                )

                fig.text(
                    x_txt,
                    y_class,
                    "CLASS",
                    ha="left",
                    va="center",
                    fontsize=8,
                    fontweight="bold",
                )

        # # ------------------------------------------------------------------
        # # 5)  SHARED LEGENDS + SUPERTITLE  (unchanged code)
        # # ------------------------------------------------------------------
        # # (Everything from here down to the final savefig is *identical* to
        # #  your original block – copy it without edits.)
        # # ------------------------------------------------------------------
        # # ---------------- copy-start -------------------------------------
        # for label, handle in list(all_handles_labels.items()):
        #     new_label = label
        #     if label in all_legends_info_main[lowest_iteration]:
        #         new_label = all_legends_info_main[lowest_iteration][label]["label"]
        #     if new_label != label:
        #         all_handles_labels[new_label] = handle
        #         del all_handles_labels[label]

        # fig.canvas.draw()
        # legend1 = axes[0].legend(
        #     all_handles_labels.values(),
        #     all_handles_labels.keys(),
        #     loc="upper left",
        #     fontsize=7,
        #     frameon=True,
        #     title="Shared Legend for all Iterations",
        #     title_fontsize="8",
        # )
        # fig.canvas.draw()
        # axes[0].annotate(
        #     f"i = {lowest_iteration}",
        #     xy=(0, 0),
        #     xycoords=legend1,
        #     xytext=(2, -6),
        #     textcoords="offset points",
        #     ha="left",
        #     va="top",
        #     bbox=dict(
        #         facecolor="white", edgecolor="grey", boxstyle="round,pad=0.4", alpha=0.5
        #     ),
        #     fontsize=7,
        # )

        # # per-subplot legends (main)
        # for idx, entry in enumerate(plot_data):
        #     if entry["iteration"] == lowest_iteration:
        #         continue
        #     ax_tmp = axes[idx]
        #     legend_labels, legend_handles = [], []
        #     for lbl in all_legends_info_main[entry["iteration"]]:
        #         legend_labels.append(
        #             all_legends_info_main[entry["iteration"]][lbl]["label"]
        #         )
        #         legend_handles.append(
        #             all_legends_info_main[entry["iteration"]][lbl]["handle"]
        #         )
        #     legend_main = ax_tmp.legend(
        #         legend_handles, legend_labels, loc="upper left", fontsize=7, frameon=True
        #     )
        #     fig.canvas.draw()
        #     ax_tmp.annotate(
        #         f"i = {entry['iteration']}",
        #         xy=(0, 0),
        #         xycoords=legend_main,
        #         xytext=(2, -6),
        #         textcoords="offset points",
        #         ha="left",
        #         va="top",
        #         bbox=dict(
        #             facecolor="white", edgecolor="grey", boxstyle="round,pad=0.4", alpha=0.5
        #         ),
        #         fontsize=7,
        #     )

        # # shared inset legend
        # for label, handle in list(all_handles_labels_inset.items()):
        #     new_label = label
        #     if label in all_legends_info_inset[lowest_iteration_inset]:
        #         new_label = all_legends_info_inset[lowest_iteration_inset][label]["label"]
        #     if new_label != label:
        #         all_handles_labels_inset[new_label] = handle
        #         del all_handles_labels_inset[label]

        # if self.include_inset and first_inset_ax_true or first_inset_ax_fake:
        #     first_inset_ax_true.legend(
        #         all_handles_labels_inset.values(),
        #         all_handles_labels_inset.keys(),
        #         loc="upper left",
        #         fontsize=5,
        #         frameon=True,
        #         title="Shared Legend for all Insets",
        #         title_fontsize="6",
        #     )

        #     if first_inset_ax_fake is not None:
        #         first_inset_ax_fake.legend(
        #             all_handles_labels_inset.values(),
        #             all_handles_labels_inset.keys(),
        #             loc="upper left",
        #             fontsize=5,
        #             frameon=True,
        #             title="Shared Legend for all Insets",
        #             title_fontsize="6",
        #         )

        # ------------------------------------------------------------------
        # 5)  ONE *SUPER* LEGEND  (figure-level, main + inset combined)
        # ------------------------------------------------------------------
        from collections import OrderedDict

        # # (a)  pretty-print labels that carry the “counts” text  -------------
        # for lbl in list(all_handles_labels):
        #     if lbl in all_legends_info_main[lowest_iteration]:
        #         pretty = all_legends_info_main[lowest_iteration][lbl]["label"]
        #         if pretty != lbl:
        #             all_handles_labels[pretty] = all_handles_labels.pop(lbl)

        # for lbl in list(all_handles_labels_inset):
        #     if (
        #         self.include_inset
        #         and lbl in all_legends_info_inset[lowest_iteration_inset]
        #     ):
        #         pretty = all_legends_info_inset[lowest_iteration_inset][lbl]["label"]
        #         if pretty != lbl:
        #             all_handles_labels_inset[pretty] = all_handles_labels_inset.pop(lbl)

        for key in list(all_handles_labels):
            if r"\Delta\chi^2" in key:  # detected threshold line
                all_handles_labels[r"$\Delta\chi^2$-Threshold"] = (
                    all_handles_labels.pop(key)
                )  # ← keep same handle

        fig.canvas.draw()

        # (b)  merge MAIN+INSET handles – keep MAIN order, append new INSET --
        combined_handles = OrderedDict()
        combined_handles.update(all_handles_labels)
        for lbl, hnd in all_handles_labels_inset.items():
            if lbl not in combined_handles:
                combined_handles[lbl] = hnd  # only add if not already present

        # (c)  single super legend ------------------------------------------
        fig.legend(
            combined_handles.values(),
            combined_handles.keys(),
            loc="upper center",
            bbox_to_anchor=(0.5, 1.05),  # one legend, slightly higher
            ncol=min(len(combined_handles), 8),
            frameon=True,
            fontsize=7,
            title="Shared Legend for all Iterations & Insets",
            title_fontsize="8",
        )

        # leave headroom so the super-legend isn’t cut off
        plt.subplots_adjust(top=0.83)

        # ------------------------------------------------------------------
        # 5-bis)  PER-SUBPLOT “COUNTS-ONLY” LEGENDS  (like the original code)
        #         ───────────────────────────────────────────────────────────
        #         • Every main panel keeps its own legend that shows the
        #           sample counts (including the first iteration).
        #         • We ALSO annotate each legend with “i = …”.
        # ------------------------------------------------------------------
        # for idx, entry in enumerate(plot_data):

        #     # axis that corresponds to this iteration
        #     # ax_tmp = axes[idx]
        #     ax_tmp = main_axes[idx]

        #     # legend_labels = []
        #     # legend_handles = []
        #     # for lbl in all_legends_info_main[entry["iteration"]]:
        #     #     legend_labels.append(
        #     #         all_legends_info_main[entry["iteration"]][lbl]["label"]
        #     #     )
        #     #     legend_handles.append(
        #     #         all_legends_info_main[entry["iteration"]][lbl]["handle"]
        #     #     )

        #     # legend_handles = [
        #     #     info["handle"]
        #     #     for info in all_legends_info_main[entry["iteration"]].values()
        #     #     if "handle" in info
        #     # ]
        #     legend_handles = [
        #         info["handle"]
        #         for info in all_legends_info_main[entry["iteration"]].values()
        #         if "handle" in info
        #     ]
        #     # legend_labels = [
        #     #     info["label"]
        #     #     for info in all_legends_info_main[entry["iteration"]].values()
        #     #     if "handle" in info
        #     # ]
        #     legend_labels = [
        #         info["label"]
        #         for info in all_legends_info_main[entry["iteration"]].values()
        #         if "handle" in info
        #     ]

        #     legend_main = ax_tmp.legend(
        #         legend_handles,
        #         legend_labels,
        #         loc="upper left",
        #         fontsize=7,
        #         frameon=True,
        #     )

        #     # small annotation “i = …” inside that legend’s BBox
        #     fig.canvas.draw()  # make sure bbox exists
        #     ax_tmp.annotate(
        #         f"i = {entry['iteration']}",
        #         xy=(0, 0),
        #         xycoords=legend_main,
        #         xytext=(2, -6),
        #         textcoords="offset points",
        #         ha="left",
        #         va="top",
        #         bbox=dict(
        #             facecolor="white",
        #             edgecolor="grey",
        #             boxstyle="round,pad=0.4",
        #             alpha=0.5,
        #         ),
        #         fontsize=7,
        #     )

        # ------------------------------------------------------------------
        # 5-bis)  one counts-only legend inside *every* panel
        # ------------------------------------------------------------------

        for ax, outer_idx, is_fake in panel_axes:  # <<< MOD
            key = outer_idx if not is_fake else (outer_idx, "fake")  # <<< NEW
            infos = all_legends_info_main[key].values()  # <<< MOD
            handles = [i["handle"] for i in infos if "handle" in i]
            labels = [i["label"] for i in infos if "handle" in i]

            leg = ax.legend(
                handles,
                labels,
                loc="upper left",
                fontsize=7,
                frameon=True,
            )

            fig.canvas.draw()  # so bbox exists
            ax.annotate(
                f"i = {outer_idx}",
                xy=(0, 0),
                xycoords=leg,
                xytext=(2, -6),
                textcoords="offset points",
                ha="left",
                va="top",
                bbox=dict(
                    facecolor="white",
                    edgecolor="grey",
                    boxstyle="round,pad=0.4",
                    alpha=0.5,
                ),
                fontsize=7,
            )

        # ------------------------------------------------------------------
        # 6)  HEAVIER BORDER AROUND EACH TWO-PANEL CELL         (3)
        # ------------------------------------------------------------------
        for ax_cell in axes[: len(plot_data)]:  # <<< NEW
            for spine in ax_cell.spines.values():
                spine.set_linewidth(1.5)

        # ------------------------------------------------------------------
        # 7)  tick visibility – only left / bottom                 (2)
        # ------------------------------------------------------------------
        # for k, ax_true in enumerate(main_axes):  # <<< NEW

        #     ax_true.tick_params(labelleft=False, labelbottom=False)
        #     if k % cols == 0:
        #         ax_true.tick_params(labelleft=True)
        #     if k // cols == rows - 1:
        #         ax_true.tick_params(labelbottom=True)

        for ax, _, is_fake in panel_axes:

            spec = ax.get_subplotspec().get_topmost_subplotspec()

            # y-ticks on first outer column
            ax.tick_params(labelleft=spec.is_first_col())

            # x-ticks only on CLASS panels that are on the last outer row
            ax.tick_params(labelbottom=(not is_fake) and spec.is_last_row())

        # hide unused outer cells
        for j in range(len(plot_data), len(axes)):
            axes[j].axis("off")

        if metric == "delta_chi_sq":
            fig.supxlabel(r"$\Delta\chi^2$", fontsize=12, y=0.05)
        else:
            fig.supxlabel(r"$-\log(\mathcal{L})$", fontsize=12, y=0.05)
        fig.supylabel("Counts" if stat == "count" else "Density", fontsize=12, x=0.08)

        plt.subplots_adjust(wspace=0, hspace=0)
        for fmt in self.save_formats:
            plot_filename = f"per_iteration_{metric}_hist_{stat}.{fmt}"
            save_path = os.path.join(self.analysis_dir, plot_filename)
            plt.savefig(save_path, bbox_inches="tight", dpi=1000)
        plt.close(fig)
        if self.verbose >= 1:
            print(
                f"[_plot_per_iteration_hist] Saved histograms ({metric}, {stat}) to {save_path}"
            )
        # ---------------- copy-end ---------------------------------------

    # ==================================================================
    #  _plot_one_panel  — ORIGINAL CODE MOVED UNCHANGED
    # ==================================================================
    def _plot_one_panel(
        self,
        ax,
        iteration,
        lowest_iteration,
        lowest_iteration_inset,
        metric,
        stat,
        global_bin_edges,
        x_min_inset,
        x_max_inset,
        y_min_inset,
        y_max_inset,
        main_arrays,
        inset_arrays,
        best_fit_loglkl,
        is_fake,
        all_handles_labels,
        all_handles_labels_inset,
        all_legends_info_main,
        all_legends_info_inset,
        first_inset_ax_container,
        threshold_label_added_container,
        threshold_label_added_inset_container,
    ):
        """
        This is **literally** your old inner-loop, parameterised so
        we can reuse it for CHAIN and CLASS.  Nothing has been edited
        except that we:
        • receive `main_arrays` / `inset_arrays` as arguments
        • pick the correct count-dictionary depending on `is_fake`
        """

        # pick counts dict
        cp = (
            self.count_points_chain if is_fake else self.count_points
        )  # <<< NEW (choose counts)

        # NEW: pick a dict-key that keeps CLASS and CHAIN separate
        dict_key = iteration if not is_fake else (iteration, "fake")  # <<< NEW
        all_legends_info_main.setdefault(dict_key, {})  # <<< NEW

        delta_threshold = getattr(self.param_connect, "delta_chi2_threshold", None)
        if isinstance(delta_threshold, list):
            if iteration < len(delta_threshold):
                delta_threshold = delta_threshold[iteration]
            else:
                delta_threshold = delta_threshold[-1]

        # ----------------------------------------------------------------
        # Everything below is 100 % the original code – only `cp`
        # substituted where it used to be `self.count_points`.
        # ----------------------------------------------------------------
        # all_legends_info_main.setdefault(iteration, {})
        ax.set_axisbelow(True)
        ax.grid(True, which="major", linestyle="-", linewidth=0.5, alpha=0.5)

        # --- Plot main panel categories ----------------------------------
        for cat, arr, item_cfg in main_arrays:
            color = item_cfg.get("color", None)
            label = item_cfg.get("label", None)
            plot_kws = item_cfg.get("plot_kws", {}).copy()
            plot_kws["bins"] = global_bin_edges
            if color is None:
                color = self.DATA_COLORS.get(cat, "gray")
            if label is None:
                label = self.category_labels.get(cat, cat)

            if metric == "delta_chi_sq":
                key = f"delta_chi_sq_{cat}"
            else:
                key = cat

            display = cp[iteration][key]["display"]
            original = cp[iteration][key]["original"]
            counts_str = (
                f"{original} points"
                if display is None or display == original
                else f"showing $\\frac{{{display}}}{{{original}}}$ points"
            )
            all_legends_info_main[dict_key][label] = {"label": counts_str}

            final_plot_kws = dict(plot_kws)
            final_plot_kws.setdefault("color", color)
            final_plot_kws.setdefault("label", label)
            final_plot_kws.setdefault("stat", stat)
            if len(arr) > 0:
                sns.histplot(x=arr, ax=ax, **final_plot_kws)

        # Optional vertical line -----------------------------------------
        if metric == "delta_chi_sq" and delta_threshold is not None:
            delta_label = (
                f"$\\Delta\\chi^2\\mathrm{{-Threshold}}={delta_threshold:.1f}$"
                if delta_threshold < 10000
                else (
                    r"$\Delta\chi^2\mathrm{-Threshold}="
                    + self.sci_notation_latex(delta_threshold)
                    + "$"
                )
            )
            delta_value = (
                f"{delta_threshold:.1f}"
                if delta_threshold < 10000
                else f"${self.sci_notation_latex(delta_threshold)}$"
            )
            ax.axvline(
                delta_threshold, color="purple", linestyle="--", label=delta_label
            )
            all_legends_info_main[dict_key][delta_label] = {"label": delta_value}

        elif (
            metric == "loglkl"
            and best_fit_loglkl is not None
            and not np.isnan(best_fit_loglkl)
        ):
            bf_label = (
                f"$\\mathrm{{Best-Fit:}}\\; -\\log(\\mathcal{{L}}) = {best_fit_loglkl:.1f}$"
                if best_fit_loglkl < 10000
                else (
                    r"$\mathrm{Best-Fit:}\; -\log(\mathcal{L}) = "
                    + self.sci_notation_latex(best_fit_loglkl)
                    + "$"
                )
            )
            bf_value = (
                f"{best_fit_loglkl:.1f}"
                if best_fit_loglkl < 10000
                else self.sci_notation_latex(best_fit_loglkl)
            )
            ax.axvline(best_fit_loglkl, color="purple", linestyle="--", label=bf_label)
            all_legends_info_main[dict_key][bf_label] = {"label": bf_value}

        # ---- INSET ------------------------------------------------------
        if self.include_inset and inset_arrays:
            ax_inset = inset_axes(
                ax, width="40%", height="40%", loc="upper right", borderpad=1
            )
            ax_inset.set_axisbelow(True)
            ax_inset.grid(True, which="major", linestyle="-", linewidth=0.5, alpha=0.5)
            # all_legends_info_inset.setdefault(iteration, {})
            all_legends_info_inset.setdefault(dict_key, {})  # <<< NEW

            if first_inset_ax_container[0] is None:
                first_inset_ax_container[0] = ax_inset

            for cat, arr, item_cfg in inset_arrays:
                color = item_cfg.get("color", None) or self.DATA_COLORS.get(cat, "gray")
                label = item_cfg.get("label", None) or self.category_labels.get(
                    cat, cat
                )
                plot_kws = item_cfg.get("plot_kws", {}).copy()
                plot_kws["bins"] = global_bin_edges
                plot_kws.setdefault("color", color)
                plot_kws.setdefault("label", label)
                plot_kws.setdefault("stat", stat)
                if len(arr) == 0:
                    continue
                sns.histplot(x=arr, ax=ax_inset, **plot_kws)

                if metric == "delta_chi_sq":
                    key = f"delta_chi_sq_{cat}"
                else:
                    key = cat

                display = cp[iteration][key]["display"]
                original = cp[iteration][key]["original"]
                counts_str = (
                    f"{original} points"
                    if display is None or display == original
                    else f"showing $\\frac{{{display}}}{{{original}}}$ points"
                )
                all_legends_info_inset[dict_key][label] = {"label": counts_str}

            if x_min_inset is not None and x_max_inset is not None:
                ax_inset.set_xlim(x_min_inset, x_max_inset)
            if y_min_inset is not None and y_max_inset is not None:
                ax_inset.set_ylim(y_min_inset, y_max_inset)

            # threshold / best-fit lines duplicated in inset
            if metric == "delta_chi_sq" and delta_threshold is not None:
                ax_inset.axvline(
                    delta_threshold, color="purple", linestyle="--", linewidth=1
                )

                all_legends_info_inset[dict_key][delta_label] = {"label": delta_value}
            elif (
                metric == "loglkl"
                and best_fit_loglkl is not None
                and not np.isnan(best_fit_loglkl)
            ):
                ax_inset.axvline(
                    best_fit_loglkl, color="purple", linestyle="--", linewidth=1
                )
                all_legends_info_inset[dict_key][bf_label] = {"label": bf_value}

            ax_inset.tick_params(axis="both", labelsize=6)
            ax_inset.set_xlabel("")
            ax_inset.set_ylabel("")
            ax_inset.minorticks_on()

            handles, labels = ax_inset.get_legend_handles_labels()
            for h, l in zip(handles, labels):
                # all_legends_info_inset[iteration].setdefault(l, {})["handle"] = h
                # if r"\Delta\chi^2\mathrm{-Threshold}=" in l or r"\mathrm{Best-Fit" in l:
                #     if not threshold_label_added_inset_container[0]:
                #         all_handles_labels_inset[l] = h
                #         threshold_label_added_inset_container[0] = True
                #     continue

                # 1. ALWAYS keep the handle for the per-subplot legend
                info = all_legends_info_inset[dict_key].setdefault(l, {})
                info["handle"] = h

                # 2. Add this artist to the *super* inset-legend only once
                if r"\Delta\chi^2" in l or r"\mathrm{Best-Fit" in l:
                    if not threshold_label_added_inset_container[0]:
                        all_handles_labels_inset[l] = h
                        threshold_label_added_inset_container[0] = True
                # no `continue` → we’ve already stored the handle; the rest of
                #   the loop body is harmless so we just fall through
                else:
                    if l not in all_handles_labels_inset:
                        all_handles_labels_inset[l] = h

            if self.x_range_inset is not None:
                ax_inset.set_xlim(self.x_range_inset)
            if self.y_range_inset is not None:
                ax_inset.set_ylim(self.y_range_inset)
            # per-subplot legends (inset)

            if iteration > lowest_iteration_inset:
                # legend_labels = [
                #     all_legends_info_inset[iteration][lbl]["label"]
                #     for lbl in all_legends_info_inset[iteration]
                # ]
                # legend_handles = [
                #     all_legends_info_inset[iteration][lbl]["handle"]
                #     for lbl in all_legends_info_inset[iteration]
                # ]

                legend_handles = [
                    info["handle"]
                    for info in all_legends_info_inset[dict_key].values()
                    if "handle" in info
                ]
                legend_labels = [
                    info["label"]
                    for info in all_legends_info_inset[dict_key].values()
                    if "handle" in info
                ]

                ax_inset.legend(
                    legend_handles,
                    legend_labels,
                    loc="upper left",
                    fontsize=6,
                    frameon=True,
                )
        # ------ MAIN legend bookkeeping ---------------------------------
        handles, labels = ax.get_legend_handles_labels()
        for h, l in zip(handles, labels):
            # all_legends_info_main[iteration].setdefault(l, {})["handle"] = h
            # if r"\Delta\chi^2\mathrm{-Threshold}=" in l or r"\mathrm{Best-Fit" in l:
            #     if not threshold_label_added_container[0]:
            #         all_handles_labels[l] = h
            #         threshold_label_added_container[0] = True
            #     continue
            # if l not in all_handles_labels:
            #     all_handles_labels[l] = h

            info = all_legends_info_main[dict_key].setdefault(l, {})
            info["handle"] = h

            # 1. ALWAYS keep the handle for the per-subplot legend
            if r"\Delta\chi^2" in l or r"\mathrm{Best-Fit" in l:
                if not threshold_label_added_container[0]:
                    all_handles_labels[l] = h
                    threshold_label_added_container[0] = True
            # no `continue` → we’ve already stored the handle; the rest of
            #   the loop body is harmless so we just fall through
            else:
                if l not in all_handles_labels:
                    all_handles_labels[l] = h
        # ----------------------------------------------------------------

        from matplotlib.ticker import LogLocator

        ax.xaxis.set_minor_locator(LogLocator(base=10.0, subs="auto", numticks=None))
        ax.minorticks_on()
        ax.set_xlabel("")
        ax.set_ylabel("")

        # custom x/y-range overrides (unchanged)
        if self.x_range is not None:
            ax.set_xlim(self.x_range)
        if self.y_range is not None:
            ax.set_ylim(self.y_range)

    def _compute_global_inset_ranges(self, all_inset_plots, metric, stat, bins="auto"):
        """
        Create temporary figures for each array that will be plotted in insets,
        so we can gather the global x/y axis ranges that Seaborn 'auto' picks.

        Parameters
        ----------
        all_inset_plots : list of (np.ndarray, dict)
            A list of tuples containing:
            (arr, item_cfg)
            where arr is the 1D array of data and item_cfg is the config
            dictionary from self.hist_inset (or similar).
        metric : str
            'delta_chi_sq' or 'loglkl'
        stat : str
            'count' or 'density'

        Returns
        -------
        (global_xmin, global_xmax, global_ymin, global_ymax) : floats
            The overall min/max x and y axis values across all inset histplots.
        """

        matplotlib.use("Agg")
        global_xmin = float("inf")
        global_xmax = float("-inf")
        global_ymin = float("inf")
        global_ymax = float("-inf")

        # We'll create a new dummy figure/axes for each array,
        # so that Seaborn's 'auto' logic runs individually.
        for arr, item_cfg in all_inset_plots:
            # Skip empty arrays just in case
            if len(arr) == 0:
                continue

            # Prepare a dummy figure
            fig, ax = plt.subplots()

            # Build final_plot_kws similarly to how you do in the real pass
            final_plot_kws = dict(item_cfg.get("plot_kws", {}))
            final_plot_kws.setdefault("stat", stat)
            final_plot_kws.setdefault("label", item_cfg.get("label", ""))
            final_plot_kws.setdefault("color", item_cfg.get("color", "gray"))
            final_plot_kws.setdefault("bins", bins)

            # Actually plot
            sns.histplot(x=arr, ax=ax, **final_plot_kws)

            # Read off the x/y limits that Seaborn assigned
            xlims = ax.get_xlim()
            ylims = ax.get_ylim()

            # Update global
            global_xmin = min(global_xmin, xlims[0])
            global_xmax = max(global_xmax, xlims[1])
            global_ymin = min(global_ymin, ylims[0])
            global_ymax = max(global_ymax, ylims[1])

            # Clean up
            plt.close(fig)

        return global_xmin, global_xmax, global_ymin, global_ymax

    def _save_iteration_summaries(self, iteration_data):
        """
        Compute summary statistics (including quantiles) per iteration & per category,
        then save as CSV, LaTeX, and Markdown.
        """

        if self.verbose >= 2:
            print("[_save_iteration_summaries] Computing per iteration statistics.")

        rows = []
        for iteration, i_data in iteration_data.items():
            for cat in self.data_categories:
                loglkl_arr = i_data.get(cat, np.array([], dtype=np.float32))
                delta_arr = i_data.get(
                    f"delta_chi_sq_{cat}", np.array([], dtype=np.float32)
                )

                # LogLKL stats
                if len(loglkl_arr) > 0:
                    stats_lkl = self._compute_extended_stats(loglkl_arr)
                    stats_lkl.update(
                        {"iteration": iteration, "set": cat, "metric": "loglkl"}
                    )
                    rows.append(stats_lkl)

                # Delta chi^2 stats
                if len(delta_arr) > 0:
                    stats_delta = self._compute_extended_stats(delta_arr)
                    stats_delta.update(
                        {"iteration": iteration, "set": cat, "metric": "delta_chi_sq"}
                    )
                    rows.append(stats_delta)

        if not rows:
            if self.verbose >= 2:
                print("[_save_iteration_summaries] No data found for summary.")
            return

        # Convert to DataFrame and save
        summary_df = pd.DataFrame(rows)
        os.makedirs(os.path.join(self.analysis_dir, "summary_table"), exist_ok=True)
        summary_path = os.path.join(
            self.analysis_dir, "summary_table/per_iteration_likelihood_summary.csv"
        )
        summary_df.to_csv(summary_path, index=False)
        if self.verbose >= 1:
            print(f"[_save_iteration_summaries] Saved summaries to {summary_path}")

        # Formatted tables
        table = tabulate(
            summary_df, headers="keys", tablefmt="grid", floatfmt=".3f", showindex=False
        )

        if self.verbose >= 4:
            print("[_save_iteration_summaries] Summary Table:")
            print(table)

        # Create self.analysis_dir/summary_table/ if it doesn't exist
        os.makedirs(os.path.join(self.analysis_dir, "summary_table/"), exist_ok=True)
        latex_table_path = os.path.join(
            self.analysis_dir, "summary_table/", "per_iteration_summary.tex"
        )
        with open(latex_table_path, "w") as f:
            latex_table = tabulate(
                summary_df, headers="keys", tablefmt="latex", floatfmt=".3f"
            )
            f.write(latex_table)
        if self.verbose >= 2:
            print(
                f"[_save_iteration_summaries] Saved LaTeX table to {latex_table_path}"
            )

        markdown_table_path = os.path.join(
            self.analysis_dir, "summary_table/", "per_iteration_summary.md"
        )
        with open(markdown_table_path, "w") as f:
            markdown_table = tabulate(
                summary_df, headers="keys", tablefmt="pipe", floatfmt=".3f"
            )
            f.write(markdown_table)
        if self.verbose >= 1:
            print(
                f"[_save_iteration_summaries] Saved Markdown table to {markdown_table_path}"
            )

    def _compute_extended_stats(self, array):
        """Compute extended stats: min, max, mean, median, std, and specified quantiles."""
        if len(array) == 0:
            return {
                "min": np.nan,
                "max": np.nan,
                "mean": np.nan,
                "median": np.nan,
                "std": np.nan,
            }

        result = {
            "min": float(np.min(array)),
            "max": float(np.max(array)),
            "mean": float(np.mean(array)),
            "median": float(np.median(array)),
            "std": float(np.std(array)),
        }
        qvals = np.quantile(array, self.quantiles)
        for q, val in zip(self.quantiles, qvals):
            result[f"q_{int(q*100)}"] = float(val)
        return result

    def _find_last_complete_iteration(self):
        """
        Determines the last complete iteration by ensuring that at least one required key
        contains valid data in each iteration.

        Returns
        -------
        last_complete_it : int or None
            The number of the last complete iteration, or None if none are complete.
        """
        required_keys = ["accepted_new", "discarded_iteration", "discarded_likelihood"]
        all_iters = sorted(self.data["iteration_data"].keys())

        last_complete_it = None

        for it in all_iters:
            iteration_data = self.data["iteration_data"].get(it, {})
            # Check if **all** required keys have empty or None parameters
            all_empty = True
            for key in required_keys:
                block = iteration_data.get(key, {})
                df = block.get("parameters", None)
                if df is not None and not (isinstance(df, pd.DataFrame) and df.empty):
                    # At least one key has valid data
                    all_empty = False
                    if self.verbose >= 3:
                        print(f"Iteration {it}: Key '{key}' has data.")
                    break
                else:
                    if self.verbose >= 3:
                        print(f"Iteration {it}: Key '{key}' is empty or None.")
            if all_empty:
                # Incomplete iteration detected
                if self.verbose >= 2:
                    print(
                        f"Iteration {it} is incomplete. Stopping at iteration {last_complete_it}."
                    )
                break
            else:
                # Complete iteration; update last_complete_it
                last_complete_it = it
                if self.verbose >= 3:
                    print(f"Iteration {it} is complete.")

        return last_complete_it

    # def _create_dummy_df(self, iteration_data, metric="delta_chi_sq"):
    #     """
    #     Create a combined histogram across selected categories and iterations using Seaborn's histplot.

    #     Parameters
    #     ----------
    #     iteration_data : dict
    #         The dictionary returned by _gather_iteration_data().
    #     metric : str
    #         The metric to plot, e.g., 'delta_chi_sq' or 'loglkl'.
    #     """
    #     if self.verbose >= 2:
    #         print(
    #             f"[_create_dummy_df] Creating dummy DataFrame for {metric} combined histogram."
    #         )

    #     # Combine categories from main panel and (optionally) inset
    #     selected_categories = {panel["category"] for panel in self.hist_main_panel}
    #     if self.include_inset:
    #         selected_categories.update(panel["category"] for panel in self.hist_inset)

    #     # 1. Gather all data into a DataFrame
    #     plot_rows = []
    #     for iteration, i_data in iteration_data.items():
    #         for cat in selected_categories:  # Only include selected categories
    #             if metric == "delta_chi_sq":
    #                 arr = i_data.get(
    #                     f"delta_chi_sq_{cat}", np.array([], dtype=np.float32)
    #                 )
    #             else:  # loglkl
    #                 arr = i_data.get(cat, np.array([], dtype=np.float32))

    #             if len(arr) == 0:
    #                 continue  # Skip empty categories

    #             # Create a unique identifier for category-iteration
    #             category_iteration = (
    #                 f"{self.category_labels.get(cat, cat)}_i{iteration}"
    #             )

    #             for val in arr:
    #                 plot_rows.append(
    #                     {"value": val, "category_iteration": category_iteration}
    #                 )

    #     if not plot_rows:
    #         if self.verbose >= 2:
    #             print(
    #                 f"[_create_dummy_df] No data available for {metric} combined histogram."
    #             )
    #         return

    #     df = pd.DataFrame(plot_rows)

    #     return df

    def _create_dummy_df(
        self,
        iteration_data,
        iteration_data_chain=None,
        metric="delta_chi_sq",
    ):
        """
        Create a combined histogram DataFrame across selected categories and
        iterations, for both CLASS-based (true) and optional CHAIN-based (fake)
        likelihoods.

        Parameters
        ----------
        iteration_data : dict
            The dict returned by _gather_iteration_data().
        iteration_data_chain : dict or None
            If provided, a parallel dict returned by _gather_iteration_data_chain().
            Rows from this dataset will be tagged "chain" in the 'source' column.
        metric : str
            'delta_chi_sq' or 'loglkl'.
        Returns
        -------
        df : pandas.DataFrame or None
            Columns: value, category_iteration, source.  Returns None if no data.
        """
        if self.verbose >= 2:
            print(f"[_create_dummy_df] Building histogram DataFrame for {metric}")

        # which categories to include
        selected = {p["category"] for p in self.hist_main_panel}
        if self.include_inset:
            selected.update({p["category"] for p in self.hist_inset})

        rows = []

        def gather(data_dict, source_label):
            """Helper to pull rows from one iteration_data dict."""
            for it, i_data in data_dict.items():
                for cat in selected:
                    key = f"{metric}_{cat}" if metric == "delta_chi_sq" else cat
                    arr = i_data.get(key, np.array([], dtype=np.float32))
                    if len(arr) == 0:
                        continue
                    label = self.category_labels.get(cat, cat)
                    # unique identifier per iteration+category
                    cat_it = f"{label}_i{it}"
                    for v in arr:
                        rows.append(
                            {
                                "value": v,
                                "category_iteration": cat_it,
                            }
                        )

        # gather CLASS (true) data
        gather(iteration_data, "class")

        # optionally gather CHAIN (fake) data
        if iteration_data_chain is not None:
            gather(iteration_data_chain, "chain")

        if not rows:
            if self.verbose >= 2:
                print(f"[_create_dummy_df] No data available for {metric}")
            return None

        df = pd.DataFrame(rows)
        return df

    def compute_global_bin_edges_seaborn_internal(
        self,
        data=None,
        *,
        # Vector variables
        x=None,
        y=None,
        hue=None,
        weights=None,
        # Histogram computation parameters
        stat="count",
        bins="auto",
        binwidth=None,
        binrange=None,
        discrete=None,
        cumulative=False,
        common_bins=True,
        common_norm=True,
        # Histogram appearance parameters
        multiple="layer",
        element="bars",
        fill=True,
        shrink=1,
        # Histogram smoothing with a kernel density estimate
        kde=False,
        kde_kws=None,
        line_kws=None,
        # Bivariate histogram parameters
        thresh=0,
        pthresh=None,
        pmax=None,
        cbar=False,
        cbar_ax=None,
        cbar_kws=None,
        # Hue mapping parameters
        palette=None,
        hue_order=None,
        hue_norm=None,
        color=None,
        # Axes information
        log_scale=None,
        legend=True,
        ax=None,
        # Other appearance keywords
        **kwargs,
    ):
        """
        Use Seaborn's internal _DistributionPlotter with a custom Hist that
        intercepts the bin edges. Returns the bin edges exactly as Seaborn
        would compute them, for univariate data.
        """

        # 1) Initialize our custom plotter
        p = Analyze_likelihoods.retrieve_bin_edges(
            data=data,
            variables=dict(x=x, y=y, hue=hue, weights=weights),  # univariate => x
        )

        p.map_hue(palette=palette, order=hue_order, norm=hue_norm)

        if ax is None:
            ax = plt.gca()

        p._attach(ax, log_scale=log_scale)

        from seaborn.utils import _default_color  # an internal function

        if p.univariate:  # Note, bivariate plots won't cycle
            if fill:
                method = ax.bar if element == "bars" else ax.fill_between
            else:
                method = ax.plot
            color = _default_color(method, hue, color, kwargs)

        if not p.has_xy_data:
            return ax

        # Default to discrete bins for categorical variables
        if discrete is None:
            discrete = p._default_discrete()

        estimate_kws = dict(
            stat=stat,
            bins=bins,
            binwidth=binwidth,
            binrange=binrange,
            discrete=discrete,
            cumulative=cumulative,
        )

        if p.univariate:

            bin_edges = p.plot_univariate_histogram(
                multiple=multiple,
                element=element,
                fill=fill,
                shrink=shrink,
                common_norm=common_norm,
                common_bins=common_bins,
                kde=kde,
                kde_kws=kde_kws,
                color=color,
                legend=legend,
                estimate_kws=estimate_kws,
                line_kws=line_kws,
                **kwargs,
            )

        return bin_edges

    class EdgeCatcherHist(Hist):
        """
        Subclass of Hist that intercepts the bin edges so we can store them.
        """

        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.captured_bin_edges = None  # We'll store the edges here

        def _eval(self, data, orient, bin_kws):

            vals = data[orient]
            weights = data.get("weight", None)

            density = self.stat == "density"
            hist, edges = np.histogram(
                vals, **bin_kws, weights=weights, density=density
            )
            self.captured_bin_edges = edges

            width = np.diff(edges)
            center = edges[:-1] + width / 2

            return pd.DataFrame({orient: center, "count": hist, "space": width})

    class retrieve_bin_edges(_DistributionPlotter):

        def plot_univariate_histogram(
            self,
            multiple,
            element,
            fill,
            common_norm,
            common_bins,
            shrink,
            kde,
            kde_kws,
            color,
            legend,
            line_kws,
            estimate_kws,
            **plot_kws,
        ):

            # -- Default keyword dicts
            kde_kws = {} if kde_kws is None else kde_kws.copy()
            line_kws = {} if line_kws is None else line_kws.copy()
            estimate_kws = {} if estimate_kws is None else estimate_kws.copy()

            # --  Input checking
            from seaborn.utils import _check_argument  # an internal check

            _check_argument("multiple", ["layer", "stack", "fill", "dodge"], multiple)
            _check_argument("element", ["bars", "step", "poly"], element)

            auto_bins_with_weights = (
                "weights" in self.variables
                and estimate_kws["bins"] == "auto"
                and estimate_kws["binwidth"] is None
                and not estimate_kws["discrete"]
            )
            if auto_bins_with_weights:
                msg = (
                    "`bins` cannot be 'auto' when using weights. "
                    "Setting `bins=10`, but you will likely want to adjust."
                )
                warnings.warn(msg, UserWarning)
                estimate_kws["bins"] = 10

            # Simplify downstream code if we are not normalizing
            if estimate_kws["stat"] == "count":
                common_norm = False

            orient = self.data_variable

            # Now initialize the Histogram estimator
            estimator = Analyze_likelihoods.EdgeCatcherHist(**estimate_kws)
            histograms = {}

            # Do pre-compute housekeeping related to multiple groups
            all_data = self.comp_data.dropna()
            all_weights = all_data.get("weights", None)

            multiple_histograms = set(self.variables) - {"x", "y"}
            if multiple_histograms:
                if common_bins:
                    bin_kws = estimator._define_bin_params(all_data, orient, None)
            else:
                common_norm = False

            if common_norm and all_weights is not None:
                whole_weight = all_weights.sum()
            else:
                whole_weight = len(all_data)

            # Estimate the smoothed kernel densities, for use later
            if kde:
                # TODO alternatively, clip at min/max bins?
                kde_kws.setdefault("cut", 0)
                kde_kws["cumulative"] = estimate_kws["cumulative"]
                densities = self._compute_univariate_density(
                    self.data_variable,
                    common_norm,
                    common_bins,
                    kde_kws,
                    warn_singular=False,
                )

            # First pass through the data to compute the histograms
            for sub_vars, sub_data in self.iter_data("hue", from_comp_data=True):

                # Prepare the relevant data
                key = tuple(sub_vars.items())
                orient = self.data_variable

                if "weights" in self.variables:
                    sub_data["weight"] = sub_data.pop("weights")
                    part_weight = sub_data["weight"].sum()
                else:
                    part_weight = len(sub_data)

                # Do the histogram computation
                if not (multiple_histograms and common_bins):
                    bin_kws = estimator._define_bin_params(sub_data, orient, None)
                res = estimator._normalize(estimator._eval(sub_data, orient, bin_kws))

                return estimator.captured_bin_edges

    def _plot_likelihood_evolution(self, iteration_data):
        """
        Creates a single figure (Figure 1) showing how the likelihood filter evolves over iterations.
        We plot four metrics:

        1) Best-Fit -log(L) per iteration
        2) Average -log(L) among newly accepted points in iteration i
        3) Ratio of newly discarded (discarded_likelihood_new + discarded_likelihood_old) (in iteration i) to newly accepted (in iteration i)
        4) (Optional) Fraction of newly accepted points whose delta_chi^2 > param_connect.delta_chi2_threshold

        CHOICES / DEFINITIONS:
        -------------------------------------------------------------------------
        - 'accepted_new': Points newly generated at iteration i that were accepted.
        - 'accepted_old': Points accepted in previous iterations but remain accepted
        in iteration i. (We do NOT use these in our ratio or average, because we
        focus on how iteration i changes the sample.)
        - 'discarded_likelihood_new': Points newly generated in iteration i but
        discarded by the likelihood filter.
        - 'discarded_likelihood_old': Points that were accepted in a previous iteration
        but got discarded in iteration i.
        - 'discarded_iteration': If the entire iteration is discarded for some reason.

        By focusing on “accepted_new” and “discarded_*_new”, we capture
        the newly generated points’ fate at each iteration. This helps diagnose if
        each iteration is producing mostly good or mostly bad points, and how
        the filter is performing on new proposals.

        If you instead want to look at “accumulated” accepted points’ average likelihood,
        just replace references to 'accepted_new' with the union of 'accepted_new' + 'accepted_old'.

        Parameters
        ----------
        iteration_data : dict
            Dictionary from _gather_iteration_data(), with keys = iteration numbers.
            Each iteration i_data has:
            - i_data['best_fit_loglkl']: float or None (best-fit -log(L) so far)
            - i_data['accepted_new']: array of -log(L) for newly accepted points
            - i_data['discarded_likelihood_new']: array of -log(L) for newly generated but discarded
            - i_data['discarded_likelihood_old']: ...
            - i_data['discarded_iteration']: ...
            - i_data['delta_chi_sq_accepted_new']: array of delta-chi^2, etc.
            ...
        """

        if self.verbose >= 2:
            print("[_plot_likelihood_evolution] Creating likelihood-evolution figure ")

        matplotlib.rcParams.update(matplotlib.rcParamsDefault)
        matplotlib.use("Agg")
        # ------------------------------------------------------
        # 1) Prepare arrays for each iteration
        # ------------------------------------------------------
        iterations = []
        best_fit_list = []  # best-fit -log(L) (accumulated or “global” best)
        avg_new_accepted_list = (
            []
        )  # average -log(L) for newly accepted points in iteration i
        ratio_discarded_list = []  # ratio (discarded_new / accepted_new)
        fraction_above_thresh_list = (
            []
        )  # fraction of newly accepted whose delta_chi^2 > threshold

        # Sort iteration keys
        sorted_iters = sorted(iteration_data.keys())

        for it in sorted_iters:
            i_data = iteration_data[it]
            if "best_fit_loglkl" not in i_data:
                # skip incomplete iteration
                continue

            chi2_threshold = getattr(self.param_connect, "delta_chi2_threshold", None)
            # if chi2_threshold is a list:
            if isinstance(chi2_threshold, list):
                if it < len(chi2_threshold):
                    chi2_threshold = chi2_threshold[it]
                else:
                    chi2_threshold = chi2_threshold[-1]

            # Record iteration number
            iterations.append(it)

            # 1a) Best-fit -log(L)
            bf_ll = i_data["best_fit_loglkl"]
            if bf_ll is not None:
                best_fit_list.append(bf_ll)
            else:
                best_fit_list.append(float("nan"))

            # 1b) Average -log(L) among newly accepted points
            arr_acc_new = i_data.get("accepted_new", None)
            if arr_acc_new is not None and len(arr_acc_new) > 0:
                avg_val = np.mean(arr_acc_new)
            else:
                avg_val = float("nan")
            avg_new_accepted_list.append(avg_val)

            # 1c) Ratio: (# newly discarded) / (# newly accepted), focusing on newly created points
            #    We sum up newly discarded from 'discarded_likelihood_new' and 'discarded_iteration' if relevant.
            #    We can also decide if 'discarded_likelihood_old' should be included. By default, let's do it:
            new_discard_count = 0
            if i_data.get("discarded_likelihood_new") is not None:
                new_discard_count += len(i_data["discarded_likelihood_new"])
            if i_data.get("discarded_iteration") is not None:
                new_discard_count += len(i_data["discarded_iteration"])
            # Optionally add 'discarded_likelihood_old' if you consider old points also part of iteration i discarding
            if i_data.get("discarded_likelihood_old") is not None:
                new_discard_count += len(i_data["discarded_likelihood_old"])

            new_accept_count = 0
            if arr_acc_new is not None:
                new_accept_count = len(arr_acc_new)

            if new_accept_count == 0:
                ratio_discarded_list.append(np.nan)
            else:
                ratio_discarded_list.append(new_discard_count / new_accept_count)

            # 1d) Fraction above threshold among newly accepted points
            if chi2_threshold is not None:
                # We check delta_chi_sq_accepted_new
                chi2_new_acc = i_data.get("delta_chi_sq_accepted_new", [])
                if len(chi2_new_acc) == 0:
                    fraction_above_thresh_list.append(np.nan)
                else:
                    frac_above = np.sum(np.array(chi2_new_acc) > chi2_threshold) / len(
                        chi2_new_acc
                    )
                    fraction_above_thresh_list.append(frac_above)
            else:
                fraction_above_thresh_list.append(np.nan)

        # If no valid iterations, bail out
        if len(iterations) == 0:
            if self.verbose:
                print(
                    "[_plot_likelihood_evolution] No valid iterations found, skipping figure."
                )
            return

        # ------------------------------------------------------
        # 2) Create figure with subplots
        # ------------------------------------------------------
        sns.set_theme(context="paper", style="whitegrid")
        fig = plt.figure(figsize=(8, 8))
        gs = fig.add_gridspec(2, 1, height_ratios=[2.0, 1.5], hspace=0.1)

        ax_top = fig.add_subplot(gs[0, 0])
        ax_bottom = fig.add_subplot(gs[1, 0], sharex=ax_top)

        # A) Top Subplot: best-fit & average new-accepted
        ax_top.plot(
            iterations,
            best_fit_list,
            marker="o",
            color="tab:red",
            label="Best-Fit $-\\log(\\mathcal{L})$",
        )
        ax_top.plot(
            iterations,
            avg_new_accepted_list,
            marker="D",
            color="tab:blue",
            label="Avg (newly accepted) $-\\log(\\mathcal{L})$",
        )

        ax_top.set_ylabel("$-\\log(\\mathcal{L})$")
        ax_top.legend(loc="best", fontsize=8)
        ax_top.grid(True, alpha=0.3)
        # Optionally use a log scale on the y-axis if your -log(L) range is large:
        ax_top.set_yscale("log")
        # Hide x-ticks so they appear only at bottom subplot
        plt.setp(ax_top.get_xticklabels(), visible=False)
        # Set y-range for the top subplot
        ax_top.set_ylim(0.1, 1.1 * max(best_fit_list + avg_new_accepted_list))

        # B) Bottom Subplot: ratio of discarded vs. accepted + fraction above threshold
        line1 = ax_bottom.plot(
            iterations,
            ratio_discarded_list,
            marker="s",
            color="darkgreen",
            label="Discard/Accept Ratio (new points)",
        )

        if chi2_threshold is not None:
            # We add a second y-axis for fraction above threshold
            ax_bottom2 = ax_bottom.twinx()
            line2 = ax_bottom2.plot(
                iterations,
                fraction_above_thresh_list,
                marker="^",
                color="crimson",
                label="Fraction > $\\Delta\\chi^2_{\\mathrm{thresh}}$ (new accepted)",
            )
            ax_bottom2.set_ylabel(
                "Frac. of new accepted above threshold", color="crimson"
            )
            ax_bottom2.tick_params(axis="y", labelcolor="crimson")
            ax_bottom2.set_ylim(-0.02, 1.02)

            # Combine legends
            lines1, labels1 = ax_bottom.get_legend_handles_labels()
            lines2, labels2 = ax_bottom2.get_legend_handles_labels()
            ax_bottom2.legend(
                lines1 + lines2, labels1 + labels2, loc="best", fontsize=8
            )
        else:
            ax_bottom.legend(loc="best", fontsize=8)

        ax_bottom.set_xlabel("Iteration")
        ax_bottom.set_ylabel("Discard/Accept Ratio")
        ax_bottom.grid(True, alpha=0.3)
        # Choose whether to use a log scale on the y-axis or not:
        # Avoid division by zero by filtering out zeros
        filtered_ratios = [r for r in ratio_discarded_list if r > 0]

        if filtered_ratios:
            min_ratio = np.min(filtered_ratios)
            max_ratio = np.max(filtered_ratios)

            # Define a threshold for switching to log scale
            if max_ratio / min_ratio > 100:  # More than two orders of magnitude
                ax_bottom.set_yscale("log")

        fig.suptitle(
            "Likelihood Filter Evolution (New Points Each Iteration)",
            fontsize=14,
            y=0.95,
        )

        # ------------------------------------------------------
        # 3) Save figure
        # ------------------------------------------------------
        for fmt in self.save_formats:
            fname = f"likelihood_evolution_over_iterations.{fmt}"
            savepath = os.path.join(self.analysis_dir, fname)
            plt.savefig(savepath, dpi=1000, bbox_inches="tight")
            if self.verbose >= 2:
                print(f"[_plot_likelihood_evolution] Saved figure to {savepath}")

        plt.close(fig)

        if self.verbose >= 2:
            print(
                "[_plot_likelihood_evolution] Created figure with likelihood evolution over iterations."
            )

    def sci_notation_latex(self, value, precision=1):
        from math import floor, log10

        if value == 0:
            return f"{0:.{precision}f}"

        exponent = int(floor(log10(abs(value))))
        mantissa = value / 10**exponent
        if round(mantissa, precision) == 1:
            return rf"10^{{{exponent}}}"
        return rf"{mantissa:.{precision}f} \times 10^{{{exponent}}}"


# --------------------------------CLASS PlotIterations--------------------------------
# This class is used to create a detailed two-panel figure with multiple columns, used to
# illustrate CONNECTs iterative process of creating the training data, including the
# acceptance/rejection of points in the training data. It creates a a two-panel 2D scatter plot with multiple columns.
# The upper panel shows the data specific to the iteration(s) in this column (newly accepted, discarded, etc.).
# The lower panel shows the accumulated state of accepted and discarded points up to that iteration (or iteration group).


class PlotIterations:
    """
    Create detailed two-panel iteration plots to illustrate CONNECT's iterative training process.

    This class generates a figure in which each column represents one or more iterations of the CONNECT
    sampling process. For each column, the figure is divided into two panels:

      - **Upper Panel:** Displays data specific to the iteration(s) in that group (for example, newly
        accepted points, discarded points by various filters, and best-fit points).
      - **Lower Panel:** Shows the accumulated state of the training data up to that iteration (i.e. the
        combined accepted points, and the cumulative discarded points).

    The class supports both scenarios when a likelihood filter is applied (which produces additional data
    categories such as "accepted_new", "accepted_old", "discarded_likelihood_new", etc.) and when it is not
    applied. In addition, it offers several customization options for marker styles, colors, grouping, and
    axis ranges. Optional contour overlays can be drawn based on the delta chi² values computed using the
    best-fit log-likelihood from each iteration and a user-defined threshold from the param_connect object.

    Parameters
    ----------
    data : dict
        A dictionary containing the iteration data. Each key (typically an iteration number) maps to a
        sub-dictionary with keys for various data categories (e.g. 'accepted_new', 'discarded_iteration',
        'failed_class', etc.). Each data category is expected to have a "parameters" DataFrame (and, when
        applicable, a "likelihood_data" DataFrame).
    param_x : str
        Name of the parameter to use for the x-axis.
    param_y : str
        Name of the parameter to use for the y-axis.
    param_connect : object
        An instance that provides parameter configuration and threshold information (for example, the
        delta chi² threshold).
    output_folder : str
        Path to the directory where the generated plots will be saved.
    legend_labels : dict, optional
        A mapping from data category keys (e.g. 'accepted_new', 'discarded_likelihood_old') to custom legend
        labels. If not provided, a set of default labels is used.
    marker_styles : dict, optional
        A dictionary specifying marker properties (such as marker shape, fill style, color, edgecolor,
        linewidth, size, and transparency) for each data category. If a color is set to "iter_color", the
        color is determined based on the iteration number.
    max_subplots : int, optional
        Maximum number of subplot columns allowed in the final figure. If the number of iterations exceeds
        this limit, iterations are automatically grouped.
    save_formats : list of str, optional
        List of file formats (e.g. ['png', 'pdf']) in which to save the figure.
    combine_iteration_0_and_1 : bool, optional
        If True, attempts to combine iterations 0 and 1 in the same subplot column.
    ignore_iteration_0_for_axis : bool, optional
        If True, excludes iteration 0 data when determining the global x and y axis ranges.
    colormap : str, optional
        Name of the matplotlib colormap to use for assigning colors to iterations (e.g. 'tab10').
    marker_size : int, optional
        Base marker size (area-based) for plotting points.
    alpha_accepted : float, optional
        Transparency for accepted points.
    alpha_discarded : float, optional
        Transparency for discarded points.
    marker_edge_width : float, optional
        Width of marker edges.
    x_range, y_range : tuple or None, optional
        Explicit x- and y-axis ranges (min, max) to use for all subplots. If not provided, the ranges are
        computed from the data.
    show_super_labels : bool, optional
        If True, adds a shared x-axis and y-axis label (supxlabel and supylabel) for the entire figure.
    verbose : int, optional
        Verbosity level (e.g., 0 = quiet, higher values yield more detailed logging).
    draw_contours : bool, optional
        If True, computes and draws filled contours (and optionally contour lines) based on delta chi² values.
    contour_levels : int, optional
        Number of contour levels to use (when applicable).
    grid_size : int, optional
        Resolution for the meshgrid used in contour interpolation (number of grid cells along each axis).
    sigma : int, optional
        Standard deviation for the Gaussian filter used to smooth the interpolated contour data.
    gamma_powernorm : float, optional
        Gamma value for PowerNorm normalization of the contour colormap.
    discretize_cmap : bool, optional
        If True, discretizes the contour colormap into a fixed number of colors.
    n_discretize_cmap : int, optional
        Number of discrete color bins to use if discretizing the colormap.
    cmap_norm_type : str, optional
        Normalization method for the contour colormap. Options are 'PercentileNorm' or 'PowerNorm'.
    cmap_contour : str, optional
        Name of the colormap for contour plotting (e.g. 'viridis').
    alpha_contourf : float, optional
        Opacity for filled contours.
    alpha_contour : float, optional
        Opacity for contour lines.
    contour_line_overlay : bool, optional
        If True, overlays contour lines on top of the filled contours.
    N : int, optional
        Number of grid cells along each axis used for selecting contour lines (affects line density).
    show_grid : bool, optional
        If True, overlays a grid on the contour plots for debugging purposes.
    contour_linewidths : float, optional
        Line width for the contour overlay lines.
    auto_selected_contourline_density : bool, optional
        If True, automatically selects a subset of contour levels based on their spatial density.
    iterations_to_plot : 'all' or list of int, optional
        Which iterations (or iteration groups) to include in the plot. If 'all', all valid iterations are plotted.
    show_super_legend : bool, optional
        If True, adds a super legend to the figure summarizing the markers and colors for each data category.
    interpolation_method : str, optional
        Interpolation method for griddata when computing contours (e.g. 'cubic', 'linear', or 'nearest').
    iteration0_figure_insert : bool, optional
        If True, inserts an inset (small sub-plot) showing iteration 0 data within the first column.
    include_lkldiscard_new_for_axis_range : bool, optional
        If True, includes data from the 'discarded_likelihood_new' category when determining axis ranges.
    include_discard_iter0_remnants_for_axis_range : bool, optional
        If True, includes remnants from iteration 0 (e.g. in 'discarded_likelihood_old') when computing axis ranges.
    axis_labels : dict, optional
        Custom axis labels mapping parameter names to display labels (which can include LaTeX formatting).
    data_types_to_plot : dict, optional
        Dictionary specifying which data types to plot in the upper and lower panels. For example, keys
        'upper_panel' and 'lower_panel' map to sub-dictionaries with boolean flags for each category.
    save_x_y_loglkl : bool, optional
        If True, saves the global x, y, and log-likelihood arrays (used for contour computation) as a pickle file.
    pickle_contour_data : any, optional
        Preloaded pickle data for contour computation. If provided, this data is used instead of re-extracting
        it from the raw data.
    delta_chi2_threshold_color : str, optional
        Color to use for the delta chi² threshold contour line (e.g. 'purple').
    delta_chi2_threshold_linewidth : float, optional
        Line width for the delta chi² threshold contour line.
    plot_threshold_line : bool, optional
        If True, plots a contour line (or vertical/horizontal line) indicating the delta chi² threshold,
        as defined by param_connect.

    Returns
    -------
    None
        This class does not return any value. It saves the generated figure(s) to the specified output folder.

    Notes
    -----
    - The upper panel of each subplot displays iteration-specific data (using markers for categories such as
      'accepted_new', 'discarded_iteration', etc.), while the lower panel shows the accumulated data (e.g.,
      'accepted_accumulated', 'accumulated_discarded_old/new').
    - Global axis ranges are computed across all iterations (except iteration 0 if specified) to ensure
      consistency between subplots.
    - When contours are drawn, the delta chi² values are computed as:
          Δχ² = 2.0 * (loglkl_all - best_fit_loglkl)
      where best_fit_loglkl is obtained from the 'best_fit' data of the iteration group with the highest index.
    - If plot_threshold_line is enabled and the param_connect object provides a delta chi² threshold, a contour
      (or line) is added at that level using the specified color and line width. This threshold is also included
      in the super legend.

    Example
    -------
    >>> plotter = PlotIterations(
            data=iteration_data_dict,
            param_x='H0',
            param_y='omega_b',
            param_connect=param_connect_instance,
            output_folder='/path/to/output',
            iterations_to_plot='all',
            draw_contours=True,
            plot_threshold_line=True,
            delta_chi2_threshold_color='purple',
            delta_chi2_threshold_linewidth=2.0,
            axis_labels={'H0': r'$H_0$', 'omega_b': r'$\\Omega_b$'},
            verbose=2
        )
    >>> plotter.plot()

    This will create and save a multi-panel figure showing the evolution of the training data across
    iterations, with separate subplots for each iteration group, including both scatter and contour data.
    """

    def __init__(
        self,
        data,
        param_x,
        param_y,
        param_connect,
        output_folder,
        legend_labels=None,
        marker_styles=None,
        max_subplots=10,
        save_formats=["png", "pdf"],
        combine_iteration_0_and_1=False,
        ignore_iteration_0_for_axis=False,
        colormap="tab10",  # Color map for the iteration colors, #tab10, tab20, tab20b, tab20c, any other matplotlib (discrete?) colormap
        marker_size=8,
        alpha_accepted=1,
        alpha_discarded=1,
        marker_edge_width=0.1,
        x_range=None,  # Override custom shared x-axis range for all subplots
        y_range=None,  # Override custom shared y-axis range for all subplots
        show_super_labels=True,
        verbose=1,
        draw_contours=True,  # Whether or not to draw filled contours on the scatter plot
        contour_levels=21,  # Redundant parameter no longer applicable
        grid_size=1000,  # for the meshgrid for the contour plot - the number of grid cells in each axis
        sigma=20,  # sigma parameter for the gaussian filter used in the contour plot
        gamma_powernorm=0.22,  # parameter used for the PowerNorm colormap normalization
        # High gamma values will make the colormap more linear, low gamma values will emphasize small differences
        discretize_cmap=False,
        n_discretize_cmap=20,
        cmap_norm_type="PercentileNorm",  #'PowerNorm', 'PercentileNorm'
        cmap_contour="viridis",  #'viridis', 'cividis', 'inferno', 'plasma', 'magma', any other matplotlib colormap
        alpha_contourf=1,
        alpha_contour=1,
        contour_line_overlay=False,
        N=250,  # N is the number of grid cells in each axis. Only one contour line crosses each cell.
        show_grid=False,
        contour_linewidths=0.5,
        auto_selected_contourline_density=False,
        iterations_to_plot="all",
        show_super_legend=True,
        use_bold_subplot_legend="auto",
        interpolation_method="cubic",
        iteration0_figure_insert=False,
        include_lkldiscard_new_for_axis_range=True,
        include_discard_iter0_remnants_for_axis_range=False,
        exclude_iter0_discard_clutter_lower_panels=True,
        axis_labels=None,
        data_types_to_plot=None,
        save_x_y_loglkl=False,
        pickle_contour_data=None,
        delta_chi2_threshold_color="purple",
        delta_chi2_threshold_linewidth=2.0,
        plot_threshold_line=False,
        log_x=False,
        log_y=False,
        show_counts_in_legend=True,
        subplot_legend_location="upper left",
        tall_subplots=True,
        fig_width="440 pts",
    ):
        """
        Initialize the PlotIterations class with user configuration and data.

        Parameters
        ----------
        data : dict
            Dictionary containing iteration_data, each iteration_data contains various keys for accepted/ discarded sets.
        param_x : str
            Parameter name for the x-axis.
        param_y : str
            Parameter name for the y-axis.
        output_folder : str
            Folder to save the generated figure.
        legend_labels : dict, optional
            Dictionary mapping data keys (like 'accepted_new', 'discarded_likelihood_old') to legend labels.
            If None, default labels are used.
        marker_styles : dict, optional
            Dictionary specifying marker, color, fillstyle, size, alpha for each data type.
            For color, 'iter_color' means use the iteration-based color.
            If None, default marker styles are used.
        max_subplots : int, optional
            Maximum number of columns (subplots) allowed.
        save_formats : list, optional
            List of file formats to save the figure in.
        combine_iteration_0_and_1 : bool, optional
            If True, tries to combine iteration 0 and 1 in the same column if possible.
        ignore_iteration_0_for_axis : bool, optional
            If True, when determining axis ranges from data, ignore iteration 0 points.
        colormap : str, optional
            Name of the colormap to use for iteration colors.
        marker_size : int, optional
            Base marker size for plotting.
        alpha_accepted : float, optional
            Alpha for accepted points.
        alpha_discarded : float, optional
            Alpha for discarded points.
        x_range : tuple or None, optional
            If provided, sets the x-axis range explicitly.
        y_range : tuple or None, optional
            If provided, sets the y-axis range explicitly.
        show_super_labels : bool, optional
            If True, show super x and y labels for the entire figure.
        verbose : int, optional
            Verbosity level.
        """
        self.use_layout_A = tall_subplots
        self.subplot_legend_location = subplot_legend_location
        self.use_bold_subplot_legend = use_bold_subplot_legend
        if use_bold_subplot_legend == "auto":
            self.use_bold_subplot_legend = True
        if self.use_layout_A and use_bold_subplot_legend == "auto":
            self.use_bold_subplot_legend = (
                False
                # Disable bold subplot legend for layout A
            )
        self.fig_width = fig_width
        self.data = data
        self.param_x = param_x
        self.param_y = param_y
        self.output_folder = output_folder
        self.max_subplots = max_subplots
        self.save_formats = save_formats
        self.combine_iteration_0_and_1 = combine_iteration_0_and_1
        self.ignore_iteration_0_for_axis = ignore_iteration_0_for_axis
        self.colormap = plt.get_cmap(colormap)
        self.marker_size = marker_size
        self.alpha_accepted = alpha_accepted
        self.alpha_discarded = alpha_discarded
        self.x_range = x_range
        self.y_range = y_range
        self.show_super_labels = show_super_labels
        self.verbose = verbose
        self.draw_contours = draw_contours
        self.contour_levels = contour_levels
        self.grid_size = grid_size
        self.sigma = sigma
        self.gamma_powernorm = gamma_powernorm
        self.discretize_cmap = discretize_cmap
        self.n_discretize_cmap = n_discretize_cmap
        self.cmap_contour = plt.get_cmap(cmap_contour)
        self.alpha_contourf = alpha_contourf
        self.alpha_contour = alpha_contour
        self.contour_line_overlay = contour_line_overlay
        self.N = N
        self.show_grid = show_grid
        self.contour_linewidths = contour_linewidths
        self.auto_selected_contourline_density = auto_selected_contourline_density
        self.iterations_to_plot = iterations_to_plot
        self.show_super_legend = show_super_legend
        self.interpolation_method = interpolation_method
        self.iteration0_figure_insert = iteration0_figure_insert
        self.include_lkldiscard_new_for_axis_range = (
            include_lkldiscard_new_for_axis_range
        )
        self.marker_edge_width = marker_edge_width
        self.axis_labels = axis_labels
        self.xlabel = None
        self.ylabel = None
        self.data_types_to_plot = data_types_to_plot
        self.include_discard_iter0_remnants_for_axis_range = (
            include_discard_iter0_remnants_for_axis_range
        )
        self.exclude_iter0_discard_clutter_lower_panels = (
            exclude_iter0_discard_clutter_lower_panels
        )
        self.save_x_y_loglkl = save_x_y_loglkl
        self.pickle_contour_data = pickle_contour_data
        self.cmap_norm_type = cmap_norm_type
        self.param_connect = param_connect
        self.delta_chi2_threshold_color = delta_chi2_threshold_color
        self.delta_chi2_threshold_linewidth = delta_chi2_threshold_linewidth
        self.plot_threshold_line = plot_threshold_line
        self.delta_chi2_threshold = getattr(
            self.param_connect, "delta_chi2_threshold", None
        )

        self.global_norm = None
        self.powernorm_offset = 0.0

        if self.axis_labels is not None:
            self.xlabel = self.axis_labels.get(param_x, param_x)
            self.ylabel = self.axis_labels.get(param_y, param_y)

        self.log_x = log_x
        self.log_y = log_y

        if self.draw_contours:
            self.xs_all, self.ys_all, self.loglkl_all = (
                self._gather_all_likelihood_points()
            )
        else:
            self.xs_all = None
            self.ys_all = None
            self.loglkl_all = None

        self.used_keys = set()
        self._legend_counts = {}
        self.show_counts_in_legend = show_counts_in_legend

        # Set legend_labels and marker_styles with defaults if None
        self.legend_labels = self._set_default_legend_labels(legend_labels)
        self.marker_styles = self._set_default_marker_styles(marker_styles)

        # Extract iteration keys
        # --------------------------------------------------------------------
        # (A) FIRST: find and store all valid iterations up to last-complete
        # --------------------------------------------------------------------
        all_keys_raw = sorted(self.data.get("iteration_data", {}).keys())
        if not all_keys_raw:
            raise ValueError("No iteration_data found in the data dictionary.")

        last_complete_it = (
            self._find_last_complete_iteration()
        )  # We'll define this method below
        self.last_complete_it = last_complete_it
        if last_complete_it is None:
            if self.verbose >= 1:
                print("No complete iterations found. Exiting.")
            all_keys_raw = []
        else:
            # Keep only up to last_complete_it
            all_keys_raw = [it for it in all_keys_raw if it <= last_complete_it]

        # Store them as "all_iteration_keys" for data processing, color assignment, etc.
        self.all_iteration_keys = all_keys_raw

        if not self.all_iteration_keys:
            raise ValueError("No valid (complete) iterations exist in the data.")

        # --------------------------------------------------------------------
        # (B) NEXT: figure out which subset the user wants to actually PLOT
        # --------------------------------------------------------------------
        if self.iterations_to_plot == "all":
            # We simply copy all of them
            self.iteration_keys = list(self.all_iteration_keys)
        else:
            # The user gave a subset e.g. [3,4,5]. Intersect with all_iteration_keys
            subset = set(self.iterations_to_plot).intersection(self.all_iteration_keys)
            self.iteration_keys = sorted(subset)

        # If there's nothing left to plot, bail out
        if not self.iteration_keys:
            if self.verbose >= 1:
                print(
                    "No iterations to plot after applying 'iterations_to_plot'. Exiting."
                )
            return

        # --------------------------------------------------------------------
        # (C) Assign stable colors for ALL iteration keys
        # --------------------------------------------------------------------
        self.iter_colors = {}
        for i, it in enumerate(self.all_iteration_keys):
            color = self.colormap(i % self.colormap.N)
            self.iter_colors[it] = color

        # --------------------------------------------------------------------
        # (D) Prepare data for ALL iteration keys
        # --------------------------------------------------------------------
        # So that "accumulated" logic can see e.g. iteration=2 even if not in 'iteration_keys'
        self.all_processed_data = self._prepare_dataframes_for(self.all_iteration_keys)

        # For convenience, you can either:
        #   (1) Keep "self.processed_data" as a reference to "self.all_processed_data"
        #   (2) Or rename the code so that you ALWAYS use self.all_processed_data.

        self.processed_data = self.all_processed_data  # minimal rename

        # --------------------------------------------------------------------
        # (E) Now create iteration groups for the SELECTED keys only
        # --------------------------------------------------------------------
        self.iteration_groups = []
        self._create_iteration_groups()  # This method will group self.iteration_keys, not the entire set

        # Compute global axis ranges using all data points across all iterations
        all_xs, all_ys = self._gather_all_data_points()

        if all_xs is not None and all_ys is not None:
            self.x_min_global = np.min(all_xs)
            self.x_max_global = np.max(all_xs)
            self.y_min_global = np.min(all_ys)
            self.y_max_global = np.max(all_ys)
            if self.verbose >= 2:
                print(f"Global axis ranges computed from all data:")
                print(f"  X-axis: ({self.x_min_global}, {self.x_max_global})")
                print(f"  Y-axis: ({self.y_min_global}, {self.y_max_global})")
        else:
            # Fallback to default ranges if no data is available
            self.x_min_global, self.x_max_global = 0, 1
            self.y_min_global, self.y_max_global = 0, 1
            if self.verbose >= 2:
                print(
                    "No data found for global axis range computation. Using default ranges (0, 1)."
                )

        if data_types_to_plot is None:
            self.data_types_to_plot = {
                "upper_panel": {
                    "accepted_new": True,
                    "discarded_likelihood_new": True,
                    "discarded_likelihood_old": True,
                    "discarded_iteration": True,
                    "discarded_oversampling": True,
                    "failed_class": True,
                    "best_fit": True,
                },
                "lower_panel": {
                    "accumulated_accepted_still": True,
                    "accumulated_discarded_old": True,
                    "accumulated_discarded_new": True,
                    "best_fit": True,
                },
            }

    def plot(self):
        """
        Main method to create and save the figure.
        """

        # Set matplotlib to default settings
        matplotlib.rcParams.update(matplotlib.rcParamsDefault)
        matplotlib.use("Agg")  # Use non-interactive backend for saving figures

        fontsize = 11
        latex_preamble = r"\usepackage{color} \usepackage{xcolor} \usepackage{siunitx} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{mathtools} \usepackage{bm} \usepackage{mathrsfs} \parindent = 0pt"

        matplotlib.rcParams.update(
            {
                "text.usetex": (
                    False  # if self.xlabel and self.ylabel is not None else False
                ),
                "font.family": "serif",
                "font.serif": "cmr10",
                "font.size": fontsize,
                "mathtext.fontset": "cm",
                "text.latex.preamble": latex_preamble,
            }
        )

        plt.rcParams["figure.dpi"] = 600
        plt.rcParams["savefig.dpi"] = 600
        matplotlib.rcParams["hatch.linewidth"] = 0.5
        plt.rcParams["xtick.labelsize"] = 8
        plt.rcParams["ytick.labelsize"] = 8
        plt.rcParams["legend.fontsize"] = 9
        # plt.rcParams["axes.linewidth"] = (
        #     10  # Sets a default thickness for all axes spines
        # )

        # make all axes spines thicker
        plt.rcParams["axes.linewidth"] = 0.9  # default is usually 0.8

        # make all ticks a bit longer and a bit thicker
        plt.rcParams["xtick.major.size"] = 3.5  # default ~3.5
        plt.rcParams["ytick.major.size"] = 3.5
        plt.rcParams["xtick.major.width"] = 0.8  # default ~0.8
        plt.rcParams["ytick.major.width"] = 0.8  # default ~0.8

        # make your legend boxes thicker
        plt.rcParams["legend.frameon"] = True
        # plt.rcParams['legend.edgecolor']    = 'black'
        plt.rcParams["legend.framealpha"] = 1.0

        # Determine axis ranges
        self._determine_axis_ranges()

        # Compute global delta_chi2 min and max if contours are to be drawn
        if (
            self.draw_contours
            and self.xs_all is not None
            and self.ys_all is not None
            and self.loglkl_all is not None
        ):
            all_dchi2 = []
            for it in self.data["iteration_data"]:
                if it > self.last_complete_it:
                    break
                best_fit_data = self.data["iteration_data"][it].get("best_fit", {})
                likelihood_df = best_fit_data.get("likelihood", None)
                if (likelihood_df is not None) and ("loglkl" in likelihood_df.columns):
                    best_loglkl = likelihood_df["loglkl"].iloc[0]
                    delta_chi2 = 2.0 * (self.loglkl_all - best_loglkl)
                    all_dchi2.append(delta_chi2)

            if all_dchi2:
                global_min = np.min([d.min() for d in all_dchi2])
                global_max = np.max([d.max() for d in all_dchi2])
                if self.verbose >= 2:
                    print(
                        f"Global Delta chi^2 range: min={global_min}, max={global_max}"
                    )
            else:
                global_min, global_max = 0, 1  # Default values
                if self.verbose >= 2:
                    print(
                        "No Delta chi^2 data found. Using default color scale (0 to 1)."
                    )

            all_dchi2_concat = np.concatenate(all_dchi2)

            if self.cmap_norm_type == "PowerNorm":
                # 1) Shift offset if negatives exist:
                self.power_offset = 0.0
                if global_min < 0.0:
                    self.power_offset = -global_min

                # 2) Create shifted array for the global norm
                dchi2_shifted = all_dchi2_concat + self.power_offset

                # 3) Use matplotlib's PowerNorm

                self.global_norm = PowerNorm(
                    gamma_powernorm=self.gamma_powernorm,
                    vmin=0.0,  # after shift
                    vmax=dchi2_shifted.max(),  # after shift
                    clip=True,
                )

                # 4. Determine contour levels using MaxNLocator

                # Suppose 20 color steps
                normed_levels = np.linspace(
                    0, 1, self.contour_levels + 1
                )  # If self.contour_levels = 20,
                data_levels = self.global_norm.inverse(
                    normed_levels
                )  # from 0..1 back to data space

                self.contour_levels_selected = data_levels

            elif self.cmap_norm_type == "PercentileNorm":
                # Default => your existing PercentileNorm

                self.global_norm = PercentileNorm(
                    all_dchi2=all_dchi2_concat,
                    n_discretize_cmap=self.n_discretize_cmap,
                    clip=True,
                )

                # Assuming PercentileNorm sets its own levels internally

                # Create normalized boundaries in [0, 1]
                normed_boundaries = np.linspace(0, 1, self.n_discretize_cmap + 1)
                # 3) Invert these boundaries to *data space* using the custom norm
                self.contour_levels_selected = self.global_norm.inverse(
                    normed_boundaries
                )

        # Create figure and axes

        # Set fixed figure size

        unit = self.fig_width.split()[1]
        value = float(self.fig_width.split()[0])

        if unit == "in":
            width = value  # Already in inches
        elif unit == "cm":
            width = value / 2.54  # Convert cm to inches
        elif unit == "pts":
            width = value / 72.27  # Convert TeX points to inches
        else:
            raise ValueError(f"Unrecognized unit in fig_width: {unit}")

        cols = len(self.iteration_groups)

        width_per_col = width / cols

        rows = 2
        fig, axes = plt.subplots(
            rows,
            cols,
            figsize=(cols * width_per_col, rows * width_per_col),
            sharex=True,
            sharey=True,
            # constrained_layout=True
        )
        if rows * cols > 1:
            axes = axes.reshape(rows, cols)
        else:
            axes = np.array([[axes]])  # Handle single subplot case

        # Define absolute position of the super ylabel and colorbar
        fig_width, fig_height = fig.get_size_inches()

        colorbar_xpos = 0.2  # 0.4inches = 1cm
        colorbar_width = 0.4  # 0.4inches = 1cm

        # Initialize a variable to store one QuadContourSet for the colorbar
        first_contour_set = None

        # Plot each group of iterations in one column
        for col_idx, group in enumerate(self.iteration_groups):
            ax_upper = axes[0, col_idx]
            ax_lower = axes[1, col_idx]

            # Plot the data for this iteration group
            contour_set = self._plot_column(
                ax_upper, ax_lower, group, norm=self.global_norm
            )
            # Store the first contour_set for the colorbar
            if first_contour_set is None and contour_set is not None:
                first_contour_set = contour_set

        # **Apply the axis limits to all subplots based on calculated ranges**
        for c in range(cols):
            for r in range(rows):
                # axes[r, c].set_xlim(self.x_min, self.x_max)
                # axes[r, c].set_ylim(self.y_min, self.y_max)

                x0, x1 = self.x_min, self.x_max
                y0, y1 = self.y_min, self.y_max
                dx = (x1 - x0) * 0.05
                dy = (y1 - y0) * 0.05
                axes[r, c].set_xlim(x0 - dx, x1 + dx)
                axes[r, c].set_ylim(y0 - dy, y1 + dy)

        # Layout A: make room for subplot-legends by enlarging the figure height and adjusting default data limits
        if getattr(self, "use_layout_A", False):
            # 1) force a draw so legend bbox is valid

            # after you’ve drawn your figure and created all legends:
            fig.canvas.draw()
            renderer = fig.canvas.get_renderer()

            # 1) find the maximum legend‐height fraction (relative to its own axes)
            max_frac = 0.0
            for ax in axes.flatten():
                leg = ax.get_legend()
                if leg is None:
                    continue

                # get legend bbox in display coords
                bbox_disp = leg.get_window_extent(renderer)

                # convert into axes coords (0…1)
                bbox_ax = bbox_disp.transformed(ax.transAxes.inverted())
                max_frac = max(max_frac, bbox_ax.height)

            h_ax = axes[0, 0].get_position().height

            # 1) capture the *original* shared y-limits
            orig_y0, orig_y1 = axes[0, 0].get_ylim()
            dy = orig_y1 - orig_y0

            # 2) compute the new top of the axis
            new_top = orig_y1 + max_frac * dy
            if self.y_range is None:
                axes[0, 0].set_ylim(orig_y0, new_top)

            if max_frac > 0:
                # 2) enlarge the figure by that same fraction
                w, h = fig.get_size_inches()
                fig.set_size_inches(w, h + 2 * max_frac * h_ax)

                # # 3) pad every axes’ y‐limits by that fraction of its data‐range
                # for ax in axes.flatten():
                #     ax.set_ylim(
                #         orig_y0, new_top)

        # Allocate space for super legend and shared axis labels and colorbar
        fig.subplots_adjust(
            left=0,  # leave space for y-label
            right=1,  # leave space for colorbar
            top=1,  # Leave space above for super legend
            bottom=0,  # Leave space below for x-label
            wspace=0,  # Reduce horizontal space between subplots
            hspace=0,  # Reduce vertical space between subplots
        )

        # Apply super labels if desired
        if self.show_super_labels:

            if self.xlabel is not None:
                # Update usetext to True
                matplotlib.rcParams["text.usetex"] = True
                # fig.supxlabel(self.xlabel, fontsize=10, y=-0.083)
            # else:
            # fig.supxlabel(self.param_x, fontsize=10, y=-0.083)

            if self.ylabel is not None:
                # ylabel_xpos = -0.77  # 0.4inches = 1cm
                matplotlib.rcParams["text.usetex"] = True
                # fig.supylabel(self.ylabel, fontsize=10, x=ylabel_xpos / fig_width)
            # else:
            # fig.supylabel(self.param_y, fontsize=10, x=ylabel_xpos / fig_width)

            # # after you’ve created fig,axes … and you know your pad in inches:
            # pad_inches_y_label = (
            #     0.425 - 0.05
            # )  # how much physical padding you want, in inches
            # pad_inches_x_label = (
            #     0.4 - 0.05
            # )  # how much physical padding you want, in inches

            # fig_width, fig_height = fig.get_size_inches()

            # # convert that to a fraction of the figure
            # pad_frac_y = pad_inches_y_label / fig_width
            # pad_frac_x = pad_inches_x_label / fig_height

            # # now place your super‐labels at exactly that physical offset
            # fig.supxlabel(
            #     self.xlabel or self.param_x,
            #     fontsize=11,
            #     y=-pad_frac_x,  # this is pad_inches_y_label from the bottom
            #     ha="center",
            # )
            # fig.supylabel(
            #     self.ylabel or self.param_y,
            #     fontsize=11,
            #     x=-pad_frac_y,  # this is pad_inches to the *left* of the left edge
            #     va="center",
            #     rotation="vertical",
            # )

            from matplotlib.transforms import Bbox

            # 1) draw everything so tick‐label bboxes are live
            fig.canvas.draw()
            renderer = fig.canvas.get_renderer()

            # ———— super‐xlabel ————————————————————————————————
            ax_bot = axes[-1, 0]
            xticks = [lbl for lbl in ax_bot.get_xticklabels() if lbl.get_text()]
            bboxes = [lbl.get_window_extent(renderer) for lbl in xticks]

            if bboxes:
                bb = Bbox.union(bboxes)
                pad_px = 8
                y_disp = bb.y0 - pad_px  # 8px below lowest tick‐label
                _, y_fig = fig.transFigure.inverted().transform((0, y_disp))

                fig.text(
                    0.5,  # centered horizontally in figure
                    y_fig,  # exactly 8px below the xtick text
                    self.xlabel or self.param_x,
                    ha="center",  # center text on x=0.5
                    va="top",  # place text *above* the y_fig line
                    fontsize=11,
                    transform=fig.transFigure,
                )

            # ———— super‐ylabel ————————————————————————————————
            ax_left = axes[0, 0]
            yticks = [lbl for lbl in ax_left.get_yticklabels() if lbl.get_text()]
            bboxes = [lbl.get_window_extent(renderer) for lbl in yticks]

            if bboxes:
                bb = Bbox.union(bboxes)
                pad_px = 10
                x_disp = bb.x0 - pad_px  # 8px left of the leftmost y‐tick text
                x_fig, _ = fig.transFigure.inverted().transform((x_disp, 0))

                fig.text(
                    x_fig,  # exactly 8px left of the y‐tick text
                    0.5,  # vertically centered in figure
                    self.ylabel or self.param_y,
                    ha="right",  # place text *to the left* of x_fig
                    va="center",  # center text vertically on y=0.5
                    rotation="vertical",
                    fontsize=11,
                    transform=fig.transFigure,
                )

        # After plotting all subplots, use first_contour_set for the colorbar
        if first_contour_set is not None:

            data_boundaries = self.contour_levels_selected

            # Add an axis for colorbar
            cbar_ax = fig.add_axes(
                [1 + colorbar_xpos / fig_width, 0, colorbar_width / fig_width, 1]
            )  # [left, bottom, width, height]
            cbar = fig.colorbar(
                first_contour_set,
                cax=cbar_ax,
                orientation="vertical",
                boundaries=data_boundaries,  # if self.discretize_cmap else None,
                format=FuncFormatter(self.custom_tick_formatter),
                spacing="uniform",
                # drawedges=False           # Draw edges between color segments
            )
            cbar.set_label(r"$\Delta \chi^2$", fontsize=11)
            # 6) Set the colorbar ticks at the same boundaries (or a subset of them)
            cbar.set_ticks(data_boundaries)
            cbar.ax.yaxis.set_major_formatter(FuncFormatter(self.custom_tick_formatter))
            cbar.ax.tick_params(labelsize=9)

            if self.cmap_norm_type == "PowerNorm" and self.power_offset != 0.0:
                old_ticks = cbar.get_ticks()  # e.g. [0., 5., 10., ...] in SHIFTED space
                new_labels = [(t - self.power_offset) for t in old_ticks]
                print(f"Old ticks: {old_ticks}")
                print(f"New labels: {new_labels}")
                print(f"Power offset: {self.power_offset}")
                # Round or format them as needed:
                new_labels = [f"{val:.1f}" for val in new_labels]
                cbar.set_ticklabels(new_labels)

        # Create super legend
        if self.show_super_legend:
            self._create_super_legend(fig)

        for r in range(rows):
            for c in range(cols):
                # Set margins
                # axes[r, c].margins(x=0.55, y=0.55)  # Set margins for the axes

                if self.log_x:
                    axes[r, c].set_xscale("log")
                if self.log_y:
                    axes[r, c].set_yscale("log")

                if c == 0:
                    axes[r, c].tick_params(
                        labelleft=True, left=True, direction="inout", zorder=1000000
                    )  # Enable y-axis tick labels and ticks for the leftmost column
                else:
                    axes[r, c].tick_params(
                        labelleft=False, left=True, direction="inout", zorder=1000000
                    )  # Disable y-axis tick labels but keep ticks on other columns

                for spine in axes[r, c].spines.values():
                    spine.set_zorder(1000000)  # Set zorder for all spines

        # Add x-axis ticks without labels to the upper row (upper panel)
        for c in range(cols):
            axes[0, c].tick_params(
                axis="x",
                which="both",
                bottom=True,
                top=False,
                labelbottom=False,
                zorder=1000000,
            )

        # Ensure x-axis ticks and labels for the lower row (lower panel)
        for c in range(cols):
            axes[1, c].tick_params(
                axis="x",
                which="both",
                bottom=True,
                top=False,
                labelbottom=True,
                zorder=1000000,
            )

        # Save the figure
        os.makedirs(self.output_folder, exist_ok=True)
        os.makedirs(os.path.join(self.output_folder, "iteration_plots"), exist_ok=True)
        param_x_safe = self._sanitize_filename(self.param_x)
        param_y_safe = self._sanitize_filename(self.param_y)
        for fmt in self.save_formats:
            filename = f"iteration_plot_{param_x_safe}_{param_y_safe}.{fmt}"
            save_path = os.path.join(self.output_folder, "iteration_plots", filename)
            fig.savefig(save_path, dpi=600, format=fmt, bbox_inches="tight")
            if self.verbose >= 1:
                print(f"Saved plot to {save_path}")

    def _set_default_legend_labels(self, legend_labels):
        """
        Set default legend labels if none are provided.

        Parameters
        ----------
        legend_labels : dict or None
            User-provided legend labels.

        Returns
        -------
        dict
            Legend labels to use.
        """
        if legend_labels is not None:
            return legend_labels

        # Define default legend labels
        default_legend_labels = {
            "accepted_new": "Accepted",
            "accepted_old": "Accepted (Old)",
            "accepted_accumulated": "Accepted (Accumulated)",
            "discarded_iteration": "Discarded (Iteration)",
            "discarded_oversampling": "Discarded (Oversampling)",
            "discarded_likelihood": "Discarded (Likelihood)",
            "discarded_likelihood_new": "Discarded (Likelihood new)",
            "discarded_likelihood_old": "Discarded (Likelihood old)",
            "failed_class": "Failed Class",
            "best_fit": "Best Fit",
            "accumulated_accepted_still": "Accepted (Still)",
            "accumulated_discarded_old": "Discarded (Old)",
            "accumulated_discarded_new": "Discarded (New)",
        }
        return default_legend_labels

    def _set_default_marker_styles(self, marker_styles):
        """
        Set default marker styles if none are provided.

        Parameters
        ----------
        marker_styles : dict or None
            User-provided marker styles.

        Returns
        -------
        dict
            Marker styles to use.
        """
        if marker_styles is not None:
            return marker_styles

        default_marker_styles = {
            # Accepted Points
            "accepted_new": {
                "marker": "o",  # Circle
                "fillstyle": "full",  # Solid fill for new
                "color": "iter_color",  # Iteration-based color
                "edgecolor": "black",  # Black edge for visibility
                "linewidth": self.marker_edge_width,  # Thicker edge
                "size": self.marker_size,
                "alpha": self.alpha_accepted,
            },
            "accepted_old": {
                "marker": "o",  # Circle
                "fillstyle": "none",  # Hollow for old
                "color": "iter_color",  # Iteration-based color for edge
                "edgecolor": "iter_color",  # Edge color matches iteration
                "linewidth": self.marker_edge_width,  # Thinner edge
                "size": self.marker_size,
                "alpha": self.alpha_accepted,
            },
            "accepted_accumulated": {
                "marker": "o",  # Circle
                "fillstyle": "full",  # Solid fill for accumulated
                "color": "iter_color",  # Iteration-based color
                "edgecolor": "black",  # Black edge for visibility
                "linewidth": self.marker_edge_width,  # Thicker edge
                "size": self.marker_size,
                "alpha": self.alpha_accepted,
            },
            # Discarded Points
            "discarded_iteration": {
                "marker": "o",  # Square
                "fillstyle": "full",  # Solid fill for new discard
                "color": "grey",  # Grey color for discarded
                "edgecolor": "black",  # Black edge for visibility
                "linewidth": self.marker_edge_width,  # Thicker edge
                "size": self.marker_size,
                "alpha": self.alpha_discarded,
            },
            "discarded_oversampling": {
                "marker": "^",  # Triangle
                "fillstyle": "full",  # Solid fill for new discard
                "color": "dimgray",  # Grey color for discarded
                "edgecolor": "black",
                "linewidth": self.marker_edge_width,  # Thicker edge
                "size": self.marker_size,
                "alpha": self.alpha_discarded,
            },
            "discarded_likelihood": {
                "marker": "s",  # Square
                "fillstyle": "none",  # Solid fill for new discard
                "color": "grey",  # Grey color for discarded
                "edgecolor": "black",  # Black edge for visibility
                "linewidth": self.marker_edge_width,  # Thicker edge
                "size": self.marker_size,
                "alpha": self.alpha_discarded,
            },
            "discarded_likelihood_new": {
                "marker": "o",  # Square
                "fillstyle": "full",  # Solid fill for new discard
                "color": "silver",  # Grey color for discarded
                "edgecolor": "black",  # Black edge for visibility
                "linewidth": self.marker_edge_width,  # Thicker edge
                "size": self.marker_size,
                "alpha": self.alpha_discarded,
            },
            "discarded_likelihood_old": {
                "marker": "s",  # Square
                "fillstyle": "full",  # Hollow for old discard
                "color": "#ECECEC",  # Grey color for discarded
                "edgecolor": "black",  # Grey edge to match fill
                "linewidth": self.marker_edge_width,  # Thinner edge
                "size": self.marker_size,
                "alpha": self.alpha_discarded,
            },
            # Special Cases
            "failed_class": {
                "marker": "x",  # Cross
                "fillstyle": "full",  # Not applicable for 'x'
                "color": "red",  # Red color for failed
                "edgecolor": "red",  # Red edge
                "linewidth": 0.8,  # Thicker edge for visibility
                "size": self.marker_size,
                "alpha": 1.0,  # Fully opaque
            },
            "best_fit": {
                "marker": "*",  # Star
                "fillstyle": "full",  # Solid for emphasis
                "color": "iter_color",  # Iteration-based color
                "edgecolor": "black",  # Black edge for emphasis
                "linewidth": 1,  # Thicker edge #What is the default linewidth? It is:
                "size": self.marker_size + 50,  # Larger size to stand out
                "alpha": 1.0,  # Fully opaque
            },
            "accumulated_accepted_still": {
                "marker": "o",  # Circle
                "fillstyle": "full",  # Solid fill
                "color": "iter_color",  # Iteration-based color
                "edgecolor": "black",  # Black edge for visibility
                "linewidth": self.marker_edge_width,  # Thicker edge
                "size": self.marker_size,
                "alpha": self.alpha_accepted,
            },
            "accumulated_discarded_old": {
                "marker": "s",  # Square
                "fillstyle": "full",  # Hollow for old discard
                "color": "#ECECEC",  # Grey color for discarded
                "edgecolor": "black",  # Grey edge to match fill
                "linewidth": self.marker_edge_width,  # Thinner edge
                "size": self.marker_size,
                "alpha": self.alpha_discarded,
            },
            "accumulated_discarded_new": {
                "marker": "o",  # Square
                "fillstyle": "full",  # Solid fill for new discard
                "color": "silver",  # Grey color for discarded
                "edgecolor": "black",  # Black edge for visibility
                "linewidth": self.marker_edge_width,  # Thicker edge
                "size": self.marker_size,
                "alpha": self.alpha_discarded,
            },
        }

        return default_marker_styles

    def _create_iteration_groups(self):
        """
        Automatically group iterations into columns, ensuring we do not exceed max_subplots.
        - If combine_iteration_0_and_1 is True, try to combine them first if possible.
        - Then group consecutive iterations minimally.
        """
        it_count = len(self.iteration_keys)
        keys = self.iteration_keys.copy()
        grouped = []

        # Step 1: Combine iteration 0 and 1 if required and possible
        if self.combine_iteration_0_and_1 and 0 in keys and 1 in keys:
            grouped.append([0, 1])
            keys.remove(0)
            keys.remove(1)

        # Step 2: Group remaining iterations, pairing them to minimize grouping
        remaining_columns = self.max_subplots - len(grouped)
        if remaining_columns <= 0:
            raise ValueError(
                "max_subplots is too small to accommodate the required iterations."
            )

        # Calculate how many iterations need to be grouped
        iterations_left = len(keys)
        # Calculate the maximum number of iterations per column
        # To minimize grouping, aim for 1 iteration per column first
        if remaining_columns >= iterations_left:
            for it in keys:
                grouped.append([it])
        else:
            # Need to group iterations
            # Calculate how many iterations to group per column
            # Minimum grouping: 2 iterations per group
            # Adjust as necessary
            base_group_size = math.ceil(iterations_left / remaining_columns)
            start = 0
            for _ in range(remaining_columns):
                end = start + base_group_size
                group = keys[start:end]
                if len(group) > 0:
                    grouped.append(group)
                start = end
            # Handle any remaining iterations
            if start < iterations_left:
                for it in keys[start:]:
                    grouped[-1].append(it)

        self.iteration_groups = grouped

    # (You can rename, or keep the old method name but add a param.)
    def _prepare_dataframes_for(self, iteration_list):
        """
        Load all data for the given iteration_list (e.g. 0..N).
        Return a dict: processed_data[it] = { 'accepted_new': df, ... }
        """
        processed = {}

        for it in iteration_list:
            iteration_data = self.data["iteration_data"][it]

            def get_df(block_key):
                blk = iteration_data.get(block_key, {})
                df = blk.get("parameters", None)

                # Remove samples with NaN values or np.infinity
                if df is not None:
                    df = df.replace([np.inf, -np.inf], np.nan)  # Convert infs to NaN
                    df = (
                        df.dropna()
                    )  # Drop rows with NaN (which now includes former infs)

                if df is not None and not df.empty:
                    # Check x/y columns exist
                    if self.param_x not in df.columns or self.param_y not in df.columns:
                        raise ValueError(
                            f"DataFrame for {block_key} in iteration {it} missing param_x = {self.param_x} or param_y = {self.param_y}"
                        )
                return df

            processed[it] = {
                "accepted_accumulated": get_df("accepted_accumulated"),
                "accepted_new": get_df("accepted_new"),
                "accepted_old": get_df("accepted_old"),
                "discarded_iteration": get_df("discarded_iteration"),
                "discarded_oversampling": get_df("discarded_oversampling"),
                "discarded_likelihood": get_df("discarded_likelihood"),
                "discarded_likelihood_new": get_df("discarded_likelihood_new"),
                "discarded_likelihood_old": get_df("discarded_likelihood_old"),
                "failed_class": get_df("failed_class"),
                "best_fit": None,
            }

            # Handle best_fit
            bf_blk = iteration_data.get("best_fit", {})
            bf_params = bf_blk.get("parameters", None)
            if bf_params is not None and not bf_params.empty:
                bf_x = bf_params[self.param_x].iloc[0]
                bf_y = bf_params[self.param_y].iloc[0]
                processed[it]["best_fit"] = (bf_x, bf_y)

        return processed

    def _determine_axis_ranges(self):
        if self.x_range is not None and self.y_range is not None:
            self.x_min, self.x_max = self.x_range
            self.y_min, self.y_max = self.y_range
            return

        xs = []
        ys = []

        relevant_categories = [
            "accepted_new",
            "discarded_iteration",
            "discarded_oversampling",
            "discarded_likelihood_old",
        ]
        if self.include_lkldiscard_new_for_axis_range:
            relevant_categories.append("discarded_likelihood_new")

        for it in self.all_iteration_keys:  # <<-- note the difference here
            if self.ignore_iteration_0_for_axis and it == 0:
                continue

            it_data = self.all_processed_data[it]  # or self.processed_data
            for cat in relevant_categories:

                if not self.include_discard_iter0_remnants_for_axis_range:
                    if it == 1 and cat == "discarded_likelihood_old":
                        continue

                df = it_data.get(cat)
                if isinstance(df, pd.DataFrame) and not df.empty:
                    xs.extend(df[self.param_x].values)
                    ys.extend(df[self.param_y].values)

        if not xs:
            self.x_min, self.x_max = 0, 1
            self.y_min, self.y_max = 0, 1
        else:
            self.x_min, self.x_max = min(xs), max(xs)
            self.y_min, self.y_max = min(ys), max(ys)

    def _plot_column(self, ax_upper, ax_lower, iteration_group, norm):
        """
        Plot the data for a given column (one or more iterations grouped together).
        Then create per-subplot legends for each panel.
        """
        # 1) Plot upper panel, retrieve which (iteration, key) were actually plotted
        plotted_keys_upper = self._plot_upper_panel(
            ax_upper, iteration_group, norm=norm
        )

        # 2) Plot lower panel, retrieve which (iteration, key) were actually plotted
        plotted_keys_lower = self._plot_lower_panel(
            ax_lower, iteration_group, norm=norm
        )

        # 3) Build legends for each panel from the keys that were actually plotted
        self._create_subplot_legend(ax_upper, plotted_keys_upper)
        self._create_subplot_legend(ax_lower, plotted_keys_lower)

        if (0 in iteration_group) and self.iteration0_figure_insert:
            # add insets
            self._plot_inset_in_first_column_upper(ax_upper, iteration_group, norm)
            self._plot_inset_in_first_column_lower(ax_lower, iteration_group, norm)

        # Return the contour_set from the lower panel for colorbar (if not already set)
        return getattr(self, "contour_set", None)

    def _plot_upper_panel(self, ax, iteration_group, norm=None):
        """
        Plot data for each iteration in 'iteration_group' on the upper panel.
        Returns a list of (iteration, data_category) that actually got plotted.
        """
        plotted_keys = []

        # **New Code Start**
        # Determine if the group has multiple iterations
        is_multi = len(iteration_group) > 1
        self.current_group_is_multi = is_multi  # NEW LINE
        # **New Code End**

        # Plot contours if available
        self._plot_contours_if_available(ax, iteration_group, norm)

        zorder = 1
        for it in iteration_group:
            it_data = self.all_processed_data[it]

            # 1) Plot your usual upper-panel categories
            upper_panel_keys = [
                "discarded_iteration",
                "discarded_oversampling",
                "discarded_likelihood_old",
                "discarded_likelihood_new",
                "accepted_new",
                "failed_class",
            ]
            # Create upper_panel_keys based on the data_types_to_plot, but in the exact order as above in upper_panel_keys
            upper_panel_keys = [
                key
                for key in upper_panel_keys
                if self.data_types_to_plot["upper_panel"].get(key, False)
            ]

            for key in upper_panel_keys:
                df = it_data.get(key, None)
                if isinstance(df, pd.DataFrame) and not df.empty:
                    # **New Code Start**
                    # If multiple iterations, ensure edgecolor is 'iter_color' for discarded categories
                    if is_multi and key.startswith("discarded"):
                        # Temporarily set 'edgecolor' to 'iter_color' in the style
                        original_edgecolor = self.marker_styles[key].get(
                            "edgecolor", "black"
                        )
                        self.marker_styles[key]["edgecolor"] = "iter_color"
                    # **New Code End**

                    # Actually plot
                    self._plot_points(ax, df, key, iteration=it, zorder=zorder)
                    plotted_keys.append((it, key))
                    zorder += 1

                    # **New Code Start**
                    # Restore original 'edgecolor' after plotting
                    if is_multi and key.startswith("discarded"):
                        self.marker_styles[key]["edgecolor"] = original_edgecolor
                    # **New Code End**

            # 2) Plot best_fit if it exists and if the key is enabled in data_types_to_plot
            if it_data["best_fit"] is not None and self.data_types_to_plot[
                "upper_panel"
            ].get("best_fit", False):
                x_bf, y_bf = it_data["best_fit"]
                style = self.marker_styles.get(
                    "best_fit",
                    {
                        "marker": "*",
                        "fillstyle": "full",
                        "color": "iter_color",
                        "size": self.marker_size + 2,
                        "alpha": 1.0,
                    },
                )
                self._scatter_single(ax, x_bf, y_bf, style, iteration=it, zorder=zorder)
                plotted_keys.append((it, "best_fit"))

        self.current_group_is_multi = False  # NEW LINE

        return plotted_keys

    def _plot_lower_panel(self, ax, iteration_group, norm):
        """
        Plot accumulated data up to 'max_it' on the lower panel:
        - All accepted points that remain accepted,
        - 'accumulated_discarded_old' for old discards from iterations < max_it,
        - 'accumulated_discarded_new' for newly discarded in iteration == max_it,
        - best_fit for the current iteration (max_it).
        Returns a list of (iteration, data_category) that actually got plotted.
        """
        plotted_keys = []
        max_it = max(iteration_group)

        # Plot contours if available
        self._plot_contours_if_available(ax, iteration_group, norm)

        for key in [
            "accepted_accumulated",
            "discarded_iteration",
            "discarded_likelihood_old",
        ]:
            try:
                df = self.all_processed_data[max_it][key]
                cols = list(df.columns) + ["iteration"]
                break
            except (KeyError, AttributeError):
                continue
        else:
            # If all attempts failed, use the fallback
            cols = [self.param_x, self.param_y, "iteration"]

        # Build "still accepted" from all iteration keys up to max_it
        accepted_accumulated_dfs = []
        for it in self.all_iteration_keys:  # <<-- changed
            if it <= max_it:
                df = self.all_processed_data[it][
                    "accepted_new"
                ]  # or self.processed_data
                if df is not None and not df.empty:
                    accepted_accumulated_dfs.append(df.assign(iteration=it))

        if accepted_accumulated_dfs:
            all_accepted_new = pd.concat(accepted_accumulated_dfs, ignore_index=True)
        else:
            # all_accepted_new = pd.DataFrame(
            #     columns=[self.param_x, self.param_y, "iteration"]
            # )

            # Make an empty dataframe with the ALL parameter columns including "iteration", not just self.param_x and self.param_y
            all_accepted_new = pd.DataFrame(columns=cols)

        # 2) Gather discarded_likelihood_old up to max_it
        old_discarded_all = []
        new_discarded_all = []
        for it in self.all_iteration_keys:
            if it <= max_it:

                df = self.all_processed_data[it]["discarded_likelihood_old"]
                if df is not None and not df.empty:
                    if "iteration" not in df.columns:
                        df = df.assign(iteration=it)
                    mask_new = df["iteration"] == max_it
                    mask_old = df["iteration"] < max_it
                    if mask_new.any():
                        new_discarded_all.append(df[mask_new])
                    if mask_old.any():
                        old_discarded_all.append(df[mask_old])

        # Convert to DataFrames
        if old_discarded_all:
            df_old_discarded = pd.concat(old_discarded_all, ignore_index=True)
        else:
            # df_old_discarded = pd.DataFrame(
            #     columns=[self.param_x, self.param_y, "iteration"]
            # )

            # Make an empty dataframe with the same columns as all_accepted_new
            df_old_discarded = pd.DataFrame(columns=cols)

        if new_discarded_all:
            df_new_discarded = pd.concat(new_discarded_all, ignore_index=True)
        else:
            # df_new_discarded = pd.DataFrame(
            #     columns=[self.param_x, self.param_y, "iteration"]
            # )

            # Make an empty dataframe with the same columns as all_accepted_new
            df_new_discarded = pd.DataFrame(columns=cols)

        # 3) Subtract from accepted set
        combined_discarded = pd.concat(
            [df_old_discarded, df_new_discarded], ignore_index=True
        )
        if not combined_discarded.empty:
            cols_merge = [col for col in combined_discarded if col != "iteration"]
            merged = all_accepted_new.merge(
                # combined_discarded[[self.param_x, self.param_y]],
                combined_discarded[cols_merge],
                # on=[self.param_x, self.param_y],
                on=cols_merge,
                how="left",
                indicator=True,
            )
            still_accepted = merged[merged["_merge"] == "left_only"].drop(
                columns=["_merge"]
            )
        else:
            still_accepted = all_accepted_new

        # Exludes discards originally accepted points from iteration 0 but discarded in iteration 1 or 2 from the plot to avoid clutter
        if self.exclude_iter0_discard_clutter_lower_panels and max_it >= 2:
            df0_accepted = self.all_processed_data[0]["accepted_new"]
            if df0_accepted is not None and not df0_accepted.empty:

                if df_old_discarded is not None and not df_old_discarded.empty:

                    # 1) Split old_discarded into two subsets: iteration ∈ [1,2] vs. everything else
                    mask_12 = df_old_discarded["iteration"].isin([1, 2])
                    df_old_target = df_old_discarded[
                        mask_12
                    ].copy()  # This subset will be filtered
                    df_old_rest = df_old_discarded[
                        ~mask_12
                    ].copy()  # df with iteration not in [1,2]

                    if not df_old_target.empty:
                        # 2) Extract iteration into “likelihood” for re‐alignment in compare_dataframes
                        df_iteration = df_old_target["iteration"].copy().to_frame()

                        # 3) Drop iteration so compare_dataframes doesn't match on it
                        df_old_target = df_old_target.drop(columns=["iteration"])

                        # 4) Drop iteration from df0_accepted if it has one (usually it does not)
                        df0_stripped = df0_accepted.drop(
                            columns=["iteration"], errors="ignore"
                        )

                        # 5) Compare: keep rows from df_old_target that are NOT in df0_accepted
                        df_old_filtered, df_iteration = self.compare_dataframes(
                            df1=df_old_target,
                            df2=df0_stripped,
                            df_likelihood=df_iteration,
                            comparison_type="new",
                            compare_context={
                                "context": "Exclude iteration-0 discards (old) for it=1 or it=2",
                                "df1": "df_old_discarded",
                                "df2": "df0_accepted",
                                "msg1": "No old discard data found",
                                "msg2": "No iteration-0 accepted data found",
                            },
                        )

                        if df_old_filtered is not None and not df_old_filtered.empty:
                            # 6) Put iteration back
                            df_old_filtered["iteration"] = df_iteration["iteration"]
                        else:
                            # If everything got filtered out
                            df_old_filtered = pd.DataFrame(
                                columns=df_old_target.columns.tolist() + ["iteration"]
                            )

                        # 7) Re‐concatenate the subset we filtered with the subset we left alone
                        df_old_discarded = pd.concat(
                            [df_old_filtered, df_old_rest], ignore_index=True
                        )

                    # else: if df_old_target was empty, do nothing – we keep df_old_discarded as is

                # 4) Exclude iteration-0 discards from new_discarded

                if df_new_discarded is not None and not df_new_discarded.empty:
                    mask_12 = df_new_discarded["iteration"].isin([1, 2])
                    df_new_target = df_new_discarded[
                        mask_12
                    ].copy()  # This subset will be filtered
                    df_new_rest = df_new_discarded[
                        ~mask_12
                    ].copy()  # df with iteration not in [1,2]
                    if not df_new_target.empty:
                        # 2) Extract iteration into “likelihood” for re‐alignment in compare_dataframes
                        df_iteration = df_new_target["iteration"].copy().to_frame()
                        # 3) Drop iteration so compare_dataframes doesn't match on it
                        df_new_target = df_new_target.drop(columns=["iteration"])
                        # 4) Drop iteration from df0_accepted if it has one (usually it does not)
                        df0_stripped = df0_accepted.drop(
                            columns=["iteration"], errors="ignore"
                        )
                        # 5) Compare: keep rows from df_new_target that are NOT in df0_accepted
                        df_new_filtered, df_iteration = self.compare_dataframes(
                            df1=df_new_target,
                            df2=df0_stripped,
                            df_likelihood=df_iteration,
                            comparison_type="new",
                            compare_context={
                                "context": "Exclude iteration-0 discards (new) for it=1 or it=2",
                                "df1": "df_new_discarded",
                                "df2": "df0_accepted",
                                "msg1": "No new discard data found",
                                "msg2": "No iteration-0 accepted data found",
                            },
                        )
                        if df_new_filtered is not None and not df_new_filtered.empty:
                            # 6) Put iteration back
                            df_new_filtered["iteration"] = df_iteration["iteration"]
                        else:
                            # If everything got filtered out
                            df_new_filtered = pd.DataFrame(
                                columns=df_new_target.columns.tolist() + ["iteration"]
                            )

                        # 7) Re‐concatenate the subset we filtered with the subset we left alone
                        df_new_discarded = pd.concat(
                            [df_new_filtered, df_new_rest], ignore_index=True
                        )

        # 4) Plot old_discarded => "accumulated_discarded_old"
        #    Use f"i < {max_it}" for the legend label
        # Only plot if accumulated_discarded_old key in data_types_to_plot lower panel is True

        if not df_old_discarded.empty and self.data_types_to_plot["lower_panel"].get(
            "accumulated_discarded_old", False
        ):
            self._plot_points(
                ax,
                df_old_discarded,
                "accumulated_discarded_old",
                iteration=f"i$<${max_it}",
                zorder=1,
            )
            plotted_keys.append((f"i$<${max_it}", "accumulated_discarded_old"))

        zorder = 2
        # 4b) Plot new_discarded => "accumulated_discarded_new"
        # Only plot if accumulated_discarded_new key in data_types_to_plot lower panel is True
        if not df_new_discarded.empty and self.data_types_to_plot["lower_panel"].get(
            "accumulated_discarded_new", False
        ):
            for itv, grp in df_new_discarded.groupby("iteration"):
                self._plot_points(
                    ax, grp, "accumulated_discarded_new", iteration=itv, zorder=zorder
                )
                plotted_keys.append((itv, "accumulated_discarded_new"))
                zorder += 1

        # 5) Plot still accepted
        # Only plot accumulated_accepted_still if key in data_types_to_plot lower panel is True
        if not still_accepted.empty and self.data_types_to_plot["lower_panel"].get(
            "accumulated_accepted_still", False
        ):
            for itv, grp in still_accepted.groupby("iteration"):
                self._plot_points(
                    ax, grp, "accumulated_accepted_still", iteration=itv, zorder=zorder
                )
                plotted_keys.append((itv, "accumulated_accepted_still"))
                zorder += 1

        # 6) Plot best_fit
        current_it_data = self.processed_data[max_it]
        if current_it_data["best_fit"] is not None and self.data_types_to_plot[
            "lower_panel"
        ].get("best_fit", False):
            x_bf, y_bf = current_it_data["best_fit"]
            style = self.marker_styles.get("best_fit", {...})
            self._scatter_single(ax, x_bf, y_bf, style, iteration=max_it, zorder=zorder)
            plotted_keys.append((max_it, "best_fit"))
            zorder += 1

        return plotted_keys

    def _plot_points(self, ax, df, key, iteration=None, zorder=1):
        """
        Plot a set of points given by a DataFrame `df` for a specific key.
        If df is None or empty, do nothing.

        Parameters
        ----------
        ax : matplotlib.axes.Axes
            The axes to plot on.
        df : pandas.DataFrame
            DataFrame containing the points to plot.
        key : str
            The data category key to determine styling.
        iteration : int or None
            The iteration number for determining color if 'iter_color' is specified.
        """
        if df is None or df.empty:
            return

            # Mark this data type as used
        self.used_keys.add(key)

        style = self.marker_styles.get(key, {})
        # Extract style params
        marker = style.get("marker", "o")
        fillstyle = style.get("fillstyle", "full")
        color = style.get("color", "grey")
        size = style.get("size", self.marker_size)
        alpha = style.get("alpha", 0.8)
        edgecolor = style.get("edgecolor", color)
        linewidth = style.get("linewidth", 0.1)

        if color == "iter_color" and iteration is not None:
            color = self.iter_colors.get(
                iteration, "blue"
            )  # default to blue if iteration color not found

        if edgecolor == "iter_color" and iteration is not None:
            edgecolor = self.iter_colors.get(iteration, "blue")

        # Determine facecolor based on fillstyle
        if fillstyle == "full":
            facecolor = color
        elif fillstyle == "none":
            facecolor = "none"
        else:
            facecolor = color  # fallback

        ax.scatter(
            df[self.param_x],
            df[self.param_y],
            marker=marker,
            facecolors=facecolor,
            edgecolors=edgecolor,
            linewidths=linewidth,
            s=size,  # matplotlib's scatter size is area-based
            alpha=alpha,
            zorder=zorder,
        )

        # Only do this if we have a valid iteration
        # (best to skip storing for iteration=None)
        if iteration is not None:
            n_points = len(df)
            # Accumulate or simply overwrite the count
            # If you have repeated calls with the same (iteration, key) for the same Axes,
            # you may need to decide whether to add or overwrite. Here we’ll just sum them up.
            old_val = self._legend_counts.get((iteration, key, ax), 0)
            self._legend_counts[(iteration, key, ax)] = old_val + n_points

    def _scatter_single(self, ax, x, y, style, iteration=None, zorder=1):
        """
        Plot a single point with given style.

        Parameters
        ----------
        ax : matplotlib.axes.Axes
            The axes to plot on.
        x : float
            X-coordinate.
        y : float
            Y-coordinate.
        style : dict
            Style dictionary for the point.
        iteration : int or None
            The iteration number for determining color if 'iter_color' is specified.
        """
        self.used_keys.add("best_fit")

        marker = style.get("marker", "o")
        fillstyle = style.get("fillstyle", "full")
        color = style.get("color", "grey")
        size = style.get("size", self.marker_size)
        alpha = style.get("alpha", 0.8)
        edgecolor = style.get("edgecolor", color)
        linewidth = style.get("linewidth", 0.1)

        if color == "iter_color" and iteration is not None:
            color = self.iter_colors.get(
                iteration, "blue"
            )  # default to blue if iteration color not found

        # Determine facecolor based on fillstyle
        if fillstyle == "full":
            facecolor = color
        elif fillstyle == "none":
            facecolor = "none"
        else:
            facecolor = color  # fallback

        ax.scatter(
            [x],
            [y],
            marker=marker,
            facecolors=facecolor,
            edgecolors=edgecolor,
            linewidths=linewidth,
            s=size,
            alpha=alpha,
            zorder=zorder,
        )

    def _sanitize_filename(self, s):
        """
        Remove or replace characters that are invalid in file names.

        Parameters
        ----------
        s : str
            Original string.

        Returns
        -------
        str
            Sanitized string.
        """
        return re.sub(r'[<>:"/\\|?* ]+', "_", s)

    def _create_subplot_legend(self, ax, plotted_keys):
        """
        Create a per-subplot legend that displays one entry per (iteration + data_category),
        labeled as 'i={iteration}' (or 'i=initial').
        """
        handles = []
        labels = []
        seen = set()

        # Determine if the group has multiple iterations
        unique_iterations = set(it for it, key in plotted_keys)
        is_multi = len(unique_iterations) > 1

        for it, key in plotted_keys:
            # Ensure we don't duplicate the same iteration+key in the legend
            if (it, key) in seen:
                continue
            seen.add((it, key))

            # Build the handle/label
            style = self.marker_styles.get(
                key, {}
            ).copy()  # Create a copy to avoid mutating the original

            # Check if the key starts with 'discarded' and the group is multi-iteration
            if is_multi and key.startswith("discarded"):
                style["edgecolor"] = "iter_color"  # Set edgecolor to 'iter_color'

            count_here = 0
            if hasattr(self, "_legend_counts"):
                count_here = self._legend_counts.get((it, key, ax), 0)

            handle, label_str = self._make_iteration_legend_entry(it, style, count_here)
            handles.append(handle)
            labels.append(label_str)

        if handles:
            legend_obj = ax.legend(
                handles,
                labels,
                loc=self.subplot_legend_location,  # upper left",
                fontsize=6,
                frameon=True,
                markerscale=0.8,
                handlelength=1,
                labelspacing=0.25,
                framealpha=0.65,
                # prop={"weight": "550"},
            )

            import matplotlib.patheffects as path_effects

            for text in legend_obj.get_texts():
                text.set_alpha(1.0)  # ensure fully visible
                if self.use_bold_subplot_legend:
                    text.set_path_effects(
                        [
                            path_effects.Stroke(
                                linewidth=0.00005, foreground="black"
                            ),  # thin border
                            path_effects.Normal(),
                        ]
                    )
            ax.figure.add_artist(legend_obj)
            legend_obj.set_zorder(15000)

    def _make_iteration_legend_entry(self, iteration, style, count=0):
        """
        Create a (handle, label_str) for a single iteration's style.
        The label is simply 'i={iteration}' or 'i=initial'.
        """
        # Determine iteration label
        if isinstance(iteration, str):
            # If iteration is a string, use it directly (e.g., "i < {max_it}")
            label_str = iteration
        elif iteration == 0:
            # Special case for the initial iteration
            label_str = "i=initial"
        else:
            # Default integer-based label
            label_str = f"i={iteration}"

        if count > 0 and self.show_counts_in_legend:
            if f"$<$" in label_str:
                label_str += f" (N={count})"
            else:
                label_str += f"  (N={count})"

        # Extract style info
        marker = style.get("marker", "o")
        fillstyle = style.get("fillstyle", "full")
        linewidth = style.get("linewidth", 0.1)
        alpha = style.get("alpha", 1.0)

        # Resolve color and edgecolor
        base_color = style.get("color", "grey")
        base_edgecolor = style.get("edgecolor", base_color)

        # Check if 'iteration' belongs to a iteration group in self.iteration_groups with multiple iterations
        # If so, use 'iter_color' as base_edgecolor

        # If 'iter_color' is used
        if base_color == "iter_color":
            base_color = self.iter_colors.get(iteration, "blue")
        if base_edgecolor == "iter_color":
            base_edgecolor = self.iter_colors.get(iteration, "blue")

        # Facecolor vs. fillstyle
        if fillstyle == "none":
            facecolor = "none"
        else:
            facecolor = base_color

        handle = Line2D(
            [0],
            [0],
            marker=marker,
            color=base_edgecolor,  # line color
            markerfacecolor=facecolor,  # fill
            markersize=6.5,
            linestyle="",
            alpha=alpha,
            linewidth=linewidth,
        )

        return handle, label_str

    def _create_super_legend(self, fig):
        """
        Create a super legend above the figure that includes only the
        data types that were actually plotted AND that you specifically want to display.
        """
        # Define which keys you want to include in the super legend
        desired_keys = []

        def add_unique_marker_key(
            marker_styles, data_types_to_plot, key1, key2, desired_keys
        ):
            """Ensures only one key is added if two keys share the same marker style and both are True."""
            if marker_styles[key1] == marker_styles[key2]:
                has_key1 = data_types_to_plot["upper_panel"].get(key1, False)
                has_key2 = data_types_to_plot["lower_panel"].get(key2, False)

                if has_key1 and has_key2:
                    desired_keys.append(key1)  # Choose key1 arbitrarily
                elif has_key1:
                    desired_keys.append(key1)
                elif has_key2:
                    desired_keys.append(key2)
            else:
                desired_keys.extend([key1, key2])

        # Usage:
        add_unique_marker_key(
            self.marker_styles,
            self.data_types_to_plot,
            "accepted_new",
            "accumulated_accepted_still",
            desired_keys,
        )
        add_unique_marker_key(
            self.marker_styles,
            self.data_types_to_plot,
            "discarded_likelihood_new",
            "accumulated_discarded_new",
            desired_keys,
        )
        desired_keys.append("best_fit")
        add_unique_marker_key(
            self.marker_styles,
            self.data_types_to_plot,
            "discarded_likelihood_old",
            "accumulated_discarded_old",
            desired_keys,
        )
        desired_keys.append("failed_class")
        desired_keys.append("discarded_oversampling")
        desired_keys.append("discarded_iteration")

        # Optionally, allow user to override the desired_keys via super_legend_keys
        if hasattr(self, "super_legend_keys") and self.super_legend_keys is not None:
            desired_keys = self.super_legend_keys

        # Intersect desired_keys with used_keys to include only those that were actually plotted
        relevant_keys = [k for k in desired_keys if k in self.used_keys]

        handles = []
        labels = []
        handler_map = {}
        dummy_handle = (
            1  # Start dummy handles from 1 to avoid conflict with standard handles
        )

        for key in relevant_keys:
            label = self.legend_labels.get(key, key)
            style = self.marker_styles.get(key, {})
            marker = style.get("marker", "o")
            fillstyle = style.get("fillstyle", "full")
            color = style.get("color", "grey")
            edgecolor = style.get("edgecolor", "black")  # Include edgecolor
            size = style.get("size", self.marker_size)
            alpha = style.get("alpha", 1.0)

            if color == "iter_color":
                # Collect unique colors used for this data type
                colors = list(self.iter_colors.values())
                handles.append(dummy_handle)
                labels.append(label)
                # Instantiate MulticolorHandler with adjusted spacing_factor
                handler_map[dummy_handle] = PlotIterations.MulticolorHandler(
                    colors=colors,
                    marker=marker,
                    markersize=6,
                )
                dummy_handle += 1
            else:
                # Single-color handler with edgecolor
                if color == "iter_color":
                    actual_color = self.iter_colors.get(0, "blue")  # Fallback color
                else:
                    actual_color = color

                if edgecolor == "iter_color":
                    actual_edgecolor = self.iter_colors.get(
                        0, "blue"
                    )  # Fallback edgecolor
                else:
                    actual_edgecolor = edgecolor

                handle = Line2D(
                    [0],
                    [0],
                    marker=marker,
                    color=actual_edgecolor,  # Line2D's `color` sets the edgecolor
                    markerfacecolor="none" if fillstyle == "none" else actual_color,
                    markeredgecolor=actual_edgecolor,  # Ensure edgecolor is included
                    markersize=6,
                    linestyle="",
                    alpha=alpha,
                    linewidth=style.get("linewidth", 0.1),
                )
                handles.append(handle)
                labels.append(label)

        if self.plot_threshold_line:
            # Add a dummy handle for the threshold line
            handles.append(
                Line2D(
                    [0],
                    [0],
                    color=self.delta_chi2_threshold_color,
                    linestyle="-",
                    linewidth=self.delta_chi2_threshold_linewidth,
                )
            )

            def format_number(x):
                if abs(x) >= 1e5 or (abs(x) < 1e-5 and x != 0):
                    return f"{x:.2e}"  # scientific notation with 2 decimal places
                else:
                    return f"{x:.5g}"  # up to 5 significant digits

            if isinstance(self.delta_chi2_threshold, list):
                # Show every threshold used from the list
                formatted = [format_number(val) for val in self.delta_chi2_threshold]
                labels.append(f"Δχ²-threshold = {', '.join(formatted)}")
            else:
                labels.append(
                    f"Δχ²-threshold = {format_number(self.delta_chi2_threshold)}"
                )

        # Create the legend only if there are handles to display
        if handles:
            num_handles = len(handles)

            # 1) decide how many inches *above* the top of the axes
            pad_inches = 0.025

            # 2) figure‐size in inches
            fig_width, fig_height = fig.get_size_inches()

            # 3) convert to a fraction of the figure height
            pad_frac = pad_inches / fig_height

            leg = fig.legend(
                handles,
                labels,
                handler_map=handler_map,
                loc="lower center",
                ncol=(
                    num_handles if num_handles < 4 else (num_handles + 1) // 2
                ),  # Dynamically set columns
                bbox_to_anchor=(
                    0.5,
                    1.0 + pad_frac,
                ),  # Adjusted position based on pad_frac
                bbox_transform=fig.transFigure,
                frameon=True,
                # fancybox
                fancybox=True,
                fontsize=7,
                handletextpad=0.5,  # Adjust padding between symbols and text
                labelspacing=0.75,  # Adjust vertical spacing between rows
                handlelength=4.0,  # Increase handle length to allocate more space for markers
            )

            # Make the frame linewidth thicker
            leg.get_frame().set_linewidth(1)
            # Make the frame color 0.6:
            leg.get_frame().set_edgecolor((0.8, 0.8, 0.8, 0.9))
            # Is

    def _gather_all_likelihood_points(self):
        """
        Gather (x, y, true_loglkl) across all iterations and relevant data categories.

        Returns:
            tuple:
                xs (np.ndarray): Array of x-values.
                ys (np.ndarray): Array of y-values.
                loglkl (np.ndarray): Array of true_loglkl values.
                Or (None, None, None) if no likelihood data is found.
        """
        # Initialize lists to collect data points
        xs = []
        ys = []
        loglkl = []

        if self.pickle_contour_data is not None:
            (xs, ys, loglkl) = self.pickle_contour_data

        # Define the relevant keys that contain likelihood data
        relevant_keys = [
            "accepted_accumulated",
            "accepted_new",
            "accepted_old",
            "discarded_iteration",
            "discarded_likelihood",
            "discarded_likelihood_new",
            "discarded_likelihood_old",
        ]

        # Iterate over all iterations
        for iteration_number, iteration_data in self.data.get(
            "iteration_data", {}
        ).items():
            if self.verbose >= 3:
                print(
                    f"[_gather_all_likelihood_points] Processing Iteration {iteration_number}"
                )

            # Iterate over each relevant data category within the iteration
            for key in relevant_keys:
                # Access the current data category
                category_data = iteration_data.get(key, {})
                # Access the 'likelihood_data' DataFrame
                likelihood_df = category_data.get("likelihood_data", None)
                # Access the 'parameters' DataFrame
                parameters_df = category_data.get("parameters", None)

                # Check if both DataFrames are valid and not empty
                if (
                    isinstance(likelihood_df, pd.DataFrame)
                    and not likelihood_df.empty
                    and isinstance(parameters_df, pd.DataFrame)
                    and not parameters_df.empty
                ):

                    # Verify that required columns exist
                    required_params = [self.param_x, self.param_y]
                    required_likelihood = ["true_loglkl"]

                    missing_params = [
                        param
                        for param in required_params
                        if param not in parameters_df.columns
                    ]
                    missing_likelihood = [
                        lk
                        for lk in required_likelihood
                        if lk not in likelihood_df.columns
                    ]

                    if missing_params or missing_likelihood:
                        if self.verbose >= 3:
                            print(
                                f"[_gather_all_likelihood_points] Iteration {iteration_number}, key '{key}':"
                            )
                            if missing_params:
                                print(f"  Missing parameters columns: {missing_params}")
                            if missing_likelihood:
                                print(
                                    f"  Missing likelihood columns: {missing_likelihood}"
                                )
                            print("  Skipping this category.")
                        continue  # Skip to the next category

                    # Ensure that the number of rows match
                    if len(parameters_df) != len(likelihood_df):
                        if self.verbose >= 3:
                            print(
                                f"[_gather_all_likelihood_points] Iteration {iteration_number}, key '{key}':"
                            )
                            print(
                                f"  Mismatched number of rows between 'parameters' ({len(parameters_df)}) and 'likelihood_data' ({len(likelihood_df)})."
                            )
                            print("  Skipping this category.")
                        continue  # Skip to the next category

                    # Extract x, y, and true_loglkl values
                    current_x = parameters_df[self.param_x].values
                    current_y = parameters_df[self.param_y].values
                    current_loglkl = likelihood_df["true_loglkl"].values

                    # Append to the aggregate lists
                    xs.extend(current_x)
                    ys.extend(current_y)
                    loglkl.extend(current_loglkl)

                    if self.verbose >= 4:
                        print(
                            f"[_gather_all_likelihood_points] Iteration {iteration_number}, key '{key}':"
                        )
                        print(f"  Extracted {len(current_x)} points.")
                else:
                    # likelihood_df or parameters_df is None or empty
                    if self.verbose >= 4:
                        print(
                            f"[_gather_all_likelihood_points] Iteration {iteration_number}, key '{key}':"
                        )
                        if likelihood_df is None or parameters_df is None:
                            print("  One or both DataFrames are None.")
                        else:
                            empty_df = likelihood_df.empty or parameters_df.empty
                            print(f"  DataFrames empty: {'Yes' if empty_df else 'No'}")
                        print("  Skipping this category.")

        # Convert lists to NumPy arrays
        if len(xs) == 0:
            if self.verbose >= 2:
                print(
                    "[_gather_all_likelihood_points] No likelihood data found across all iterations."
                )
            return None, None, None

        xs = np.array(xs, dtype=float)
        ys = np.array(ys, dtype=float)
        loglkl = np.array(loglkl, dtype=float)

        if self.verbose >= 3:
            print(
                f"[_gather_all_likelihood_points] Collected {len(xs)} points from all iterations."
            )

        if self.save_x_y_loglkl:
            # Pickle data to to something similar to this:
            os.makedirs(self.output_folder, exist_ok=True)
            os.makedirs(
                os.path.join(self.output_folder, "iteration_plots"), exist_ok=True
            )
            param_x_safe = self._sanitize_filename(self.param_x)
            param_y_safe = self._sanitize_filename(self.param_y)
            filename = f"{param_x_safe}_{param_y_safe}_loglkl.pickle"
            save_path = os.path.join(self.output_folder, "iteration_plots", filename)
            with open(save_path, "wb") as f:
                pickle.dump((xs, ys, loglkl), f)
            if self.verbose >= 2:
                print(
                    f"Saved x, y, loglkl data to {save_path} as pickle fil for later use."
                )

        return xs, ys, loglkl

    def _gather_all_data_points(self):
        """
        Gather all x and y points from all relevant data categories across all iterations.

        Returns:
            tuple:
                xs (np.ndarray): Array of all x-values.
                ys (np.ndarray): Array of all y-values.
        """
        xs = []
        ys = []
        for it in self.iteration_keys:
            it_data = self.processed_data[it]
            for key, df in it_data.items():
                if isinstance(df, pd.DataFrame) and not df.empty:
                    xs.extend(df[self.param_x].values)
                    ys.extend(df[self.param_y].values)
                elif key == "best_fit" and df is not None:
                    # df is a tuple (x_bf, y_bf)
                    x_bf, y_bf = df
                    xs.append(x_bf)
                    ys.append(y_bf)

        if xs and ys:
            return np.array(xs), np.array(ys)
        else:
            return None, None

    def _compute_and_interpolate_delta_chi2_for_iteration(
        self,
        xs_all,  # 1D np.array of x-values for all points
        ys_all,  # 1D np.array of y-values for all points
        loglkl_all,  # 1D np.array of log-likelihood for all points
        best_loglkl,  # float => best loglkl for this iteration
        grid_size=800,
        method="linear",
        sigma=10,
    ):
        """
        Compute delta_chi2 = 2 * (loglkl_all - best_loglkl) for all points,
        then interpolate it onto a grid, optionally smoothing with gaussian_filter.

        Returns:
            Xi, Yi, Zi  => 2D arrays for the interpolated (delta_chi2) grid
        """
        # 1) Compute delta_chi2
        delta_chi2 = 2.0 * (loglkl_all - best_loglkl)

        if self.cmap_norm_type == "PowerNorm" and self.power_offset != 0.0:
            delta_chi2 = delta_chi2 + self.power_offset

        # 2) Interpolate the data
        x_min, x_max = np.min(xs_all), np.max(xs_all)
        y_min, y_max = np.min(ys_all), np.max(ys_all)
        xi = np.linspace(x_min, x_max, grid_size)
        yi = np.linspace(y_min, y_max, grid_size)

        Xi, Yi = np.meshgrid(xi, yi)

        Zi = griddata((xs_all, ys_all), delta_chi2, (Xi, Yi), method=method)

        # 3) Smooth
        Zi_smoothed = gaussian_filter(Zi, sigma=sigma)

        return Xi, Yi, Zi_smoothed

    def _plot_contours_for_iteration_group(
        self,
        ax,
        xs_all,
        ys_all,
        loglkl_all,
        best_loglkl,
        norm,
        draw_contours=True,
        contour_levels=1000,
        grid_size=800,
        sigma=10,
        gamma_powernorm=0.28,
        discretize_cmap=False,
        n_discretize_cmap=15,
        cmap_contour="seismic",  #'viridis',
        alpha_contourf=0.4,
        alpha_contour=0.4,
        contour_line_overlay=True,
        N=50,
        show_grid=False,
        contour_linewidths=0.2,
        iteration=None,
    ):
        """
        1) Compute delta_chi2 using best_loglkl from the iteration group’s max iteration.
        2) Interpolate (xs_all, ys_all, delta_chi2).
        3) Plot contourf and optionally contour lines.

        Returns:
            contour (QuadContourSet) or None
        """
        if not draw_contours:
            return None

        # 1) Interpolate
        Xi, Yi, Zi = self._compute_and_interpolate_delta_chi2_for_iteration(
            xs_all=xs_all,
            ys_all=ys_all,
            loglkl_all=loglkl_all,
            best_loglkl=best_loglkl,
            grid_size=grid_size,
            method=self.interpolation_method,
            sigma=sigma,
        )
        if self.verbose >= 3:
            print(f"Contour grid Zi statistics: min={Zi.min()}, max={Zi.max()}")

        # 3) Possibly discretize the colormap
        if discretize_cmap:
            base_cmap = plt.get_cmap(cmap_contour)
            colors = base_cmap(np.linspace(0, 1, n_discretize_cmap))
            cmap = cm.colors.ListedColormap(colors)
        else:
            cmap = plt.get_cmap(cmap_contour)

        """    
        # 2) Create normalized boundaries in [0, 1]
        normed_boundaries = np.linspace(0, 1, n_discretize_cmap + 1)
        # 3) Invert these boundaries to *data space* using the custom norm
        data_boundaries = norm.inverse(normed_boundaries)
        """

        # 4) Create the contourf (fill)
        # pownorm = PowerNorm(gamma=gamma)
        contourf = ax.contourf(
            Xi,
            Yi,
            Zi,
            levels=self.contour_levels_selected,
            # contour_levels_auto,
            cmap=cmap,
            alpha=alpha_contourf,
            norm=norm,
            zorder=0,
        )

        if contour_line_overlay:
            # 5) Create a *temporary* blank figure/axes for contour line density selection

            if self.auto_selected_contourline_density:
                temp_fig, temp_ax = plt.subplots()
                temp_contour = temp_ax.contour(
                    Xi,
                    Yi,
                    Zi,
                    levels=self.contour_levels_selected,  # data_boundaries,
                    alpha=0,
                    norm=norm,
                )

                # 6) Use your density-based selection logic
                # This yields selected levels or selected contour "paths".
                selected_contours = self.select_contours_based_on_density(
                    contour=temp_contour,
                    Xi=Xi,
                    Yi=Yi,
                    levels=self.contour_levels_selected,
                    N=N,
                )

                # close the temp figure
                plt.close(temp_fig)

        # 7) If overlay lines
        if contour_line_overlay:
            # We can just do a second contour call with the selected levels
            # or do your advanced code with partial paths.
            # For simplicity, let's do a second call with selected levels
            # (the advanced code is fine if you already have it.)
            ax.contour(
                Xi,
                Yi,
                Zi,
                levels=(
                    selected_contours
                    if self.auto_selected_contourline_density
                    else self.contour_levels_selected
                ),  # data_boundaries,
                cmap=cmap,
                alpha=alpha_contour,
                linewidths=contour_linewidths,
                norm=norm,
                zorder=9999,
            )

        # 8) *** Plot the delta_chi2 threshold line***

        if isinstance(self.delta_chi2_threshold, list):
            if iteration < len(self.delta_chi2_threshold):
                delta_chi2_threshold = self.delta_chi2_threshold[iteration]
            else:
                delta_chi2_threshold = self.delta_chi2_threshold[-1]

        if delta_chi2_threshold is not None and self.plot_threshold_line:
            # Plot a contour line at the threshold value. The level is a list with one element.
            ax.contour(
                Xi,
                Yi,
                Zi,
                levels=[delta_chi2_threshold],
                colors=self.delta_chi2_threshold_color,
                linewidths=self.delta_chi2_threshold_linewidth,
                linestyles="-",
                zorder=10000,
            )

        # 8) Optionally show grid on the main ax for debugging
        if show_grid:
            self.overlay_grid_cells(ax, Xi, Yi, N=N, color="black", linewidth=0.5)

        return contourf

    def select_contours_based_on_density(self, contour, Xi, Yi, levels, N=50):
        # Record the start time

        x_min, x_max = Xi.min(), Xi.max()
        y_min, y_max = Yi.min(), Yi.max()
        x_cells = N
        y_cells = N
        cell_width = (x_max - x_min) / x_cells
        cell_height = (y_max - y_min) / y_cells

        # Initialize the density grid and count per contour level
        density_grid = [[set() for _ in range(x_cells)] for _ in range(y_cells)]
        contour_count = {i: 0 for i in range(len(levels))}

        paths_by_layer = self.process_contour_paths(contour, levels)

        # Debugging grid dimensions and cell sizes
        print(f"Grid dimensions: x_cells={x_cells}, y_cells={y_cells}")
        print(f"Cell width: {cell_width}, Cell height: {cell_height}")

        # First pass: Populate the density grid
        print("\nFirst Pass: Populating the density grid")
        for i, paths_in_layer in enumerate(paths_by_layer):
            print(f"Processing contour level {i}")
            for path in paths_in_layer:
                vertices = path.vertices
                x_vals, y_vals = vertices[:, 0], vertices[:, 1]
                print(f"Contour {i} has {len(vertices)} vertices before filtering.")

                # Filter vertices based on grid boundaries
                valid_mask = (
                    (x_vals >= x_min)
                    & (x_vals <= x_max)
                    & (y_vals >= y_min)
                    & (y_vals <= y_max)
                )
                x_vals = x_vals[valid_mask]
                y_vals = y_vals[valid_mask]
                print(f"Contour {i} has {len(x_vals)} vertices after filtering.")

                # Debugging a sample of vertices after filtering
                if len(x_vals) > 0:
                    print(
                        f"Contour {i} vertices sample after filtering: {list(zip(x_vals[:5], y_vals[:5]))}"
                    )

                if len(x_vals) == 0 or len(y_vals) == 0:
                    print(
                        f"Contour {i} has no valid vertices within bounds and is skipped."
                    )
                    continue

                x_indices = np.floor((x_vals - x_min) / cell_width).astype(int)
                y_indices = np.floor((y_vals - y_min) / cell_height).astype(int)
                x_indices = np.clip(x_indices, 0, x_cells - 1)
                y_indices = np.clip(y_indices, 0, y_cells - 1)

                # New print statement for grid assignment
                for x_val, y_val, x_idx, y_idx in zip(
                    x_vals, y_vals, x_indices, y_indices
                ):
                    print(f"Vertex ({x_val}, {y_val}) maps to cell ({y_idx}, {x_idx})")

                unique_cells = set(zip(x_indices, y_indices))
                for x_idx, y_idx in unique_cells:
                    density_grid[y_idx][x_idx].add(i)
                    print(f"Contour {i} added to cell ({y_idx}, {x_idx}).")

        # Second pass: Count the contour levels in shared cells
        print("\nSecond Pass: Counting contour levels in cells with multiple contours")
        for y_idx in range(y_cells):
            for x_idx in range(x_cells):
                if len(density_grid[y_idx][x_idx]) > 1:
                    print(
                        f"Cell ({y_idx}, {x_idx}) has multiple contours: {density_grid[y_idx][x_idx]}"
                    )
                    for contour_level in density_grid[y_idx][x_idx]:
                        contour_count[contour_level] += 1
                        print(
                            f"Contour {contour_level} count incremented to {contour_count[contour_level]}."
                        )

        # Collect all grid cells with multiple contours
        multi_contour_cells = [
            (y, x, density_grid[y][x])
            for y in range(y_cells)
            for x in range(x_cells)
            if len(density_grid[y][x]) > 1
        ]
        print(f"\nMulti-contour cells collected: {multi_contour_cells}")

        # First Phase: Add levels with zero conflicts
        selected_levels = set()
        print("\nFirst Phase: Adding levels with zero conflicts")
        for contour_level, count in contour_count.items():
            if count == 0:
                selected_levels.add(contour_level)
                print(
                    f"Contour {contour_level} added to selected_levels as it has zero conflicts."
                )

        # Second Phase: Iterate through contour levels based on their count
        sorted_contours = sorted(contour_count.items(), key=lambda x: x[1])
        print("\nSecond Phase: Resolving conflicts in multi-contour cells")
        for contour_level, count in sorted_contours:
            if contour_level in selected_levels:
                continue

            # Check if adding this contour causes any conflicts across all cells
            can_add = True
            for y_idx, x_idx, intersecting_levels in multi_contour_cells:
                if contour_level in intersecting_levels:
                    # Check if any of the intersecting levels in this cell are already in selected_levels
                    if any(level in selected_levels for level in intersecting_levels):
                        print(
                            f"Contour {contour_level} conflicts with selected levels in cell ({y_idx}, {x_idx})."
                        )
                        can_add = False
                        break
            if can_add:
                selected_levels.add(contour_level)
                print(
                    f"Contour {contour_level} added to selected_levels after resolving conflicts."
                )

        # Convert selected_levels to final output
        final_selected_levels = [levels[i] for i in sorted(selected_levels)]
        if levels[0] not in final_selected_levels:
            final_selected_levels.insert(0, levels[0])
        if levels[-1] not in final_selected_levels:
            final_selected_levels.append(levels[-1])

        print(f"\nFinal selected levels: {final_selected_levels}")

        return np.array(final_selected_levels)

    def process_contour_paths(self, contour_set, levels):
        """
        Process contour paths for compatibility with different versions of Matplotlib.

        Parameters:
        - contour_set: The QuadContourSet object returned by ax.contour or ax.contourf.
        - levels: The contour levels used.

        Returns:
        - A list of paths organized by contour levels.
        """
        paths_by_layer = []

        # Check if get_paths() is available (newer versions of Matplotlib)
        """if hasattr(contour_set, 'get_paths'):
            # New version: Iterate over get_paths() and split paths if necessary
            for i, joined_paths_in_layer in enumerate(contour_set.get_paths()):
                separated_paths_in_layer = []
                path_vertices = []
                path_codes = []
                for verts, code in joined_paths_in_layer.iter_segments():
                    if code == Path.MOVETO:
                        if path_vertices:
                            separated_paths_in_layer.append(Path(np.array(path_vertices), np.array(path_codes)))
                        path_vertices = [verts]
                        path_codes = [code]
                    elif code == Path.LINETO:
                        path_vertices.append(verts)
                        path_codes.append(code)
                    elif code == Path.CLOSEPOLY:
                        path_vertices.append(verts)
                        path_codes.append(code)
                if path_vertices:
                    separated_paths_in_layer.append(Path(np.array(path_vertices), np.array(path_codes)))

                paths_by_layer.append(separated_paths_in_layer)

        else:"""
        # Old version: Iterate over contour_set.collections
        for i, collection in enumerate(contour_set.collections):
            paths_in_layer = []
            for path in collection.get_paths():
                paths_in_layer.append(path)
            # Print the paths_in_layer so that I can see what it looks like
            print(f"Paths in layer {i}: {paths_in_layer}")
            # Also print its dimensions
            print(f"Dimensions of paths_in_layer: {np.shape(paths_in_layer)}")
            paths_by_layer.append(paths_in_layer)

        return paths_by_layer

    def overlay_grid_cells(self, ax, Xi, Yi, N=20, color="black", linewidth=0.5):
        """
        Overlay grid cells on the contour plot for visualization.

        Parameters:
        - ax: The axes on which to plot the grid.
        - Xi, Yi: The mesh grid used for plotting.
        - N: The number of segments in each axis of the grid.
        - color: Color of the grid lines.
        - linewidth: Width of the grid lines.
        """
        x_min, x_max = Xi.min(), Xi.max()
        y_min, y_max = Yi.min(), Yi.max()

        # Determine the grid cell size
        x_cell_size = (x_max - x_min) / N
        y_cell_size = (y_max - y_min) / N

        # Plot vertical grid lines
        for i in range(N + 1):
            x_coord = x_min + i * x_cell_size
            ax.plot(
                [x_coord, x_coord],
                [y_min, y_max],
                color=color,
                linewidth=linewidth,
                linestyle="--",
                alpha=0.5,
            )

        # Plot horizontal grid lines
        for j in range(N + 1):
            y_coord = y_min + j * y_cell_size
            ax.plot(
                [x_min, x_max],
                [y_coord, y_coord],
                color=color,
                linewidth=linewidth,
                linestyle="--",
                alpha=0.5,
            )

    def custom_tick_formatter(self, x, pos):
        """
        Formats the tick labels:
        - Uses standard notation for |x| < 1000
        - Uses scientific notation for |x| >= 1000
        """
        if abs(x) >= 10000:
            return f"{x:.1e}"
        else:
            return f"{x:.0f}"

    def _plot_contours_if_available(self, ax, iteration_group, norm):
        """
        Plot contourf and optional contour lines based on the 'best_fit' log-likelihood
        for the maximum iteration in 'iteration_group'.

        This method:
        1. Finds the iteration with the highest index in the group.
        2. Fetches best_fit['likelihood'] to get 'loglkl'.
        3. Calls `_plot_contours_for_iteration_group` to generate the contours.
        4. Stores the first QuadContourSet returned in `self.contour_set`
            for a shared colorbar if not already stored.

        Parameters
        ----------
        ax : matplotlib.axes.Axes
            The axes on which to draw the contour plot.
        iteration_group : list of int
            List of iteration indices that are grouped together.
        norm : matplotlib.colors.Normalize or None
            A normalization object used for mapping data to color space.
            If None, no contours will be plotted (or if global data is missing).

        Returns
        -------
        contour_set : matplotlib.contour.QuadContourSet or None
            The contour set if plotted, else None.
        """
        max_it = max(iteration_group)
        best_fit_data = self.data["iteration_data"][max_it].get("best_fit", {})
        likelihood_df = best_fit_data.get("likelihood", None)

        if likelihood_df is not None and ("loglkl" in likelihood_df.columns):
            # Extract best log-likelihood for the maximum iteration in the group
            best_loglkl = likelihood_df["loglkl"].iloc[0]

            # Ensure we have global data for plotting (xs_all, ys_all, loglkl_all) and a valid norm
            if (
                norm is not None
                and self.xs_all is not None
                and self.ys_all is not None
                and self.loglkl_all is not None
            ):

                # Actually create the contours
                contour_set = self._plot_contours_for_iteration_group(
                    ax=ax,
                    xs_all=self.xs_all,
                    ys_all=self.ys_all,
                    loglkl_all=self.loglkl_all,
                    best_loglkl=best_loglkl,
                    norm=norm,
                    draw_contours=self.draw_contours,
                    contour_levels=self.contour_levels,
                    grid_size=self.grid_size,
                    sigma=self.sigma,
                    gamma_powernorm=self.gamma_powernorm,
                    discretize_cmap=self.discretize_cmap,
                    n_discretize_cmap=self.n_discretize_cmap,
                    cmap_contour=self.cmap_contour,
                    alpha_contourf=self.alpha_contourf,
                    alpha_contour=self.alpha_contour,
                    contour_line_overlay=self.contour_line_overlay,
                    N=self.N,
                    show_grid=self.show_grid,
                    contour_linewidths=self.contour_linewidths,
                    iteration=max_it,
                )

                # If we're using a single shared colorbar, store this contour set if not already set
                if not hasattr(self, "contour_set") or self.contour_set is None:
                    self.contour_set = contour_set

                return contour_set
            else:
                if self.verbose >= 2:
                    print(
                        f"No global likelihood data or valid norm => skipping contour for iteration {max_it}"
                    )
                return None
        else:
            if self.verbose >= 2:
                print(
                    f"No best_fit likelihood => skipping contour for iteration {max_it}"
                )
            return None

    def _plot_inset_in_first_column_upper(self, ax_main, iteration_group, norm=None):
        """
        Creates a small inset within the upper-panel Axes 'ax_main' (first column),
        plotting iteration 0 data with global axis ranges.
        """
        # 1) Create the inset axes.
        inset_ax = inset_axes(
            ax_main,
            width="33%",  # 40% of parent Axes' width
            height="33%",  # 40% of parent Axes' height
            loc="upper right",  # "upper right",  # Position of the inset
            borderpad=0.5,
        )
        inset_ax.set_zorder(0)

        # 2) Determine which iterations to plot in the inset.
        #    Typically, only iteration 0.
        it_show = 0
        if it_show not in self.processed_data:
            if self.verbose:
                print("No iteration 0 found; skipping upper inset.")
            return

        # 3) Gather data for iteration 0
        it_data = self.processed_data[it_show]

        # 4) Plot contours if available
        if self.draw_contours and self.global_norm is not None:
            best_fit_data = self.data["iteration_data"][it_show].get("best_fit", {})
            likelihood_df = best_fit_data.get("likelihood", None)
            if likelihood_df is not None and ("loglkl" in likelihood_df.columns):
                best_loglkl = likelihood_df["loglkl"].iloc[0]
                self._plot_contours_for_iteration_group(
                    ax=inset_ax,
                    xs_all=self.xs_all,
                    ys_all=self.ys_all,
                    loglkl_all=self.loglkl_all,
                    best_loglkl=best_loglkl,
                    norm=self.global_norm,
                    draw_contours=self.draw_contours,
                    contour_levels=self.contour_levels,
                    grid_size=self.grid_size,
                    sigma=self.sigma,
                    gamma_powernorm=self.gamma_powernorm,
                    discretize_cmap=self.discretize_cmap,
                    n_discretize_cmap=self.n_discretize_cmap,
                    cmap_contour=self.cmap_contour,
                    alpha_contourf=self.alpha_contourf,
                    alpha_contour=self.alpha_contour,
                    contour_line_overlay=self.contour_line_overlay,
                    N=self.N,
                    show_grid=self.show_grid,
                    contour_linewidths=self.contour_linewidths,
                )

        # 5) Plot the same data categories as the upper panel
        upper_panel_keys = [
            "discarded_iteration",
            "discarded_oversampling",
            "discarded_likelihood_old",
            "discarded_likelihood_new",
            "failed_class",
            "accepted_new",
        ]
        for key in upper_panel_keys:
            df = it_data.get(key, None)
            if isinstance(df, pd.DataFrame) and not df.empty:
                self._plot_points(inset_ax, df, key, iteration=it_show)

        # 6) Plot best_fit if available
        if it_data["best_fit"] is not None:
            x_bf, y_bf = it_data["best_fit"]
            style = self.marker_styles.get(
                "best_fit",
                {
                    "marker": "*",
                    "fillstyle": "full",
                    "color": "iter_color",
                    "size": self.marker_size + 90,  # Ensure it's visible in the inset
                    "alpha": 1.0,
                },
            )
            self._scatter_single(inset_ax, x_bf, y_bf, style, iteration=it_show)

        # 7) Set global axis ranges
        inset_ax.set_xlim(self.x_min_global, self.x_max_global)
        inset_ax.set_ylim(self.y_min_global, self.y_max_global)

        # 8) Customize inset appearance
        inset_ax.tick_params(
            axis="both",
            labelsize=5,  # keep it small…
            length=2,  # your existing tick length
        )

        # Set more ticks on the x-axis
        inset_ax.xaxis.set_major_locator(plt.MaxNLocator(5))
        inset_ax.yaxis.set_major_locator(plt.MaxNLocator(5))

        import matplotlib.patheffects as pe

        for lbl in inset_ax.get_xticklabels() + inset_ax.get_yticklabels():
            # now give it a fine white outline so it pops off the markers
            lbl.set_path_effects(
                [
                    pe.Stroke(linewidth=0.00005, foreground="black"),  # thin border
                    pe.Normal(),
                ]
            )

    def _plot_inset_in_first_column_lower(self, ax_main, iteration_group, norm=None):
        """
        Creates a small inset within the lower-panel Axes 'ax_main' (first column),
        plotting iteration 0 data with global axis ranges.
        """
        # 1) Create the inset axes.
        inset_ax = inset_axes(
            ax_main,
            width="33%",  # 40% of parent Axes' width
            height="33%",  # 40% of parent Axes' height
            loc="upper right",  # Position of the inset
            borderpad=0.5,
        )

        # 2) Determine which iterations to plot in the inset.
        #    Typically, only iteration 0.
        it_show = 0
        if it_show not in self.processed_data:
            if self.verbose:
                print("No iteration 0 found; skipping lower inset.")
            return

        # 3) Gather data for iteration 0
        it_data = self.processed_data[it_show]

        # 4) Plot contours if available
        if self.draw_contours and self.global_norm is not None:
            best_fit_data = self.data["iteration_data"][it_show].get("best_fit", {})
            likelihood_df = best_fit_data.get("likelihood", None)
            if likelihood_df is not None and ("loglkl" in likelihood_df.columns):
                best_loglkl = likelihood_df["loglkl"].iloc[0]
                self._plot_contours_for_iteration_group(
                    ax=inset_ax,
                    xs_all=self.xs_all,
                    ys_all=self.ys_all,
                    loglkl_all=self.loglkl_all,
                    best_loglkl=best_loglkl,
                    norm=self.global_norm,
                    draw_contours=self.draw_contours,
                    contour_levels=self.contour_levels,
                    grid_size=self.grid_size,
                    sigma=self.sigma,
                    gamma_powernorm=self.gamma_powernorm,
                    discretize_cmap=self.discretize_cmap,
                    n_discretize_cmap=self.n_discretize_cmap,
                    cmap_contour=self.cmap_contour,
                    alpha_contourf=self.alpha_contourf,
                    alpha_contour=self.alpha_contour,
                    contour_line_overlay=self.contour_line_overlay,
                    N=self.N,
                    show_grid=self.show_grid,
                    contour_linewidths=self.contour_linewidths,
                )

        # 5) Plot accumulated discarded_old and discarded_new
        #    Use f"$i < {it_show}$" for the legend label
        df_old_discarded = []
        df_new_discarded = []
        for it in self.iteration_keys:
            if it <= it_show:
                df = self.processed_data[it]["discarded_likelihood_old"]
                if df is not None and not df.empty:
                    df_old_discarded.append(df.assign(iteration=it))
                df_new = self.processed_data[it]["discarded_likelihood_new"]
                if df_new is not None and not df_new.empty:
                    df_new_discarded.append(df_new.assign(iteration=it))

        if df_old_discarded:
            df_old_discarded = pd.concat(df_old_discarded, ignore_index=True)
            self._plot_points(
                inset_ax,
                df_old_discarded,
                "accumulated_discarded_old",
                iteration=f"$i < {it_show}$",
            )

        if df_new_discarded:
            df_new_discarded = pd.concat(df_new_discarded, ignore_index=True)
            self._plot_points(
                inset_ax,
                df_new_discarded,
                "accumulated_discarded_new",
                iteration=it_show,
            )

        # 6) Plot still accepted
        #    Gather accumulated_accepted_still up to iteration 0
        df_accepted_still = []
        for it in self.iteration_keys:
            if it <= it_show:
                df = self.processed_data[it]["accepted_new"]
                if df is not None and not df.empty:
                    df_accepted_still.append(df.assign(iteration=it))

        if df_accepted_still:
            df_accepted_still = pd.concat(df_accepted_still, ignore_index=True)
            self._plot_points(
                inset_ax,
                df_accepted_still,
                "accumulated_accepted_still",
                iteration=it_show,
            )

        # 7) Plot best_fit if available
        if it_data["best_fit"] is not None:
            x_bf, y_bf = it_data["best_fit"]
            style = self.marker_styles.get(
                "best_fit",
                {
                    "marker": "*",
                    "fillstyle": "full",
                    "color": "iter_color",
                    "size": self.marker_size + 70,  # Ensure it's visible in the inset
                    "alpha": 1.0,
                },
            )
            self._scatter_single(inset_ax, x_bf, y_bf, style, iteration=it_show)

        # 8) Set global axis ranges
        inset_ax.set_xlim(self.x_min_global, self.x_max_global)
        inset_ax.set_ylim(self.y_min_global, self.y_max_global)

        # 9) Customize inset appearance
        inset_ax.tick_params(labelsize=5, length=2)  # Smaller font for inset  #

        # Set more ticks on the x-axis
        inset_ax.xaxis.set_major_locator(plt.MaxNLocator(5))
        inset_ax.yaxis.set_major_locator(plt.MaxNLocator(5))

        import matplotlib.patheffects as pe

        for lbl in inset_ax.get_xticklabels() + inset_ax.get_yticklabels():
            # now give it a fine white outline so it pops off the markers
            lbl.set_path_effects(
                [
                    pe.Stroke(linewidth=0.00005, foreground="black"),  # thin border
                    pe.Normal(),
                ]
            )

    def _find_last_complete_iteration(self):
        """
        Return the highest iteration number that is considered 'complete'.
        If none are complete, return None.
        Adjust the 'required_keys' as you see fit
        (or make them optional arguments).
        """
        required_keys = ["accepted_new", "discarded_iteration", "discarded_likelihood"]
        all_iters = sorted(self.data.get("iteration_data", {}).keys())

        last_complete_it = None
        for it in all_iters:
            iteration_dict = self.data["iteration_data"].get(it, {})
            # We'll say an iteration is 'incomplete' if it lacks ANY data in required_keys
            # Or if everything is None/empty.
            all_empty = True
            for rk in required_keys:
                block = iteration_dict.get(rk, {})
                df = block.get("parameters", None)
                if df is not None and not (isinstance(df, pd.DataFrame) and df.empty):
                    # Found something => not "empty" for that key
                    all_empty = False
                    break
            if all_empty:
                # Means this iteration is missing data => break out
                break
            else:
                last_complete_it = it

        return last_complete_it

    def compare_dataframes(
        self,
        df1,
        df2,
        df_likelihood=None,
        df_likelihood2=None,
        comparison_type="new",
        verbose=1,
        compare_context=None,
    ):
        """
        Compare two DataFrames (df1, df2) to identify samples that are 'new', 'removed', or 'common',
        while preserving one-to-one matching of duplicates. Also re-aligns likelihood data if provided.

        Parameters
        ----------
        df1 : pd.DataFrame
            The first DataFrame (e.g., the "current" iteration's data).
        df2 : pd.DataFrame
            The second DataFrame (e.g., the "previous" iteration's data).
        df_likelihood : pd.DataFrame, optional
            Likelihood rows aligned with df1 (same length, same row order as df1 BEFORE sorting).
        df_likelihood2 : pd.DataFrame, optional
            Likelihood rows aligned with df2 (same length, same row order as df2 BEFORE sorting).
        comparison_type : {'new','removed','common'}, default='new'
            - 'new': return rows in df1 that are not in df2.
            - 'removed': return rows in df2 that are not in df1.
            - 'common': return rows present in both df1 and df2.
        verbose : int, optional
            If >0, prints some debugging info.

        Returns
        -------
        matched_params : pd.DataFrame
            Subset of parameter rows that match the requested relationship,
            extracted from the correct perspective (df1 or df2, or intersection).
        matched_likelihood : pd.DataFrame or None
            Subset of likelihood rows that align with matched_params. If none given,
            returns None.
        """

        # --------------------------------------------------
        # Step 1: Validate input parameters
        # --------------------------------------------------
        valid_types = ["new", "removed", "common"]
        if comparison_type not in valid_types:
            raise ValueError(
                f"comparison_type must be one of {valid_types}, got: {comparison_type}"
            )

        # --------------------------------------------------
        # Step 2: Handle edge cases (if df1 or df2 is empty)
        # --------------------------------------------------
        if df1 is None or df1.empty:
            if verbose > 0:
                print(
                    f'\n[compare_dataframes] [{compare_context["context"]}] df1 ({compare_context["df1"]}) is empty; returning trivial result:\n {compare_context["msg1"]}'
                )
            if comparison_type == "removed" and df2 is not None:
                return df2.copy().reset_index(drop=True), df_likelihood2
            return None, None

        if df2 is None or df2.empty:
            if verbose > 0:
                print(
                    f'\n[compare_dataframes] [{compare_context["context"]}] df2 ({compare_context["df2"]}) is empty; returning trivial result:\n {compare_context["msg2"]}'
                )
            if comparison_type == "new":
                return df1.copy().reset_index(drop=True), df_likelihood
            return None, None

        # --------------------------------------------------
        # Step 2b: Ensure likelihood data and dataframes match in length
        if df_likelihood is not None and len(df_likelihood) != len(df1):
            raise ValueError(
                f'\nLength mismatch: df_likelihood ({len(df_likelihood)}) and df1 ({compare_context["df1"]}) ({len(df1)}) are not equal.'
            )
        if df_likelihood2 is not None and len(df_likelihood2) != len(df2):
            raise ValueError(
                f'\nLength mismatch: df_likelihood2 ({len(df_likelihood2)}) and df2 ({compare_context["df2"]}) ({len(df2)}) are not equal.'
            )

        # --------------------------------------------------
        # Step 3: Preprocess df1 (current iteration's data)
        # --------------------------------------------------
        df1 = df1.copy()
        df1["_temp_idx1"] = df1.index  # Store original row index before sorting

        # Identify the relevant parameter columns (excluding helper columns)
        param_cols = [c for c in df1.columns if c not in ["_temp_idx1", "dup_id"]]

        # Sort df1 so that identical samples appear together
        df1_sorted = df1.sort_values(param_cols, kind="mergesort").reset_index(
            drop=True
        )

        # Assign a 'dup_id' to each duplicate row so they can be matched one-to-one
        df1_sorted["dup_id"] = df1_sorted.groupby(param_cols).cumcount()

        # Reorder the likelihood data to match this new sorted order
        df_likelihood_sorted = (
            df_likelihood.iloc[df1_sorted["_temp_idx1"]].reset_index(drop=True)
            if df_likelihood is not None
            else None
        )

        # --------------------------------------------------
        # Step 4: Preprocess df2 (previous iteration's data)
        # --------------------------------------------------
        df2 = df2.copy()
        df2["_temp_idx2"] = df2.index

        df2_sorted = df2.sort_values(param_cols, kind="mergesort").reset_index(
            drop=True
        )
        df2_sorted["dup_id"] = df2_sorted.groupby(param_cols).cumcount()

        df_likelihood2_sorted = (
            df_likelihood2.iloc[df2_sorted["_temp_idx2"]].reset_index(drop=True)
            if df_likelihood2 is not None
            else None
        )

        # --------------------------------------------------
        # Step 5: Perform Merge to Find Matches
        # --------------------------------------------------
        """
        We now compare df1_sorted and df2_sorted to determine which samples belong to which category:
        
        - 'new': Samples in df1 but not in df2 (found using a LEFT JOIN)
        - 'removed': Samples in df2 but not in df1 (found using a RIGHT JOIN)
        - 'common': Samples that exist in both df1 and df2 (found using an INNER JOIN)

        The 'merge' function combines both dataframes based on their common parameter columns + 'dup_id'.
        This ensures that duplicate rows match correctly and one-to-one.
        
        The 'how' parameter controls which type of comparison we perform:
        
        - 'left' (for 'new'): Keeps all rows from df1_sorted, adds matches from df2_sorted.
        - 'right' (for 'removed'): Keeps all rows from df2_sorted, adds matches from df1_sorted.
        - 'inner' (for 'common'): Keeps only rows that exist in BOTH df1_sorted and df2_sorted.

        The 'indicator=True' adds a new column `_merge`, which labels each row as:
        - 'left_only'  → Present only in df1 (new sample)
        - 'right_only' → Present only in df2 (removed sample)
        - 'both'       → Present in both (common sample)
        """
        if comparison_type == "new":
            merge_type = "left"
            indicator = True
        elif comparison_type == "removed":
            merge_type = "right"
            indicator = True
        else:  # 'common'
            merge_type = "inner"
            indicator = False

        merged = df1_sorted.merge(
            df2_sorted,
            on=param_cols + ["dup_id"],
            how=merge_type,
            indicator=indicator,
            suffixes=("_df1", "_df2"),
        )

        # --------------------------------------------------
        # Step 6: Extract the Matching Rows from the Merge
        # --------------------------------------------------
        """
        Now that we have merged df1_sorted and df2_sorted, we extract the rows based on `_merge`:

        - For 'new': We filter only rows labeled as 'left_only' (i.e., samples that appear in df1 but not df2).
        - For 'removed': We filter only rows labeled as 'right_only' (samples in df2 but not df1).
        - For 'common': We take all merged rows, since they exist in both dataframes.
        """
        if comparison_type == "new":
            matched_df = merged[merged["_merge"] == "left_only"].drop(
                columns=["_merge"]
            )
        elif comparison_type == "removed":
            matched_df = merged[merged["_merge"] == "right_only"].drop(
                columns=["_merge"]
            )
        else:  # 'common'
            matched_df = merged

        # Extract the original rows from df1 or df2
        matched_params = (
            df1.iloc[matched_df["_temp_idx1"]].copy()
            if comparison_type != "removed"
            else df2.iloc[matched_df["_temp_idx2"]].copy()
        )

        # Remove helper columns
        matched_params.drop(
            columns=["dup_id", "_temp_idx1", "_temp_idx2"],
            inplace=True,
            errors="ignore",
        )
        matched_params.reset_index(drop=True, inplace=True)

        # --------------------------------------------------
        # Step 7: Extract Aligned Likelihood Data
        # --------------------------------------------------
        matched_likelihood = None
        if comparison_type in ["new", "common"] and df_likelihood_sorted is not None:
            matched_likelihood = (
                df_likelihood.iloc[matched_df["_temp_idx1"]]
                .copy()
                .reset_index(drop=True)
            )
        elif comparison_type == "removed" and df_likelihood2_sorted is not None:
            matched_likelihood = (
                df_likelihood2.iloc[matched_df["_temp_idx2"]]
                .copy()
                .reset_index(drop=True)
            )

        return matched_params, matched_likelihood

    class MulticolorHandler(HandlerBase):
        def __init__(self, colors, marker="o", markersize=8):
            """
            Custom handler to create a legend entry with multiple colors for markers.

            Parameters
            ----------
            colors : list
                List of colors to display.
            marker : str, optional
                Marker style, by default 'o'.
            markersize : int, optional
                Size of the markers, by default 8.
            spacing_factor : float, optional
                Factor to control the spacing between markers, by default 1.4.
            """
            super().__init__()
            self.colors = colors
            self.marker = marker
            self.markersize = markersize

        def create_artists(
            self,
            legend,
            orig_handle,
            xdescent,
            ydescent,
            width,
            height,
            fontsize,
            trans,
        ):
            """
            Create artists for the legend entry.

            Parameters
            ----------
            legend : matplotlib.legend.Legend
                The legend instance.
            orig_handle : object
                Original handle being represented by the legend.
            xdescent : float
                Space taken up by the previous elements in x-direction.
            ydescent : float
                Space taken up by the previous elements in y-direction.
            width : float
                Width allocated for the legend entry.
            height : float
                Height allocated for the legend entry.
            fontsize : float
                Font size of the legend text.
            trans : matplotlib.transforms.Transform
                Transformation to apply to the artists.

            Returns
            -------
            list
                List of artists to display in the legend.
            """

            artists = []
            n_colors = len(self.colors)
            if n_colors == 0:
                return artists  # no markers

            # We'll split the handle's available width among (n_colors+1) segments
            dx = width / (n_colors + 1)
            center_y = (height / 2) - ydescent

            for i, color in enumerate(self.colors, start=1):
                # place each marker at i*dx inside [xdescent, xdescent+width]
                x = xdescent + i * dx
                artist = mlines.Line2D(
                    [x],
                    [center_y],
                    marker=self.marker,
                    color=color,
                    markeredgecolor=color,
                    markersize=self.markersize,
                    linestyle="",
                    transform=trans,
                )
                artists.append(artist)

            return artists


# ---------------------------------CLASS TrianglePlot---------------------------------#
# This class is used to create a 2D pairplot of the final-iteration snapshot of accepted vs. discarded points
# For a selected set of  all iterations and all parameter (as default) or a selected set of iterations and parameters.
# It is mostly designed to provide a broad overview of the parameter space explored by the MCMC sampler and
# identify interesting parameter pairs to investigate further with the PlotIterations class.
# Note: This class can be very memory-intensive for a large number of iterations and parameters.
#      It is not designed to give a detailed view of the parameter space, but rather a broad overview.
class TrianglePlot:
    """
    Create a 2D pairplot (corner plot) of the final-iteration snapshot of accepted versus discarded
    points across iterations. This plot provides a broad overview of the parameter space explored by
    the MCMC sampler and helps identify interesting parameter pairs for further detailed analysis with
    the PlotIterations class.

    The class is designed to handle large datasets (potentially very memory‐intensive) by focusing on
    the final snapshot of the iterative process. By default, all iterations and parameters are included,
    but the user may specify a subset of iterations and/or parameters. Additionally, an optional overlay
    of delta χ² contours can be added to each subplot, with a threshold line defined by the provided
    `param_connect` object.

    Parameters
    ----------
    data : dict
        A master dictionary containing iteration data, where each key (typically an iteration number)
        maps to a sub-dictionary. Each sub-dictionary should include keys for various data categories
        (e.g. 'accepted_new', 'discarded_iteration', etc.), each with associated DataFrames (and, when
        applicable, likelihood data).
    output_folder : str
        Path to the directory where the generated pairplot(s) will be saved.
    save_formats : list of str, optional
        List of file formats (e.g. ['png']) in which to save the plot.
    iterations_to_plot : 'all' or list of int, optional
        Specifies which iterations to include. If 'all', the final snapshot from all valid iterations
        (up to the last complete iteration) is used.
    params_triangleplot : 'all' or list of str, optional
        List of parameter names to include in the pairplot. If 'all', every parameter (except meta columns)
        is plotted.
    verbose : int, optional
        Verbosity level (0 = quiet; higher numbers increase the amount of log output).
    param_labels : dict or None, optional
        A mapping from parameter names to display labels (e.g. LaTeX strings) to be used on the axes.
    ignore_iteration_0_for_axis : bool, optional
        If True, data from iteration 0 is excluded when determining axis limits.
    include_lkldiscard_new_for_axis_range : bool, optional
        If True, data from the 'discarded_likelihood_new' category is included in axis-range determination.
    data_types_to_plot : dict or None, optional
        Dictionary specifying which data types to include in the pairplot. For example, keys may include
        'accepted', 'discarded_likelihood', etc., with boolean values.
    downsampling_fraction : float, optional
        Fraction of data points to randomly remove per group to reduce plot clutter (between 0 and 1).
    colormap : str, optional
        Name of the color palette (e.g. 'tab10') used to assign colors to different iteration groups.
    save_params_loglkls : bool, optional
        If True, the combined parameter and likelihood data (used for contour interpolation) will be saved as a pickle.
    pickle_contour_data : any, optional
        Preloaded data for contour computation. If provided, this data is used instead of extracting it
        from `data`.

    Contour-related parameters:
    ---------------------------
    plot_contours : bool, optional
        If True, overlay delta χ² contours on each pair of parameter subplots.
    grid_size : int, optional
        Number of grid points along each axis for the interpolation grid used in contour plotting.
    sigma : float, optional
        Standard deviation for the Gaussian filter used to smooth the interpolated contour grid.
    interpolation_method : str, optional
        Interpolation method to use with scipy.griddata (e.g. 'linear', 'nearest', 'cubic').
    discretize_cmap : bool, optional
        If True, discretize the contour colormap into a fixed number of colors.
    n_discretize_cmap : int, optional
        Number of discrete color bins to use if `discretize_cmap` is True.
    cmap_contour : str, optional
        Name of the colormap used for drawing the contour plots (e.g. 'viridis').
    alpha_contourf : float, optional
        Opacity for the filled contours.
    alpha_contour : float, optional
        Opacity for the contour lines.
    contour_line_overlay : bool, optional
        If True, draws contour lines on top of the filled contours.
    contour_linewidths : float, optional
        Width (in points) for the contour lines.

    Threshold overlay parameters:
    -----------------------------
    param_connect : object, optional
        An object that holds configuration information (such as `delta_chi2_threshold`) used to
        determine the threshold level for contour plotting.
    plot_threshold_line : bool, optional
        If True and if `param_connect` provides a delta χ² threshold, a line is drawn on each subplot
        indicating the threshold level.
    delta_chi2_threshold_color : str, optional
        Color for the threshold line (e.g. 'purple').
    delta_chi2_threshold_linewidth : float, optional
        Line width for the threshold line.

    Other plotting parameters:
    ----------------------------
    marker_size : int, optional
        Base size for scatter plot markers.
    marker_alpha : float, optional
        Transparency for markers.
    marker_edge_lw : float, optional
        Line width for marker edges.

    Returns
    -------
    None
        The class does not return a value; instead, it saves the generated pairplot to the specified
        output folder.

    Notes
    -----
    - The final snapshot is constructed from the data in the last complete iteration, aggregating points
      from accepted and discarded categories.
    - The pairplot is created using Seaborn, with customization for marker shapes, colors, and legends.
    - If contour plotting is enabled, delta χ² is computed as:
          Δχ² = 2 × (loglkl_all − best_fit_loglkl)
      where best_fit_loglkl is taken from the best-fit likelihood of the last complete iteration.
    - A threshold line is added to each subplot if `plot_threshold_line` is True and a threshold value is provided.
    - Due to memory constraints, this class provides a broad overview of the parameter space rather than
      a detailed examination.

    Example
    -------
    >>> tp = TrianglePlot(
            data=iteration_data_dict,
            output_folder='/path/to/output',
            iterations_to_plot='all',
            params_triangleplot='all',
            verbose=2,
            param_labels={'H0': r'$H_0$', 'omega_b': r'$\\Omega_b$'},
            ignore_iteration_0_for_axis=True,
            plot_contours=True,
            grid_size=1000,
            sigma=10,
            interpolation_method='linear',
            discretize_cmap=False,
            cmap_contour='viridis',
            alpha_contourf=1,
            alpha_contour=0.8,
            contour_line_overlay=False,
            marker_size=2,
            marker_alpha=1,
            marker_edge_lw=0.1,
            param_connect=param_connect_instance,
            plot_threshold_line=True,
            delta_chi2_threshold_color='purple',
            delta_chi2_threshold_linewidth=2
        )
    >>> tp.plot()

    This will generate a corner plot displaying the final-iteration snapshot of the training data
    (accepted vs. discarded), optionally overlaid with delta χ² contours and a threshold indicator.
    """

    def __init__(
        self,
        data,
        output_folder,
        save_formats=["png"],
        iterations_to_plot="all",
        params_triangleplot="all",
        verbose=1,
        param_labels=None,
        ignore_iteration_0_for_axis=False,
        include_lkldiscard_new_for_axis_range=False,
        data_types_to_plot=None,
        downsampling_fraction=0,
        colormap="tab10",
        save_params_loglkls=False,
        pickle_contour_data=None,
        preferred_legend_position="upper right",
        custom_axis_ranges=None,  # dict e.g. {'H0': (50,80), 'Ω_m':[0.1,1]}
        log_scale_params=None,  # iterable/ set e.g. {'H0','Gamma_dcdm'}
        custom_ticks=None,  # dict e.g. {'H0':[40,60,80], 'Ω_m':[.2,.4,.6]}
        grid_vars=None,
        # Below: new contour-related arguments
        plot_contours=False,
        grid_size=1000,  # number of grid points along each axis
        sigma=10,  # smoothing sigma for gaussian_filter
        interpolation_method="linear",
        discretize_cmap=False,
        n_discretize_cmap=20,
        cmap_contour="viridis",
        alpha_contourf=1,
        alpha_contour=0.8,
        contour_line_overlay=False,
        contour_linewidths=0.4,
        marker_size=2,
        marker_alpha=1,
        marker_edge_lw=0.1,
        # NEW: Parameters for the threshold overlay:
        param_connect=None,  # New: pass in the object that holds delta_chi2_threshold
        plot_threshold_line=False,
        delta_chi2_threshold_color="purple",
        delta_chi2_threshold_linewidth=2,
    ):
        """
        Parameters
        ----------
        data : dict
            Master dictionary with structure:
                data["iteration_data"][iteration_number] = {
                   'accepted_new': {...}, 'discarded_iteration': {...}, etc.
                }
        output_folder : str
            Path to the folder where plots will be saved.
        iterations_to_plot : 'all' or list of iteration numbers
            Which iterations to consider. If 'all', use all iteration keys up to the last complete one.
        params_triangleplot : 'all' or list of str
            Which parameters to include in the pairplot.
        verbose : int
            Verbosity level (0=quiet, 1=normal, 2=debug, 3=trace).
        param_labels : dict or None
            Custom axis labels { 'param_name': r'$H_0$', ... } to use in the pairplot.
        ignore_iteration_0_for_axis : bool
            Whether to exclude iteration 0 data when determining axis ranges.
        include_lkldiscard_new_for_axis_range : bool
            Whether to include 'discarded_likelihood_new' in axis-range determination.

        plot_contours : bool
            Whether to plot delta-chi^2 contours in each pair of parameters.
            Number of contour levels (or you can pass an explicit sequence).
        grid_size : int
            Number of points along each axis for the interpolation grid.
        sigma : float
            Gaussian-filter smoothing parameter.
        interpolation_method : str
            'linear', 'nearest', or 'cubic' for scipy.griddata.
        discretize_cmap : bool
            If True, discretize the colormap into n_discretize_cmap bins.
        n_discretize_cmap : int
            Number of discrete colors if discretize_cmap=True.
        cmap_contour : str
            Name of the colormap for the contour plots.
        alpha_contourf : float
            Alpha for the filled contour.
        alpha_contour : float
            Alpha for the contour lines.
        contour_line_overlay : bool
            If True, draws contour lines on top of contourf. If False, only contourf.
        contour_linewidths : float
            Width of contour lines.
        """

        self.data = data
        self.output_folder = output_folder
        self.save_formats = save_formats
        self.iterations_to_plot = iterations_to_plot
        self.params_triangleplot = params_triangleplot
        self.verbose = verbose
        self.param_labels = param_labels
        self.ignore_iteration_0_for_axis = ignore_iteration_0_for_axis
        self.include_lkldiscard_new_for_axis_range = (
            include_lkldiscard_new_for_axis_range
        )
        self.marker_size = marker_size
        self.marker_alpha = marker_alpha
        self.marker_edge_lw = marker_edge_lw
        self.data_types_to_plot = data_types_to_plot
        self.downsampling_fraction = downsampling_fraction
        self.colormap = colormap
        self.preferred_legend_position = preferred_legend_position

        self.custom_axis_ranges = custom_axis_ranges or {}

        # Rewrite custom_axis_ranges to be a dict of tuples, if any of the values are lists
        for key, val in list(self.custom_axis_ranges.items()):
            if isinstance(val, (list, tuple)):
                if len(val) != 2:
                    raise ValueError(
                        f"custom_axis_ranges['{key}'] must be 2-element sequence, got {val!r}"
                    )
                self.custom_axis_ranges[key] = (val[0], val[1])

        if isinstance(log_scale_params, str):
            self.log_scale_params = {log_scale_params}
        else:
            self.log_scale_params = set(log_scale_params or [])
        self.custom_ticks = custom_ticks or {}

        self.grid_vars = grid_vars

        # The main DataFrame used to plot points (accepted/discarded)
        self.df_long = pd.DataFrame()

        # Will store info about how to color/marker different categories
        self.categories_info = {}

        # Contour-related
        self.plot_contours = plot_contours
        self.grid_size = grid_size
        self.sigma = sigma
        self.interpolation_method = interpolation_method
        self.discretize_cmap = discretize_cmap
        self.n_discretize_cmap = n_discretize_cmap
        self.cmap_contour = cmap_contour
        self.alpha_contourf = alpha_contourf
        self.alpha_contour = alpha_contour
        self.contour_line_overlay = contour_line_overlay
        self.contour_linewidths = contour_linewidths

        self.save_params_loglkls = save_params_loglkls
        self.pickle_contour_data = pickle_contour_data

        # Will store a single color normalization / first contour set for a shared colorbar
        self.global_norm = None
        self.first_contour_set = None
        self._delta_chi2_all = None

        # New: threshold overlay parameters
        self.param_connect = param_connect
        self.plot_threshold_line = plot_threshold_line
        self.delta_chi2_threshold_color = delta_chi2_threshold_color
        self.delta_chi2_threshold_linewidth = delta_chi2_threshold_linewidth
        self.delta_chi2_threshold = getattr(
            self.param_connect, "delta_chi2_threshold", None
        )
        self.delta_chi2_threshold = 4800  # test

        # Will store the arrays of (x_all, y_all, loglkl_all) for *all* param pairs,
        # but we only do the interpolation pair by pair.  We'll just keep the raw data:
        self.param_arrays = {}  # dict keyed by (paramX, paramY) -> (xs, ys, delta_chi2)

        self.kde_styles = {
            "accepted": {"fill": False, "linestyle": "-"},  # solid edge
            "discarded_likelihood": {"fill": False, "linestyle": "--"},  # dashed edge
            "discarded_oversampling": {
                "fill": False,
                "linestyle": ":",  # dash-dot edge
            },
            "discarded_iteration": {"fill": False, "linestyle": "-."},  # dotted edge
        }

        if self.data_types_to_plot is None:
            self.data_types_to_plot = {
                "accepted": True,
                "discarded_likelihood": True,
                "discarded_oversampling": True,
                "discarded_iteration": True,
            }

    ############################################################################
    # Main plot method
    ############################################################################
    def plot(self, diag_kind="kde", corner=True, include_discarded=True):
        """
        Build the final-snapshot DataFrame and create the Seaborn pairplot.
        If plot_contours=True, overlay delta-chi^2 contours on each subplot.

        Parameters
        ----------
        diag_kind : str
            'kde' or 'hist' for the diagonal subplots.
        corner : bool
            If True, only plot the lower triangle (and diagonal).
        include_discarded : bool
            If True, includes both accepted+discarded points. If False, accepted only.
        """

        matplotlib.rcParams.update(matplotlib.rcParamsDefault)

        matplotlib.use("Agg")

        # -- Basic figure styling
        fontsize = 11
        latex_preamble = r"\usepackage{siunitx} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{mathtools} \usepackage{bm} \usepackage{mathrsfs} \parindent=0pt"

        if self.param_labels is not None:
            # Use LaTeX for styling if param_labels is set
            matplotlib.rcParams.update(
                {
                    "text.usetex": False,
                    "font.family": "serif",
                    "font.serif": "cmr10",
                    "font.size": fontsize,
                    "mathtext.fontset": "cm",
                    "text.latex.preamble": latex_preamble,
                }
            )
        else:
            # Do not interpret underscores or other symbols as math text
            matplotlib.rcParams.update(
                {
                    "text.usetex": False,  # Disable LaTeX
                    "font.family": "serif",
                    "font.serif": "cmr10",
                    "font.size": fontsize,
                    "mathtext.fontset": "dejavusans",  # Or another fontset that supports plain text
                }
            )

        plt.rcParams["figure.dpi"] = 1000
        plt.rcParams["savefig.dpi"] = 1000
        matplotlib.rcParams["hatch.linewidth"] = 0.5
        plt.rcParams["xtick.labelsize"] = 8
        plt.rcParams["ytick.labelsize"] = 8
        plt.rcParams["legend.fontsize"] = 8

        # 1) Find last complete iteration
        last_complete_it = self._find_last_complete_iteration()
        if self.verbose >= 3:
            print(f"Last complete iteration: {last_complete_it}")
        if last_complete_it is None:
            print("No complete iterations found. Exiting.")
            return

        # 2) Determine which iterations to use
        if self.iterations_to_plot == "all":
            iters_to_use = sorted(
                it for it in self.data["iteration_data"] if it <= last_complete_it
            )
        else:
            iters_to_use = sorted(
                it for it in self.iterations_to_plot if it <= last_complete_it
            )

        if self.verbose >= 2:
            print(f"Using iterations up to {last_complete_it}: {iters_to_use}")

        # 3) Build final snapshot DF: accepted vs. discarded
        self.df_long = self._build_final_snapshot_df(
            iters_to_use, last_complete_it, params=self.params_triangleplot
        )
        if self.verbose >= 2:
            print(f"Final DataFrame shape: {self.df_long.shape}")
            print(
                f"Final group counts:\n{self.df_long['group_key'].value_counts(dropna=False)}"
            )

        if self.df_long.empty:
            print("No data to plot after final snapshot filtering. Exiting.")
            return

        self._filter_data_types()

        # Optionally drop discarded points
        if not include_discarded:
            self.df_long = self.df_long[self.df_long["status"] == "accepted"]
            if self.df_long.empty:
                print("No accepted data to plot. Exiting.")
                return

        # 6) Apply downsampling if requested
        if self.downsampling_fraction > 0.0:
            self._downsample_df_long()
            if self.df_long.empty:
                print("No data to plot after downsampling. Exiting.")
                return

        # If ignoring iteration 0 data for axis range, figure out param limits
        param_limits = None
        if self.ignore_iteration_0_for_axis:
            param_limits = self._determine_axis_ranges(iters_to_use)

        # default limits (from data) + user-overrides
        combined_limits = {}
        if param_limits is not None:
            combined_limits.update(param_limits)
        if self.custom_axis_ranges is not None:
            combined_limits.update(self.custom_axis_ranges)

        # Decide which columns to plot
        meta_cols = [
            "iteration_origin",
            "iteration_discard",
            "discard_reason",
            "status",
            "group_key",
        ]
        if self.params_triangleplot == "all":
            plot_cols = [c for c in self.df_long.columns if c not in meta_cols]
        else:
            plot_cols = list(self.params_triangleplot)
        if self.verbose >= 2:
            print(f"Plotting columns: {plot_cols}")

        # 4) Prepare palette + markers
        hue_col = "group_key"
        palette_dict, markers_dict, hue_order = self._make_custom_palette_and_markers()

        # Convert group_key to Categorical with a defined order
        self.df_long["group_key"] = pd.Categorical(
            self.df_long["group_key"], categories=hue_order, ordered=True
        )
        self.df_long.sort_values("group_key", inplace=True)

        # Set fixed figure size
        textwidth = 440  # JCAP uses a textwidth of 440 pts
        width = textwidth / 72.27
        # height = width  # square figure
        # pp.figure.set_size_inches(width, height)

        n = len(plot_cols)
        facet_size = width / n

        # 5) Create pairplot
        pp = sns.pairplot(
            data=self.df_long,
            hue=hue_col,
            hue_order=hue_order[::-1],  # reverse or not, up to you
            vars=plot_cols,
            palette=palette_dict,
            markers=markers_dict,
            diag_kind=diag_kind,
            corner=corner,
            height=facet_size,  # <— makes each subplot facet_size×facet_size
            aspect=1,  # <— enforces 1:1 physical aspect
            plot_kws={
                "s": self.marker_size,
                "alpha": self.marker_alpha,
                "edgecolor": "black",
                "linewidth": self.marker_edge_lw,
            },  # Adjust size/alpha as needed
            diag_kws={"cut": 0},
        )

        # Set to zero white space between subplots
        pp.figure.subplots_adjust(wspace=0, hspace=0)

        # pp.fig.set_size_inches(width, width)

        # 6) (Optional) apply custom axis labels
        if self.param_labels:
            self._apply_custom_axis_labels(pp, x_vars=plot_cols, y_vars=plot_cols)

        # Remove Seaborn’s default legend and create a custom one
        self.palette_dict = palette_dict

        self._customize_kde_styles(pp, hue_order)

        # 7) (Optional) fix axis ranges if ignoring iteration 0
        if combined_limits is not None:
            num_vars = len(plot_cols)
            for i in range(num_vars):
                for j in range(num_vars):
                    ax_ij = pp.axes[i, j]
                    if ax_ij is None:
                        continue
                    param_x = plot_cols[j]
                    param_y = plot_cols[i]

                    if param_x in combined_limits:
                        ax_ij.set_xlim(combined_limits[param_x])  # <<< MOD >>>
                    if param_y in combined_limits:
                        ax_ij.set_ylim(combined_limits[param_y])  # <<< MOD >>>

                    # ---------- log–scales ----------                    # <<< NEW >>>
                    if param_x in self.log_scale_params:
                        ax_ij.set_xscale("log")
                    if param_y in self.log_scale_params:
                        ax_ij.set_yscale("log")

                    # ---------- custom ticks ----------                   # <<< NEW >>>
                    if param_x in self.custom_ticks:
                        ax_ij.set_xticks(self.custom_ticks[param_x])
                    if param_y in self.custom_ticks:
                        ax_ij.set_yticks(self.custom_ticks[param_y])

        # 8) If requested, plot the 2D \Delta \chi^2 contours on each subplot
        if self.plot_contours:
            self._plot_contours_on_pairplot(
                pp, plot_cols, plot_cols, last_complete_it, plot_mode="triangle"
            )

        for ax in pp.axes.flatten():
            if ax is None:
                continue

            # a+b) all four spines on, width=1.0
            for spine in ax.spines.values():
                spine.set_visible(True)
                spine.set_linewidth(1.0)

            if ax.get_xscale() == ax.get_yscale() == "linear":
                ax.ticklabel_format(style="sci", axis="both", scilimits=(-3, 3))

            # c) ticks point in
            # ax.tick_params(direction='in', top=True, right=True, labelsize=8) # Adjust

            # 1) grab the built-in offset Text object
            off = ax.yaxis.get_offset_text()
            txt = off.get_text().strip()
            # hide the default one
            off.set_visible(False)

            # if there was anything to show, place it yourself:
            if txt:
                # 2) find the top‐left corner of this Axes in figure coords
                bb = ax.get_position()  # Bbox(x0, y0, x1, y1) in fig‐fraction
                x_fig = bb.x0 - 0.005  # a hair to the left
                y_fig = bb.y1 - 0.01  # a hair below

                # 3) put it there
                #    (ha='right' so it reads back into the plot, va='bottom' so it's just above)
                pp.figure.text(
                    x_fig,
                    y_fig,
                    txt,
                    ha="right",
                    va="bottom",
                    transform=pp.figure.transFigure,
                    fontsize=ax.yaxis.get_offset_text().get_fontsize(),
                )

        legend = self._create_custom_legend(pp)

        # from matplotlib.patches import Rectangle

        # # compute the extent of the grid in figure coords
        # l, b, w, h = (
        #     pp.figure.subplotpars.left,
        #     pp.figure.subplotpars.bottom,
        #     pp.figure.subplotpars.right - pp.figure.subplotpars.left,
        #     pp.figure.subplotpars.top - pp.figure.subplotpars.bottom,
        # )

        # pp.figure.patches.append(
        #     Rectangle(
        #         (l, b),
        #         w,
        #         h,
        #         transform=pp.figure.transFigure,
        #         fill=False,
        #         edgecolor="0.5",
        #         linewidth=1,
        #         linestyle="--",
        #     )
        # )

        # 9) Save
        os.makedirs(self.output_folder, exist_ok=True)
        os.makedirs(os.path.join(self.output_folder, "pairplots"), exist_ok=True)
        for fmt in self.save_formats:
            filename = f"triangle_plot.{fmt}"
            save_path = os.path.join(self.output_folder, "pairplots", filename)
            pp.savefig(
                save_path, dpi=1000, format=fmt, bbox_inches="tight"
            )  # bbox_inches="tight"
            if self.verbose >= 1:
                print(f"Saved plot to {save_path}")

    def plot_matrix(self, diag_kind="kde", corner=True, include_discarded=True):
        """
        Build the final-snapshot DataFrame and create the Seaborn pairplot.
        If plot_contours=True, overlay delta-chi^2 contours on each subplot.

        Parameters
        ----------
        diag_kind : str
            'kde' or 'hist' for the diagonal subplots.
        corner : bool
            If True, only plot the lower triangle (and diagonal).
        include_discarded : bool
            If True, includes both accepted+discarded points. If False, accepted only.
        """

        matplotlib.rcParams.update(matplotlib.rcParamsDefault)

        matplotlib.use("Agg")

        # -- Basic figure styling
        fontsize = 11
        latex_preamble = r"\usepackage{siunitx} \usepackage{amsmath} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{mathtools} \usepackage{bm} \usepackage{mathrsfs} \parindent=0pt"

        if self.param_labels is not None:
            # Use LaTeX for styling if param_labels is set
            matplotlib.rcParams.update(
                {
                    "text.usetex": False,
                    "font.family": "serif",
                    "font.serif": "cmr10",
                    "font.size": fontsize,
                    "mathtext.fontset": "cm",
                    "text.latex.preamble": latex_preamble,
                }
            )
        else:
            # Do not interpret underscores or other symbols as math text
            matplotlib.rcParams.update(
                {
                    "text.usetex": False,  # Disable LaTeX
                    "font.family": "serif",
                    "font.serif": "cmr10",
                    "font.size": fontsize,
                    "mathtext.fontset": "dejavusans",  # Or another fontset that supports plain text
                }
            )

        plt.rcParams["figure.dpi"] = 600
        plt.rcParams["savefig.dpi"] = 600
        matplotlib.rcParams["hatch.linewidth"] = 0.5
        plt.rcParams["xtick.labelsize"] = 8
        plt.rcParams["ytick.labelsize"] = 8
        plt.rcParams["legend.fontsize"] = 8

        # 1) Find last complete iteration
        last_complete_it = self._find_last_complete_iteration()
        if self.verbose >= 3:
            print(f"Last complete iteration: {last_complete_it}")
        if last_complete_it is None:
            print("No complete iterations found. Exiting.")
            return

        # 2) Determine which iterations to use
        if self.iterations_to_plot == "all":
            iters_to_use = sorted(
                it for it in self.data["iteration_data"] if it <= last_complete_it
            )
        else:
            iters_to_use = sorted(
                it for it in self.iterations_to_plot if it <= last_complete_it
            )

        if self.verbose >= 2:
            print(f"Using iterations up to {last_complete_it}: {iters_to_use}")

        # 3) Build final snapshot DF: accepted vs. discarded
        if self.grid_vars is None:
            params = "all"
        else:
            if self.grid_vars["columns"] == "all":
                params = "all"
            else:
                params = self.grid_vars["columns"] + self.grid_vars["rows"]
                # Only unique params
                params = list(set(params))

        self.df_long = self._build_final_snapshot_df(
            iters_to_use, last_complete_it, params=params
        )
        if self.verbose >= 2:
            print(f"Final DataFrame shape: {self.df_long.shape}")
            print(
                f"Final group counts:\n{self.df_long['group_key'].value_counts(dropna=False)}"
            )

        if self.df_long.empty:
            print("No data to plot after final snapshot filtering. Exiting.")
            return

        self._filter_data_types()

        # Optionally drop discarded points
        if not include_discarded:
            self.df_long = self.df_long[self.df_long["status"] == "accepted"]
            if self.df_long.empty:
                print("No accepted data to plot. Exiting.")
                return

        # 6) Apply downsampling if requested
        if self.downsampling_fraction > 0.0:
            self._downsample_df_long()
            if self.df_long.empty:
                print("No data to plot after downsampling. Exiting.")
                return

        # If ignoring iteration 0 data for axis range, figure out param limits
        param_limits = None
        if self.ignore_iteration_0_for_axis:
            param_limits = self._determine_axis_ranges(iters_to_use)

        # default limits (from data) + user-overrides
        combined_limits = {}
        if param_limits is not None:
            combined_limits.update(param_limits)
        if self.custom_axis_ranges is not None:
            combined_limits.update(self.custom_axis_ranges)

        # Decide which columns to plot
        meta_cols = [
            "iteration_origin",
            "iteration_discard",
            "discard_reason",
            "status",
            "group_key",
        ]

        if self.grid_vars is not None:
            x_vars = self.grid_vars["columns"]
            if x_vars == "all":
                x_vars = [c for c in self.df_long.columns if c not in meta_cols]
            y_vars = self.grid_vars["rows"]

            if not x_vars or not y_vars:
                print("No variables to plot. Exiting.")
                return

        # 4) Prepare palette + markers
        hue_col = "group_key"
        palette_dict, markers_dict, hue_order = self._make_custom_palette_and_markers()

        # Convert group_key to Categorical with a defined order
        self.df_long["group_key"] = pd.Categorical(
            self.df_long["group_key"], categories=hue_order, ordered=True
        )
        self.df_long.sort_values("group_key", inplace=True)

        # Set fixed figure size
        textwidth = 440  # JCAP uses a textwidth of 440 pts
        width = textwidth / 72.27
        # height = width  # square figure
        # pp.figure.set_size_inches(width, height)

        n = len(x_vars)
        facet_size = width / n

        # 5) Create pairplot
        pp = sns.pairplot(
            data=self.df_long,
            hue=hue_col,
            hue_order=hue_order[::-1],  # reverse or not, up to you
            # vars=plot_cols,
            x_vars=x_vars,
            y_vars=y_vars,
            palette=palette_dict,
            markers=markers_dict,
            diag_kind=diag_kind,
            # corner=corner,
            height=facet_size,  # <— makes each subplot facet_size×facet_size
            aspect=1,  # <— enforces 1:1 physical aspect
            plot_kws={
                "s": self.marker_size,
                "alpha": self.marker_alpha,
                "edgecolor": "black",
                "linewidth": self.marker_edge_lw,
            },  # Adjust size/alpha as needed
            diag_kws={"cut": 0},
        )

        # Set to zero white space between subplots
        pp.figure.subplots_adjust(wspace=0, hspace=0)

        # pp.fig.set_size_inches(width, width)

        # 6) (Optional) apply custom axis labels
        if self.param_labels:

            self._apply_custom_axis_labels(pp, x_vars, y_vars)

        # Remove Seaborn’s default legend and create a custom one
        self.palette_dict = palette_dict

        self._customize_kde_styles(pp, hue_order)

        # 7) (Optional) fix axis ranges if ignoring iteration 0
        if combined_limits is not None:
            for i in range(len(y_vars)):
                for j in range(len(x_vars)):
                    ax_ij = pp.axes[i, j]
                    if ax_ij is None:
                        continue
                    param_x = x_vars[j]
                    param_y = y_vars[i]

                    if param_x in combined_limits:
                        ax_ij.set_xlim(combined_limits[param_x])  # <<< MOD >>>
                    if param_y in combined_limits:
                        ax_ij.set_ylim(combined_limits[param_y])  # <<< MOD >>>

                    # --------- log–scales ----------                    # <<< NEW >>>
                    if param_x in self.log_scale_params:
                        ax_ij.set_xscale("log")
                    if param_y in self.log_scale_params:
                        ax_ij.set_yscale("log")

                    # ---------- custom ticks ----------                   # <<< NEW >>>
                    if param_x in self.custom_ticks:
                        ax_ij.set_xticks(self.custom_ticks[param_x])
                    if param_y in self.custom_ticks:
                        ax_ij.set_yticks(self.custom_ticks[param_y])

        # 8) If requested, plot the 2D \Delta \chi^2 contours on each subplot
        if self.plot_contours:
            self._plot_contours_on_pairplot(
                pp, x_vars, y_vars, last_complete_it, plot_mode="matrix"
            )

        for ax in pp.axes.flatten():
            if ax is None:
                continue

            # a+b) all four spines on, width=1.0
            for spine in ax.spines.values():
                spine.set_visible(True)
                spine.set_linewidth(1.0)

            if ax.get_xscale() == ax.get_yscale() == "linear":
                ax.ticklabel_format(style="sci", axis="both", scilimits=(-3, 3))

            # c) ticks point in
            # ax.tick_params(direction='in', top=True, right=True, labelsize=8) # Adjust

            # 1) grab the built-in offset Text object
            off = ax.yaxis.get_offset_text()
            txt = off.get_text().strip()
            # hide the default one
            off.set_visible(False)

            # if there was anything to show, place it yourself:
            # And ax is the first column (most left):
            if txt and ax.get_subplotspec().colspan.start == 0:
                # 2) find the top‐left corner of this Axes in figure coords
                bb = ax.get_position()  # Bbox(x0, y0, x1, y1) in fig‐fraction
                x_fig = bb.x0 - 0.005  # a hair to the left
                y_fig = bb.y1 - 0.01  # a hair below

                # 3) put it there
                #    (ha='right' so it reads back into the plot, va='bottom' so it's just above)
                pp.figure.text(
                    x_fig,
                    y_fig,
                    txt,
                    ha="right",
                    va="bottom",
                    transform=pp.figure.transFigure,
                    fontsize=ax.yaxis.get_offset_text().get_fontsize(),
                )

        self.preferred_legend_position = "center"

        legend = self._create_custom_legend(pp)

        # from matplotlib.patches import Rectangle

        # # compute the extent of the grid in figure coords
        # l, b, w, h = (
        #     pp.figure.subplotpars.left,
        #     pp.figure.subplotpars.bottom,
        #     pp.figure.subplotpars.right - pp.figure.subplotpars.left,
        #     pp.figure.subplotpars.top - pp.figure.subplotpars.bottom,
        # )

        # pp.figure.patches.append(
        #     Rectangle(
        #         (l, b),
        #         w,
        #         h,
        #         transform=pp.figure.transFigure,
        #         fill=False,
        #         edgecolor="0.5",
        #         linewidth=1,
        #         linestyle="--",
        #     )
        # )

        # 9) Save
        os.makedirs(self.output_folder, exist_ok=True)
        os.makedirs(os.path.join(self.output_folder, "pairplots"), exist_ok=True)
        for fmt in self.save_formats:
            filename = f"grid_plot.{fmt}"
            save_path = os.path.join(self.output_folder, "pairplots", filename)
            pp.savefig(
                save_path, dpi=600, format=fmt, bbox_inches="tight"
            )  # bbox_inches="tight"
            if self.verbose >= 1:
                print(f"Saved plot to {save_path}")

    ############################################################################
    # Internal helper methods
    ############################################################################

    def _find_last_complete_iteration(self):
        """
        Determines the last complete iteration by ensuring that at least one required key
        contains valid data in each iteration.

        Returns
        -------
        last_complete_it : int or None
            The number of the last complete iteration, or None if none are complete.
        """
        required_keys = ["accepted_new", "discarded_iteration", "discarded_likelihood"]
        all_iters = sorted(self.data["iteration_data"].keys())

        last_complete_it = None

        for it in all_iters:
            iteration_data = self.data["iteration_data"].get(it, {})
            # Check if **all** required keys have empty or None parameters
            all_empty = True
            for key in required_keys:
                block = iteration_data.get(key, {})
                df = block.get("parameters", None)
                if df is not None and not (isinstance(df, pd.DataFrame) and df.empty):
                    # At least one key has valid data
                    all_empty = False
                    if self.verbose >= 3:
                        print(f"Iteration {it}: Key '{key}' has data.")
                    break
                else:
                    if self.verbose >= 3:
                        print(f"Iteration {it}: Key '{key}' is empty or None.")
            if all_empty:
                # Incomplete iteration detected
                if self.verbose >= 2:
                    print(
                        f"Iteration {it} is incomplete. Stopping at iteration {last_complete_it}."
                    )
                break
            else:
                # Complete iteration; update last_complete_it
                last_complete_it = it
                if self.verbose >= 3:
                    print(f"Iteration {it} is complete.")

        return last_complete_it

    def _build_final_snapshot_df(self, iters_to_use, last_complete_it, params):
        """
        Creates a long-form DataFrame with final snapshot data:
        - still accepted points at final iteration range
        - discarded points in the iteration range
        """
        # 1) Gather accepted & discarded
        still_accepted_df = self._gather_still_accepted_points(
            iters_to_use, last_complete_it
        )
        discarded_df = self._gather_discarded_points(iters_to_use, last_complete_it)
        df_full = pd.concat([still_accepted_df, discarded_df], ignore_index=True)

        if df_full.empty:
            if self.verbose:
                print("No accepted or discarded data => empty DataFrame.")
            return df_full

        # 2) Figure out the parameter columns
        meta_cols = [
            "iteration_origin",
            "iteration_discard",
            "discard_reason",
            "status",
            "group_key",
        ]

        if params == "all":
            # Take everything that’s not a meta col
            candidate_cols = [c for c in df_full.columns if c not in meta_cols]
        else:
            # Use only user-specified columns
            candidate_cols = params

        if not candidate_cols:
            if self.verbose:
                print("No parameter columns found => returning empty DataFrame.")
            return pd.DataFrame()

        # 3) Filter df_full to keep only those columns + meta
        keep_cols = candidate_cols + meta_cols
        df_filtered = df_full[keep_cols]

        if self.verbose >= 3:
            print(f"Final snapshot DataFrame shape: {df_filtered.shape}")
            print(f"Group counts:\n{df_filtered['group_key'].value_counts()}")

        return df_filtered

    def _gather_still_accepted_points(self, iters_to_use, last_complete_it):
        """
        Gathers points that remain accepted at iteration `last_complete_it`.
        Removes any that were discarded in earlier iterations.

        Parameters
        ----------
        iters_to_use : list of int
            Iterations to include.
        last_complete_it : int
            The last complete iteration number.

        Returns
        -------
        df_accepted : pd.DataFrame
            DataFrame of still accepted points.
        """
        # 1) Gather all "accepted_new" from each iteration in `iters_to_use`
        #    We'll store them in one DataFrame with an 'iteration_origin' column
        accepted_list = []
        for it in iters_to_use:
            df_new = self._get_parameters_df(it, "accepted_new")
            if df_new is not None and not df_new.empty:
                # Tag them with iteration_origin=it
                df_new = df_new.copy()
                df_new["iteration_origin"] = it
                accepted_list.append(df_new)

        if accepted_list:
            all_accepted_new = pd.concat(accepted_list, ignore_index=True)
        else:
            # No accepted points
            return pd.DataFrame()

        # 2) Gather discards that happen up to last_complete_it,
        #    so we can remove them from the accepted set
        discards = []
        # We'll specifically look at *all* relevant discard categories that can re-discard accepted points:
        # e.g. "discarded_iteration", "discarded_oversampling", "discarded_likelihood", ...
        discard_keys = [
            "discarded_likelihood",
            "discarded_likelihood_new",
            "discarded_likelihood_old",
        ]
        for it in iters_to_use:
            for dk in discard_keys:
                df_dk = self._get_parameters_df(it, dk)
                if df_dk is not None and not df_dk.empty:
                    df_dk = df_dk.copy()
                    df_dk["iteration_discard"] = it
                    discards.append(df_dk)
        if discards:
            df_discards = pd.concat(discards, ignore_index=True)
        else:
            df_discards = pd.DataFrame(columns=all_accepted_new.columns)

        # 3) Remove from accepted set any that appear in df_discards
        if not df_discards.empty:
            # Merge on all parameter columns
            param_list = list(all_accepted_new.columns)
            # remove meta columns we introduced
            param_list = [p for p in param_list if p not in ("iteration_origin",)]
            # Ensure that all discard DataFrames have the same parameter columns
            # It's assumed that parameter columns are consistent across iterations and discard keys

            # To prevent AttributeError, ensure df_discards has the necessary columns
            # Here, we assume that parameter columns are consistent, as per the completeness check
            merged = all_accepted_new.merge(
                df_discards[param_list], on=param_list, how="left", indicator=True
            )
            # Keep only rows that didn't match => those remain accepted
            still_accepted = merged[merged["_merge"] == "left_only"].drop(
                columns=["_merge"]
            )
        else:
            still_accepted = all_accepted_new

        if still_accepted.empty:
            return pd.DataFrame()

        # 4) Format final
        # Add columns for iteration_discard=None, discard_reason=None, status='accepted'
        still_accepted["iteration_discard"] = None
        still_accepted["discard_reason"] = None
        still_accepted["status"] = "accepted"

        # 5) Create group_key => 'accepted__it=XYZ' to use in pairplot hue
        def make_group_key(row):
            return f"accepted__it={row['iteration_origin']}"

        still_accepted["group_key"] = still_accepted.apply(make_group_key, axis=1)

        return still_accepted

    def _gather_discarded_points(self, iters_to_use, last_complete_it):
        """
        Gathers all points that were discarded *by* iteration in iters_to_use,
        up to `last_complete_it`. Each row is assigned iteration_discard=that iteration,
        plus a discard_reason. iteration_origin is unknown.

        Parameters
        ----------
        iters_to_use : list of int
            Iterations to include.
        last_complete_it : int
            The last complete iteration number.

        Returns
        -------
        df_discards : pd.DataFrame
            DataFrame of discarded points.
        """
        discard_records = []

        # Map each discard key to a short reason label
        reason_map = {
            "discarded_iteration": "iteration",
            "discarded_oversampling": "oversampling",
            "discarded_likelihood": "likelihood",
            "discarded_likelihood_new": "likelihood",
            "discarded_likelihood_old": "likelihood",
        }
        for it in iters_to_use:
            # For each discard key
            for dk, reason_label in reason_map.items():
                df_dk = self._get_parameters_df(it, dk)
                if df_dk is not None and not df_dk.empty:
                    df_dk = df_dk.copy()
                    df_dk["iteration_origin"] = None  # we don't track origin
                    df_dk["iteration_discard"] = it
                    df_dk["discard_reason"] = reason_label
                    df_dk["status"] = "discarded"

                    # We'll define group_key => 'discarded_{reason}__it={it}'
                    # e.g. 'discarded_likelihood__it=5'
                    df_dk["group_key"] = df_dk.apply(
                        lambda row: f"discarded_{reason_label}__it={it}", axis=1
                    )

                    discard_records.append(df_dk)

        if discard_records:
            df_discards = pd.concat(discard_records, ignore_index=True)
        else:
            df_discards = pd.DataFrame()

        return df_discards

    def _get_parameters_df(self, iteration, key):
        """
        Safely fetch the 'parameters' DataFrame from data["iteration_data"][iteration][key].
        Returns None if missing/empty.

        Parameters
        ----------
        iteration : int
            Iteration number.
        key : str
            Key within the iteration (e.g., 'accepted_new').

        Returns
        -------
        df : pd.DataFrame or None
            The parameters DataFrame or None if not available.
        """
        iteration_dict = self.data["iteration_data"].get(iteration, {})
        block = iteration_dict.get(key, {})
        df = block.get("parameters", None)
        if df is None:
            if self.verbose >= 2:
                print(f"Parameters for key '{key}' in iteration {iteration} are None.")
            return None
        if isinstance(df, str) and df.lower() == "none":
            if self.verbose >= 2:
                print(
                    f"Parameters for key '{key}' in iteration {iteration} are 'None'."
                )
            return None
        if isinstance(df, pd.DataFrame) and df.empty:
            if self.verbose >= 2:
                print(
                    f"Parameters DataFrame for key '{key}' in iteration {iteration} is empty."
                )
            return None
        return df.copy()

    def _get_param_list(self):
        """
        Determine which parameters to plot, based on `params_triangleplot`.

        Returns
        -------
        list of str
            List of parameter names to include in the pairplot.
        """
        if self.params_triangleplot == "all":
            # We rely on self.df_long to figure out all param columns
            # but we haven't built it yet at init time. So for pairplot, we do it after DF is built.
            # let's assume after building the DF we check all non-meta columns as param columns
            if not self.df_long.empty:
                meta_cols = [
                    "iteration_origin",
                    "iteration_discard",
                    "discard_reason",
                    "status",
                    "group_key",
                ]
                return [col for col in self.df_long.columns if col not in meta_cols]
            else:
                return []
        else:
            # user gave a specific list
            return list(self.params_triangleplot)

    def _make_custom_palette_and_markers(self):
        """
        Build a dictionary for `palette` and `markers` keyed by each unique `group_key`.
        Also, create a `hue_order` list based on defined priority.

        Returns
        -------
        palette_dict : dict
            Mapping from group_key to color.
        markers_dict : dict
            Mapping from group_key to marker style.
        hue_order : list
            Ordered list of group_keys based on priority.
        """
        if self.df_long.empty:
            return {}, {}, []

        unique_groups = self.df_long["group_key"].unique()

        # Define marker styles for different discard reasons
        reason_to_marker = {
            "iteration": "^",  # triangle
            "oversampling": "s",  # square
            "likelihood": "o",  # circle
            "unknown": "x",  # cross for unknown reasons
        }

        # Define priority mapping
        priority_map = {}
        for grp in unique_groups:
            if grp.startswith("accepted__it="):
                # Extract iteration number
                it_num = int(grp.split("accepted__it=")[1])
                # Status priority: accepted = 3
                status_priority = 3
                # Discard reason priority: not applicable
                reason_priority = 0
            elif grp.startswith("discarded_likelihood__it="):
                it_num = int(grp.split("discarded_likelihood__it=")[1])
                status_priority = 2
                reason_priority = 3
            elif grp.startswith("discarded_oversampling__it="):
                it_num = int(grp.split("discarded_oversampling__it=")[1])
                status_priority = 2
                reason_priority = 2
            elif grp.startswith("discarded_iteration__it="):
                it_num = int(grp.split("discarded_iteration__it=")[1])
                status_priority = 2
                reason_priority = 1
            else:
                # Unknown groups get the lowest priority
                it_num = 0
                status_priority = 0
                reason_priority = 0

            # Higher iteration number means higher priority within the same status and reason
            priority_map[grp] = (status_priority, reason_priority, it_num)

        # **Sort groups based on priority: lower status_priority first, then lower reason_priority, then higher iteration**
        sorted_groups = sorted(
            unique_groups, key=lambda x: priority_map[x], reverse=False
        )

        # Separate discarded and accepted groups
        discarded_groups = [g for g in sorted_groups if g.startswith("discarded_")]
        accepted_groups = [g for g in sorted_groups if g.startswith("accepted__it=")]

        # Extract unique iterations from discarded_groups
        discarded_iterations = sorted(
            set(int(g.split("__it=")[1]) for g in discarded_groups),
            reverse=False,  # Assuming higher iterations are "later" and thus darker
        )

        # Assign greyscale colors to discarded iterations
        greys_cmap = plt.cm.Greys
        n_disc_iters = max(len(discarded_iterations), 1)
        grey_values = np.linspace(0.3, 0.8, n_disc_iters)  # From light to dark

        iteration_to_grey = {
            it: greys_cmap(grey_values[i]) for i, it in enumerate(discarded_iterations)
        }

        palette_dict = {}
        markers_dict = {}

        # Assign colors and markers for discarded groups
        for grp in discarded_groups:
            # Parse group_key to extract reason and iteration
            # Expected format: 'discarded_{reason}__it={it}'
            sub = grp[len("discarded_") :]  # e.g., 'likelihood__it=5'
            parts = sub.split("__it=")
            if len(parts) == 2:
                reason = parts[0]  # e.g., 'likelihood'
                iteration_str = parts[1]  # e.g., '5'
                try:
                    it_num = int(iteration_str)
                except ValueError:
                    it_num = None
            else:
                reason = "unknown"
                it_num = None

            # Assign greyscale color based on iteration
            if it_num in iteration_to_grey:
                color_rgba = iteration_to_grey[it_num]
            else:
                color_rgba = (0.5, 0.5, 0.5, 1.0)  # Default grey if iteration not found

            palette_dict[grp] = color_rgba

            # Assign marker based on reason
            mark = reason_to_marker.get(
                reason, "x"
            )  # fallback to 'x' if reason not recognized
            markers_dict[grp] = mark

        # Assign colors to accepted groups using a color palette (e.g., tab10)
        accepted_cmap = sns.color_palette(
            self.colormap, n_colors=max(10, len(accepted_groups))
        )

        # Assign colors and markers for accepted groups last
        for i, grp in enumerate(accepted_groups):
            palette_dict[grp] = accepted_cmap[i % len(accepted_cmap)]
            markers_dict[grp] = "o"  # circle for accepted

        # Define hue_order: discarded groups first, accepted groups last
        hue_order = discarded_groups + accepted_groups

        return palette_dict, markers_dict, hue_order

    def _create_custom_legend(self, pp):
        """
        Fully manual legend in its own inset Axes.
        - dynamic col spacing to avoid overlap
        - separate fixed text gap
        - proper rounded gray frame
        """
        fig = pp.fig

        # 1) remove Seaborn's built-in legend
        if getattr(pp, "_legend", None):
            pp._legend.remove()

        # 2) finalize draw so we can measure text widths
        fig.canvas.draw()
        renderer = fig.canvas.get_renderer()

        # 3) collect iteration numbers & colors
        all_iters = set()
        status_to_color = {
            "accepted": {},
            "discarded_likelihood": {},
            "discarded_oversampling": {},
            "discarded_iteration": {},
        }

        def _parse(gk):
            if gk.startswith("accepted__it="):
                return "accepted", int(gk.split("=")[1])
            for key in (
                "discarded_likelihood",
                "discarded_oversampling",
                "discarded_iteration",
            ):
                if gk.startswith(f"{key}__it="):
                    return key, int(gk.split("=")[1])
            return None, None

        for gk, col in self.palette_dict.items():
            st, it = _parse(gk)
            if st is not None:
                all_iters.add(it)
                status_to_color[st][it] = col

        iteration_list = sorted(all_iters)
        ncols = len(iteration_list) + 1  # +1 for the "Style" column

        # 4) determine which rows to draw
        rows = []
        if iteration_list:
            rows.append(("header", None))
        for st, marker in (
            ("accepted", "o"),
            ("discarded_likelihood", "o"),
            ("discarded_oversampling", "s"),
            ("discarded_iteration", "^"),
        ):
            if status_to_color[st]:
                rows.append((st, (self._get_status_label(st), marker)))
        nrows = len(rows)

        # 5) sizing constants (in points)
        fs = 8  # font‐size
        ms = 7  # marker‐size
        gap_text = 1.75 * fs  # gap before category label
        bdp = 0.5 * fs  # border‐pad inside frame
        col_gap = 0.3 * fs  # <-- updated col_gap
        row_ht = 1.9 * fs  # row height
        fp = FontProperties(size=fs)

        # 6) measure maximum width of any iteration‐label "i=k"
        max_i_w = 0.0
        for it in iteration_list:

            # 1) query the raw pixel‐width
            w_px, _, _ = renderer.get_text_width_height_descent(
                f"i={it}", fp, ismath=False
            )

            # 2) convert pixels → points (there are 72 points in an inch):
            w_pt = w_px * (72.0 / fig.dpi)

            max_i_w = max(max_i_w, w_pt)

        w_px, _, _ = renderer.get_text_width_height_descent("Style", fp, ismath=False)
        w_pt = w_px * (72.0 / fig.dpi)
        max_i_w = max(max_i_w, w_pt)

        # 7) dynamic column spacing: large enough for text or marker + 0.3em
        col_width = max(max_i_w, ms) + col_gap

        # 8) measure maximum category‐label width
        # max_lab_w = 0.0
        # for ridx, (st, info) in enumerate(rows):
        #     if st == "header":
        #         continue
        #     lab, _ = info
        #     w, _, _ = renderer.get_text_width_height_descent(lab, fp, ismath=False)
        #     max_lab_w = max(max_lab_w, w)

        max_lab_w = 0.0
        for ridx, (st, info) in enumerate(rows):
            if st == "header":
                continue
            lab, _ = info
            w_px, _, _ = renderer.get_text_width_height_descent(lab, fp, ismath=False)
            w_pt = w_px * (72.0 / fig.dpi)  # <-- convert pixels → points
            max_lab_w = max(max_lab_w, w_pt)

        # 9) total legend size in points
        total_w = 2 * bdp + ncols * col_width + gap_text + max_lab_w
        total_h = 2 * bdp + nrows * row_ht

        # 10) convert to figure fraction
        fig_w, fig_h = fig.get_size_inches()
        fw = total_w / (72 * fig_w)
        fh = total_h / (72 * fig_h)

        # anchor legend just above the top of the axes‐grid
        # left_f, right_f = fig.subplotpars.left, fig.subplotpars.right
        # cx = 0.5 * (left_f + right_f)
        # topt = fig.subplotpars.top  # ← use the top of the axes region
        # llx = cx - 0.5 * fw
        # lly = topt - fh

        # Draw figure

        pp.figure.canvas.draw()

        import matplotlib.transforms as mtrans

        # Find all active axes (not None)
        live_axes = [ax for row in pp.axes for ax in row if ax is not None]

        # Union their positions in figure-fraction units
        axes_box = mtrans.Bbox.union([ax.get_position() for ax in live_axes])

        # Compute horizontal center of actual plot grid
        cx = 0.5 * (axes_box.x0 + axes_box.x1)
        topy = axes_box.y1 + 0.01  # place slightly above the top

        # Legend lower-left corner:
        llx = cx - 0.5 * fw
        lly = topy

        # 12) inset axes for the legend
        ax = fig.add_axes([llx, lly, fw, fh])
        ax.axis("off")
        ax.set_facecolor("none")
        ax.patch.set_visible(False)

        # 14) precompute normalized positions
        def fx(pt):
            return pt / total_w

        centers = [fx(bdp + (i + 0.5) * col_width) for i in range(ncols)]
        style_c = centers[-1]
        label_x = fx(bdp + ncols * col_width + gap_text)
        ys = [(bdp + (nrows - i - 0.5) * row_ht) / total_h for i in range(nrows)]

        # 15) draw each row’s icons + text
        for ridx, (st, info) in enumerate(rows):
            y = ys[ridx]
            if st == "header":
                # iteration labels + “Style”
                for i, it in enumerate(iteration_list):
                    ax.text(
                        centers[i],
                        y,
                        f"i={it}",
                        ha="center",
                        va="center",
                        fontsize=fs,
                        transform=ax.transAxes,
                    )
                ax.text(
                    style_c,
                    y,
                    " Style",
                    ha="center",
                    va="center",
                    fontsize=fs,
                    transform=ax.transAxes,
                )
            else:
                lab, mk = info
                # a) colored markers
                for i, it in enumerate(iteration_list):
                    col = status_to_color[st].get(it, (1, 1, 1, 0))
                    ax.plot(
                        [centers[i]],
                        [y],
                        marker=mk,
                        markersize=ms,
                        markerfacecolor=col,
                        markeredgecolor=col,
                        linestyle="",
                        transform=ax.transAxes,
                    )
                # b) style line
                ls = self.kde_styles[st]["linestyle"]
                half = ms / total_w
                ax.plot(
                    [style_c - half, style_c + half],
                    [y, y],
                    linestyle=ls,
                    color=("tab:blue" if st == "accepted" else "gray"),
                    linewidth=1.5,
                    transform=ax.transAxes,
                )
                # c) category label
                ax.text(
                    label_x,
                    y,
                    lab,
                    ha="left",
                    va="center",
                    fontsize=fs + 1,
                    transform=ax.transAxes,
                )

        fig.canvas.draw()
        renderer = fig.canvas.get_renderer()

        #  17) collect all content bboxes (in *display* coords)
        bboxes = []
        for artist in ax.texts + ax.lines:
            try:
                bb = artist.get_window_extent(renderer)
                if bb.width > 0 and bb.height > 0:
                    bboxes.append(bb)
            except Exception:
                pass

        if bboxes:
            #  18) union them
            content_bb = Bbox.union(bboxes)

            #  19) pad in *points* → *pixels*
            pad_pts = bdp  # your borderpad in points
            pad_px = pad_pts * fig.dpi / 72.0
            pad_px = 0
            padded = Bbox.from_bounds(
                content_bb.x0 - pad_px,
                content_bb.y0 - pad_px,
                content_bb.width + 2 * pad_px,
                content_bb.height + 2 * pad_px,
            )

            #  20) convert to *figure* fractional coords
            inv_fig = fig.transFigure.inverted()
            (x0, y0), (x1, y1) = inv_fig.transform(
                [
                    (padded.x0, padded.y0),
                    (padded.x1, padded.y1),
                ]
            )
            w_fig, h_fig = x1 - x0, y1 - y0

            pad_frac = bdp / total_w

            #  21) draw the tight patch on the *figure*
            tight_box = FancyBboxPatch(
                (x0, y0),
                w_fig,
                h_fig,
                boxstyle=f"round,pad={pad_frac:.3f}",  # pad=0 because we already padded manually
                facecolor="white",
                edgecolor="0.6",
                linewidth=1.2,
                alpha=0.9,
                transform=fig.transFigure,
                zorder=-2,
            )
            fig.patches.append(tight_box)

            # Check if the bottom of the legend frame is below the top of the top row of subplots
            # If so, move the legend frame up above the top row of subplots, so it doesn't overlap.
            # It should be placed a bit above the top of the figure.

            margin_y = 0.025  # Adjust this value to control the distance from the top of the figure
            # calculate position of lower frame-edge
            lower_frame_edge = y0 - pad_frac
            topy = fig.subplotpars.top
            if lower_frame_edge < topy + margin_y:
                # Move the legend frame up and the tight_box up
                delta_y = topy + margin_y - lower_frame_edge
                # Move the legend frame up
                new_y0 = y0 + delta_y
                new_lly = lly + delta_y

                ax.set_position([llx, new_lly, fw, fh])
                tight_box.set_y(new_y0)

        def _legend_fits_upper_triangle(pp, fw, fh, margin=0.005):
            """
            Check whether a box of width=fw and height=fh (in figure-fraction units)
            will fit into the “empty” upper-right triangle slot of a PairGrid,
            and if so return (True, llx, lly) giving the lower-left corner
            in figure-fraction coords, flush against the right and top of that cell.

            pp       : your seaborn.PairGrid object
            fw, fh   : legend width & height in figure fraction (0–1)
            margin   : small gap from the subplot spines, also in figure fraction
            """
            fig = pp.fig
            # grab the diagonal cell in the top row:
            # pp.axes is an n×n array; the 0,0 entry is the upper-left corner cell.
            diag = pp.axes[0, 0].get_position()

            # horizontal free region runs from right edge of diag cell + margin
            # up to the right edge of the entire grid (subplotpars.right − margin)
            x0_free = diag.x1 + margin
            x1_free = fig.subplotpars.right - margin
            avail_width = x1_free - x0_free

            # vertical free region runs from just below the top of the figure
            # (subplotpars.top − margin) down to diag.x1 − fh minus our margin
            # (so that the legend box of height fh just sits inside that band)

            y0_free = diag.y0 + margin  # just above the bottom of the diag cell
            y1_free = fig.subplotpars.top - margin  # just below the top of the figure
            avail_height = y1_free - y0_free

            if (fw <= avail_width) and (fh <= avail_height):
                # flush it against the right and top of that free cell:
                llx = x1_free - fw
                lly = diag.y1 - fh - margin
                return True, llx, lly
            else:
                return False, None, None

        fits, tri_llx, tri_lly = _legend_fits_upper_triangle(
            pp, w_fig + pad_frac, h_fig + pad_frac, margin=0.005
        )
        # also check that countour plots are not drawn, because then it draws a colorbar in the corner
        if (
            fits
            and not self.plot_contours
            and self.preferred_legend_position == "upper right"
        ):
            sp = fig.subplotpars
            margin = 0
            # compute a lower‐left that *flushes right* at sp.right – margin
            new_x0 = sp.right - margin - w_fig - pad_frac
            new_y0 = sp.top - margin - h_fig - pad_frac

            shift_x = new_x0 - x0
            shift_y = new_y0 - y0

            new_llx = llx + shift_x
            new_lly = lly + shift_y

            # 2) move the inset‐axes
            ax.set_position([new_llx, new_lly, fw, fh])

            # 3) move the patch itself
            tight_box.set_x(new_x0)
            tight_box.set_y(new_y0)

        return None

    def _apply_custom_axis_labels(self, pp, x_vars, y_vars):
        """
        Label the bottom row with x_vars and the leftmost column with y_vars.
        Can handle both square corner‐plots (x_vars == y_vars) and rectangular grids.
        """
        # set "text.usetex" to True for LaTeX rendering
        plt.rcParams["text.usetex"] = True

        n_rows = len(y_vars)
        n_cols = len(x_vars)

        for i in range(n_rows):
            for j in range(n_cols):
                ax = pp.axes[i][j]
                if ax is None:
                    continue

                # Only label x on the bottom row
                if i == n_rows - 1:
                    name = x_vars[j]
                    label = (
                        self.param_labels.get(name, name) if self.param_labels else name
                    )
                    ax.set_xlabel(label, fontsize=11)

                # Only label y on the first column
                if j == 0:
                    name = y_vars[i]
                    label = (
                        self.param_labels.get(name, name) if self.param_labels else name
                    )
                    ax.set_ylabel(label, fontsize=11)

    def _determine_axis_ranges(self, iters_to_use):
        """
        Determine axis ranges based on relevant categories, optionally excluding iteration 0.

        Parameters
        ----------
        iters_to_use : list of int
            Iterations to include in the axis range calculation.

        Returns
        -------
        param_limits : dict
            Dictionary with parameter names as keys and (min, max) tuples as values.
        """
        relevant_categories = [
            "accepted_new",
            "discarded_iteration",
            "discarded_oversampling",
        ]
        if self.include_lkldiscard_new_for_axis_range:
            relevant_categories.append("discarded_likelihood_new")
        param_values = {}  # Dict of lists to collect values for each parameter

        if self.verbose >= 2:
            print(
                f"Gathering data for axis range calculation from categories: {relevant_categories}"
            )

        for it in iters_to_use:
            # Optionally skip iteration 0
            if self.ignore_iteration_0_for_axis and it == 0:
                if self.verbose >= 2:
                    print(f"Excluding iteration '{it}' from axis range calculation.")
                continue

            it_data = self.data["iteration_data"].get(it, {})

            for category in relevant_categories:
                df = it_data.get(category, {}).get("parameters", None)
                if isinstance(df, pd.DataFrame) and not df.empty:
                    for param in df.columns:
                        if param not in param_values:
                            param_values[param] = []
                        param_values[param].extend(df[param].values)
                        if self.verbose >= 3:
                            print(
                                f"Iteration '{it}', Category '{category}', Parameter '{param}': Adding {len(df)} points."
                            )

        # Compute min and max for each parameter
        param_limits = {}
        for param, values in param_values.items():
            if not values:
                if self.verbose >= 2:
                    print(f"No data found for parameter '{param}'. Skipping.")
                continue
            param_min = np.min(values)
            param_max = np.max(values)
            param_limits[param] = (param_min, param_max)
            if self.verbose >= 3:
                print(f"Parameter '{param}': min={param_min}, max={param_max}")

        if not param_limits:
            # If no data found, set default ranges
            param_limits = {}
            if self.verbose >= 2:
                print(
                    "No data found in relevant categories for axis range calculation. Using default ranges."
                )
            # Optionally, you could set default ranges here if desired

        else:
            if self.verbose >= 2:
                print(
                    "Calculated parameter axis ranges (excluding iteration 0 if requested):"
                )
                for p, (lo, hi) in param_limits.items():
                    print(f"  {p}: [{lo}, {hi}]")

        return param_limits

    def _get_status_label(self, status):
        """
        Helper method to convert status keys to readable labels.
        """
        label_map = {
            "accepted": "Accepted",
            "discarded_likelihood": "Discarded (likelihood)",
            "discarded_oversampling": "Discarded (oversampling)",
            "discarded_iteration": "Discarded (iteration)",
            # Add more mappings if necessary
        }
        return label_map.get(status, status.replace("_", " ").capitalize())

    def _customize_kde_styles(self, pp, hue_order):
        """
        Customize the KDE plot styles for different groups in the pairplot diagonal subplots.
        Additionally, create a line-style legend within the first diagonal plot.
        Specifically:
        - 'accepted'      => filled KDE with a solid-edge line
        - 'discarded_*'   => transparent fill and distinct dashed/dotted/dash-dot edges
            * discarded_likelihood => dashed
            * discarded_oversampling => dash-dot
            * discarded_iteration => dotted

        This function also adds a legend to the first diagonal plot indicating the line styles.
        """
        import matplotlib.collections as mc  # for PolyCollection

        # Desired styles mapped by status
        status_style = self.kde_styles

        # To keep track of active statuses
        active_statuses = set()

        # Iterate over each diagonal axis
        for ax in pp.diag_axes:
            if ax is None:
                continue  # corner=True might produce some None axes in upper triangle

            # Collect all PolyCollection KDEs in this axis
            poly_collections = [
                artist
                for artist in ax.collections
                if isinstance(artist, mc.PolyCollection)
            ]

            if self.verbose >= 2:
                print(
                    f"[DEBUG] Diagonal Axis: Found {len(poly_collections)} PolyCollections."
                )

            # Assume that the order of poly_collections matches the hue_order
            for idx, group_key in enumerate(hue_order):
                if idx >= len(poly_collections):
                    if self.verbose >= 2:
                        print(
                            f"[DEBUG] Diagonal Axis: Not enough PolyCollections for hue_order."
                        )
                    break  # More hue_order groups than PolyCollections

                poly = poly_collections[idx]

                # Extract the status from group_key
                # group_key like: "accepted__it=5" or "discarded_likelihood__it=3"
                status = group_key.split("__")[0]

                # Get style info
                style = status_style.get(status, {"fill": False, "linestyle": "-"})

                # Apply fill or make transparent
                if style["fill"]:
                    # Keep the original facecolor (from the palette)
                    pass
                else:
                    # Make fill transparent
                    poly.set_facecolor((1, 1, 1, 0))  # RGBA tuple with alpha=0

                # Set the edge linestyle
                poly.set_linestyle(style["linestyle"])

                # Ensure edge color remains as per palette
                assigned_color = self.palette_dict.get(group_key, "black")
                poly.set_edgecolor(assigned_color)

                # Ensure line width is visible
                poly.set_linewidth(2.5)

                if self.verbose >= 2:
                    print(
                        f"[DEBUG] Diagonal Axis: {group_key} => status={status}, "
                        f"fill={style['fill']}, linestyle={style['linestyle']}, color={assigned_color}"
                    )

                # Record the active status
                active_statuses.add(status)

        if self.verbose >= 2:
            print("[DEBUG] Completed customizing KDE styles (PolyCollection edges).")

        # Now, create the line-style legend within the first diagonal axis
        if active_statuses:
            first_diag_ax = pp.diag_axes[0]
            if first_diag_ax is not None:
                # Define line-style legend elements based on active_statuses
                legend_elements = []
                for status in sorted(active_statuses):
                    style = status_style.get(status, {"fill": False, "linestyle": "-"})
                    # Assign a representative color for the line in the legend
                    # Here, we can use a standard color like black or gray
                    if status.startswith("accepted"):
                        rep_color = "tab:blue"  # or choose another color if preferred
                    else:
                        rep_color = "gray"

                    label = self._get_status_label(status)

                    legend_elements.append(
                        Line2D(
                            [0],
                            [0],
                            color=rep_color,
                            linestyle=style["linestyle"],
                            lw=1.3,
                            label=label,
                        )
                    )

                # Remove any existing line-style legend in the first diagonal axis to prevent duplication
                handles, labels = first_diag_ax.get_legend_handles_labels()
                # Optionally, remove existing legend if any
                if first_diag_ax.get_legend() is not None:
                    first_diag_ax.legend_.remove()

                # # Add the new line-style legend
                # first_diag_ax.legend(
                #     handles=legend_elements,
                #     loc="upper left",  # Adjust as needed
                #     frameon=True,
                #     fontsize=7,
                #     title="Line Styles",
                #     title_fontsize=7,
                # )

    def _filter_data_types(self):
        """
        Filter self.df_long based on self.data_types_to_plot, which might look like:
            {
            'accepted': True,
            'discarded_likelihood': False,
            'discarded_oversampling': True,
            'discarded_iteration': True
            }
        This method:
        1) Parses the 'group_key' to identify the data type (accepted, discarded_likelihood, etc.)
        2) Drops rows for which data_types_to_plot[...] is False.
        """
        if self.df_long.empty:
            if self.verbose >= 2:
                print("[_filter_data_types] df_long is empty. Nothing to filter.")
            return

        if not self.data_types_to_plot:
            # If we have no dict, do nothing
            return

        def parse_data_type(group_key):
            """
            group_key might look like:
            - 'accepted__it=3'
            - 'discarded_likelihood__it=5'
            - 'discarded_oversampling__it=2'
            - 'discarded_iteration__it=0'
            We'll parse out the prefix before '__it='.
            """
            if group_key.startswith("accepted__it="):
                return "accepted"
            elif group_key.startswith("discarded_likelihood__it="):
                return "discarded_likelihood"
            elif group_key.startswith("discarded_oversampling__it="):
                return "discarded_oversampling"
            elif group_key.startswith("discarded_iteration__it="):
                return "discarded_iteration"
            else:
                # fallback, or if there's some unknown pattern
                return "unknown"

        # Build a mask that is True for rows we want to keep
        keep_mask = []
        for idx, row in self.df_long.iterrows():
            gk = row["group_key"]
            dt = parse_data_type(gk)
            # Check if dt is in self.data_types_to_plot keys
            if dt in self.data_types_to_plot:
                if self.data_types_to_plot[dt]:
                    keep_mask.append(True)
                else:
                    keep_mask.append(False)
            else:
                # If not recognized in data_types_to_plot, you could either keep or drop it
                # Let's keep by default
                keep_mask.append(True)

        self.df_long = self.df_long[keep_mask].copy()
        self.df_long.reset_index(drop=True, inplace=True)

        if self.verbose >= 2:
            print(
                "[_filter_data_types] Completed filtering based on data_types_to_plot."
            )
            print("Remaining group_key distribution:")
            print(self.df_long["group_key"].value_counts())

    def _downsample_df_long(self):
        """
        Downsamples the DataFrame `self.df_long` by randomly removing
        a fraction of data points per group, based on `self.downsampling_fraction`.
        """
        if not (0.0 <= self.downsampling_fraction < 1.0):
            raise ValueError(
                "downsampling_fraction must be between 0.0 and 1.0 (non-inclusive)"
            )

        if self.verbose >= 2:
            print(
                f"Applying downsampling: removing {self.downsampling_fraction * 100}% of data points per group."
            )

        # Group by 'group_key'
        grouped = self.df_long.groupby("group_key")

        # Apply sampling: keep (1 - downsampling_fraction) fraction of each group
        df_downsampled = grouped.apply(
            lambda x: (
                x.sample(frac=(1.0 - self.downsampling_fraction), random_state=42)
                if len(x) > 1
                else x
            )
        ).reset_index(drop=True)

        if self.verbose >= 2:
            print(f"DataFrame shape after downsampling: {df_downsampled.shape}")
            print(
                f"Group counts after downsampling:\n{df_downsampled['group_key'].value_counts()}"
            )

        self.df_long = df_downsampled

    ############################################################################
    # Contour-related methods
    ############################################################################

    def _plot_contours_on_pairplot(
        self, pp, x_vars, y_vars, last_complete_it, plot_mode
    ):
        """
        Main driver for computing and drawing Delta chi^2 contours
        in each pair of parameters sub-axes.
        """
        # Step A) Gather all data with param_X, param_Y, and 'true_loglkl' across relevant iteration_data
        #         in the same style as your PlotIterations._gather_all_likelihood_points
        xs_all, ys_all, loglkl_all, param_names = self._gather_all_likelihood_points()

        if xs_all is None:
            if self.verbose:
                print(
                    "[_plot_contours_on_pairplot] No global likelihood data found; skipping contours."
                )
            return

        # Step B) Get best fit log-lkl from last_complete_it
        best_loglkl = self._get_best_fit_loglkl(last_complete_it)
        if best_loglkl is None:
            if self.verbose:
                print(
                    "[_plot_contours_on_pairplot] No best_fit loglkl found; skipping contours."
                )
            return

        self._delta_chi2_all = 2.0 * (loglkl_all - best_loglkl)

        # D) Create a single PercentileNorm for the entire dataset
        #    (all subplots will share the same normalization)
        self.global_norm = PercentileNorm(
            all_dchi2=self._delta_chi2_all,
            n_discretize_cmap=self.n_discretize_cmap,
            clip=True,
        )

        normed_boundaries = np.linspace(0, 1, self.n_discretize_cmap + 1)
        data_boundaries = self.global_norm.inverse(normed_boundaries)

        fig = pp.fig
        axes = pp.axes
        n_rows = len(y_vars)
        n_cols = len(x_vars)

        # We'll keep track of the first QuadContourSet for a single colorbar
        self.first_contour_set = None

        # Loop over the lower-triangle subplots (for corner=True) or all subplots (corner=False)
        for i in range(n_rows):
            for j in range(n_cols):
                ax_ij = axes[i, j]
                # If corner=True, the upper triangle is None.  If not corner,
                # we might want to skip i < j or i>j.  Typically, the main data is in i>=j if corner=True.
                if ax_ij is None:
                    continue
                if i == j:
                    continue  # skip diagonal

                param_y = y_vars[i]
                param_x = x_vars[j]

                # 1) Extract the subset that has param_x and param_y
                if param_x not in param_names or param_y not in param_names:
                    # If we didn't gather loglkl for this param, skip
                    continue

                # On-the-fly approach:
                # xvals, yvals, dchi2_vals = self._extract_2d_slice(param_x, param_y)
                xvals, yvals, global_indices = self._extract_2d_slice(param_x, param_y)

                if xvals.size < 5:
                    # Not enough points to do a decent interpolation
                    continue

                """
                # 3) Interpolate onto a grid
                Xi, Yi, Zi = self._compute_and_interpolate_delta_chi2(
                    xvals, yvals, dchi2_vals,
                    grid_size=self.grid_size,
                    method=self.interpolation_method,
                    sigma=self.sigma
                )
                """

                Xi, Yi, Zi = self._interpolate_dchi2_grid(
                    xvals,
                    yvals,
                    global_indices,
                    grid_size=self.grid_size,
                    method=self.interpolation_method,
                    sigma=self.sigma,
                )

                # 4) Create the discrete or continuous colormap
                if self.discretize_cmap:
                    base_cmap = plt.get_cmap(self.cmap_contour)
                    colors = base_cmap(np.linspace(0, 1, self.n_discretize_cmap))
                    cmap_used = cm.colors.ListedColormap(colors)
                else:
                    cmap_used = plt.get_cmap(self.cmap_contour)

                # 6) Filled contours
                contourf = ax_ij.contourf(
                    Xi,
                    Yi,
                    Zi,
                    levels=data_boundaries,
                    cmap=cmap_used,
                    alpha=self.alpha_contourf,
                    norm=self.global_norm,
                    zorder=0,
                )

                # 5) Optionally add contour lines
                if self.contour_line_overlay:
                    ax_ij.contour(
                        Xi,
                        Yi,
                        Zi,
                        levels=data_boundaries,
                        cmap=cmap_used,
                        alpha=self.alpha_contour,
                        norm=self.global_norm,
                        linewidths=self.contour_linewidths,
                        zorder=0,
                    )

                # 8) Store the first contour set for the colorbar
                if self.first_contour_set is None:
                    self.first_contour_set = contourf

                # *** NEW: Plot the delta_chi2 threshold line ***
                # (This only happens if a threshold is provided and the toggle is True.)

                if isinstance(self.delta_chi2_threshold, list):
                    if self.iterations_to_plot != "all":
                        iter = max(self.iterations_to_plot)
                    else:
                        iter = last_complete_it
                    if iter < len(self.delta_chi2_threshold):
                        delta_chi2_threshold = self.delta_chi2_threshold[iter]
                    else:
                        delta_chi2_threshold = self.delta_chi2_threshold[-1]
                elif isinstance(self.delta_chi2_threshold, (int, float)):
                    delta_chi2_threshold = self.delta_chi2_threshold

                if delta_chi2_threshold is not None and self.plot_threshold_line:
                    ax_ij.contour(
                        Xi,
                        Yi,
                        Zi,
                        levels=[delta_chi2_threshold],
                        colors=self.delta_chi2_threshold_color,
                        linewidths=self.delta_chi2_threshold_linewidth,
                        linestyles="-",
                        zorder=10000,
                    )
                    # Add a legend for the threshold line
                    # Create a custom legend handle (a Line2D) that looks like the threshold line
                    # threshold_handle = Line2D(
                    #     [0],
                    #     [0],
                    #     color=self.delta_chi2_threshold_color,
                    #     lw=self.delta_chi2_threshold_linewidth,
                    #     linestyle="-",
                    # )
                    # # Use this handle for the legend; note that you can format the label as desired.
                    # label = r"$\Delta \chi^2{\rm -thres} = %s$" % delta_chi2_threshold
                    # ax_ij.legend(
                    #     [threshold_handle],
                    #     [label],
                    #     loc="upper left",
                    #     fontsize=7,
                    #     frameon=False,
                    # )

        # Step G) Create a single colorbar if we have at least one contour
        if self.first_contour_set is not None:

            if plot_mode == "triangle":
                # cbar_ax = fig.add_axes(
                #     [
                #         1 - n_rows / 100,
                #         n_rows / 100 + 0.02,
                #         0.02,
                #         (2 * n_rows - 3) / (2 * n_rows) * 0.9,
                #     ]
                # )  # [left, bottom, width, height]

                pad = 0.01  # small gap in figure‐fraction units
                bar_width = 0.02  # 2% of figure width

                # bottom‐right Axes in your PairGrid
                br_ax = pp.axes[-1, -1]
                bb = br_ax.get_position()  # Bbox(x0,y0,x1,y1)

                # get the vertical span of the *entire* grid:
                bottom_y = min(ax.get_position().y1 for ax in pp.axes[-1, :] if ax)
                top_y = max(ax.get_position().y1 for ax in pp.axes[0, :] if ax)

                # now build the cbar axes so that its right edge is at bb.x1 - pad
                left = bb.x1 - pad - bar_width
                bottom = bottom_y + pad
                height = (top_y - bottom_y) - 2 * pad

                cbar_ax = fig.add_axes([left, bottom, bar_width, height])

            elif plot_mode == "matrix":
                # full-height bar on the right of the grid
                pad = 0.01  # gap between grid and colorbar
                bar_width = 0.02  # 2% of figure width
                left = fig.subplotpars.right + pad  # right edge of subplots + pad
                bottom = fig.subplotpars.bottom
                height = fig.subplotpars.top - bottom

                cbar_ax = fig.add_axes([left, bottom, bar_width, height])

            cbar = fig.colorbar(
                self.first_contour_set,
                cax=cbar_ax,
                orientation="vertical",
                label=r"$\Delta \chi^2$",
                boundaries=data_boundaries,  # if self.discretize_cmap else None,
                format=FuncFormatter(self.custom_tick_formatter),
                spacing="uniform",
            )

            # ————— add this block —————
            # only if we have exactly one threshold value:
            if self.plot_threshold_line:
                th = delta_chi2_threshold

                # draw a horizontal line at y=th in data‐coords
                cbar.ax.hlines(
                    th,
                    xmin=0,
                    xmax=1,  # full width of the colorbar
                    color=self.delta_chi2_threshold_color,
                    linewidth=self.delta_chi2_threshold_linewidth,
                    linestyle="-",
                    label=f"$\Delta\chi^2={th}$",
                    zorder=11,
                    clip_on=False,
                )
                # and drop a little text next to it:
                # now place the text so its RIGHT edge sits at x=0 (the left side of the bar)
                if plot_mode == "triangle":
                    cbar.ax.text(
                        -0.15,  # 0.0 is the very left edge; -0.01 pushes it 1% further left
                        th,
                        rf"thres={th}",
                        va="center",
                        ha="right",  # right‐align the text to that x‐pos
                        transform=cbar.ax.get_yaxis_transform(),
                        fontsize=7,
                        zorder=11,
                        clip_on=False,
                    )
                else:
                    # cbar.ax.text(
                    #     3.5,  # 3.0 is the very right edge; 3.01 pushes it 1% further right
                    #     th,
                    #     rf"$\Delta\chi^2={th}$",
                    #     va="center",
                    #     ha="left",  # left-align the text to that x-position
                    #     transform=cbar.ax.get_yaxis_transform(),
                    #     fontsize=7,
                    #     zorder=11,
                    #     clip_on=False,
                    # )
                    # skip - do nothing:
                    pass

            cbar.set_ticks(data_boundaries)
            cbar.ax.yaxis.set_major_formatter(FuncFormatter(self.custom_tick_formatter))

    def _gather_all_likelihood_points(self):
        """
        Collect all points across all iteration_data blocks that have:
           - parameters DataFrame with columns [paramX, paramY, ...]
           - likelihood_data DataFrame with column 'true_loglkl'
        For maximum generality, store them in a structure keyed by parameter name.

        Returns
        -------
        big_df['true_loglkl'] : numpy array
        """
        # For simplicity, we’ll gather ALL param columns from all blocks
        # and store them in a single big DataFrame with 'true_loglkl'.
        # Then we can slice it by param name later.

        records = []
        iteration_dict = self.data.get("iteration_data", {})
        for it, it_data in iteration_dict.items():
            # For each iteration, consider relevant categories that might have "likelihood_data"
            for cat_name in (
                "accepted_new",
                "accepted_old",
                "accepted_accumulated",
                "discarded_iteration",
                "discarded_oversampling",
                "discarded_likelihood",
                "discarded_likelihood_new",
                "discarded_likelihood_old",
            ):
                block = it_data.get(cat_name, {})
                df_params = block.get("parameters", None)
                df_lkl = block.get("likelihood_data", None)
                if (
                    isinstance(df_params, pd.DataFrame)
                    and not df_params.empty
                    and isinstance(df_lkl, pd.DataFrame)
                    and not df_lkl.empty
                    and "true_loglkl" in df_lkl.columns
                    and len(df_params) == len(df_lkl)
                ):
                    # Merge them row-by-row
                    merged_df = df_params.copy()
                    merged_df["true_loglkl"] = df_lkl["true_loglkl"].values
                    records.append(merged_df)

        if not records:
            return None, None, None, set()

        big_df = pd.concat(records, ignore_index=True)

        if "true_loglkl" not in big_df.columns or big_df["true_loglkl"].empty:
            return None, None, None, set()

        if self.pickle_contour_data is not None:
            pickle_df = self.pickle_contour_data
            big_df = pd.concat([big_df, pickle_df], ignore_index=True)

        # We'll simply store it as an attribute so we can do subsetting later:
        self.likelihood_big_df = big_df  # dataframe with all parameters and true_loglkl

        if self.save_params_loglkls:
            os.makedirs(self.output_folder, exist_ok=True)
            os.makedirs(os.path.join(self.output_folder, "pairplots"), exist_ok=True)
            pickle_filename = f"triangle_plot_contour_data.pickle"
            save_path = os.path.join(self.output_folder, "pairplots", pickle_filename)

            with open(save_path, "wb") as f:
                pickle.dump(self.big_df, f)

        param_names = set(big_df.columns) - {"true_loglkl"}

        # We do not actually return xs_all, ys_all here (like in PlotIterations),
        # but rather placeholders. We’ll do the slicing for each param pair in _extract_2d_slice
        # Return something so the caller knows we succeeded:
        return 0, 0, big_df["true_loglkl"].values, param_names

    def _extract_2d_slice(self, param_x, param_y):
        """
        From self.likelihood_big_df, extract columns param_x, param_y, and 'true_loglkl'.
        Return them as numpy arrays (xvals, yvals, delta_chi2).
        The delta_chi2 is computed outside, but we can store it inside as well.
        Simpler is to store loglkl and do the difference in the main routine.
        For this snippet, let's store only param columns and let the caller have dchi2 array already.

        We'll let the calling function do: dchi2_vals = 2*(loglkl_all - best_loglkl).
        But that means we need to do an inner join on the big df's true_loglkl.

        Actually, to keep it consistent, let's do the full approach: we do a new column "dchi2 = 2*(loglkl - best_lkl)"?
        That requires best_lkl though.  We'll keep it simpler: we just return xvals, yvals, and
        we re-map them row-by-row with the same ordering as big_df. Then the calling function
        can subset these rows. In this example, we do the simpler approach:
        We'll do a mask = [these columns exist]. We'll actually do it straightforwardly:
        """
        df = self.likelihood_big_df
        # filter out any row that has NaN in param_x or param_y
        mask = df[[param_x, param_y, "true_loglkl"]].notna().all(axis=1)
        sub_df = df.loc[mask, [param_x, param_y, "true_loglkl"]].copy()
        if sub_df.empty:
            return np.array([]), np.array([]), np.array([])

        xvals = sub_df[param_x].values
        yvals = sub_df[param_y].values

        global_indices = sub_df.index.to_numpy()
        # We'll store them in a class variable so we can re-fetch them in the calling code:
        return xvals, yvals, global_indices

    def _interpolate_dchi2_grid(
        self, xvals, yvals, global_indices, grid_size=200, method="linear", sigma=5
    ):
        """
        Using self._delta_chi^2_all, build Xi, Yi, Zi for contour plotting.
        """
        if self._delta_chi2_all is None:
            return None, None, None

        # local dchi2 from the global indices
        local_dchi2 = self._delta_chi2_all[global_indices]

        # 1) Build a regular grid
        x_min, x_max = np.min(xvals), np.max(xvals)
        y_min, y_max = np.min(yvals), np.max(yvals)
        Xi = np.linspace(x_min, x_max, grid_size)
        Yi = np.linspace(y_min, y_max, grid_size)
        Xi, Yi = np.meshgrid(Xi, Yi)

        # 2) Interpolate
        Zi = griddata((xvals, yvals), local_dchi2, (Xi, Yi), method=method)
        # 3) Smooth
        if sigma > 0:
            Zi = gaussian_filter(Zi, sigma=sigma)

        return Xi, Yi, Zi

    def _get_best_fit_loglkl(self, iteration):
        """
        Retrieve best_fit['likelihood']['loglkl'] for iteration if available.
        """
        it_dict = self.data["iteration_data"].get(iteration, {})
        bf_block = it_dict.get("best_fit", {})
        lk_df = bf_block.get("likelihood", None)
        if (
            isinstance(lk_df, pd.DataFrame)
            and not lk_df.empty
            and "loglkl" in lk_df.columns
        ):
            return lk_df["loglkl"].iloc[0]
        return None

    def custom_tick_formatter(self, x, pos):
        """
        Formats the tick labels:
        - Uses standard notation for |x| < 1000
        - Uses scientific notation for |x| >= 1000
        """
        if abs(x) >= 10000:
            return f"{x:.1e}"
        else:
            return f"{x:.0f}"

    class IterationLabelHandler(HandlerBase):
        """
        Draws a row of text labels for iteration numbers (e.g., "i=0  i=1  ...")
        in a single legend entry.
        """

        def __init__(self, iterations, spacing=1.0, fontsize=None):
            super().__init__()
            self.iterations = iterations
            self.spacing = spacing
            self.fontsize = fontsize

        def create_artists(
            self,
            legend,
            orig_handle,
            xdescent,
            ydescent,
            width,
            height,
            fontsize,
            trans,
        ):
            artists = []
            if self.fontsize is not None:
                fontsize = self.fontsize

            n_iter = len(self.iterations)
            if n_iter == 0:
                return artists
            n_cols = n_iter + 1  # +1 for the extra "line-style" label

            dx = (width / n_cols) * self.spacing
            center_y = (height / 2) - ydescent

            # --- Now: loop over actual iterations, not over col ---
            for idx, it_number in enumerate(self.iterations):
                x = xdescent + idx * dx + dx / 2
                label_str = f"i={it_number}"
                text = mtext.Text(
                    x,
                    center_y,
                    label_str,
                    ha="center",
                    va="center",
                    fontsize=fontsize,
                    transform=trans,
                )
                artists.append(text)

            # Add the extra "line-style" label at the last column
            x = xdescent + (n_cols - 1) * dx + dx / 2
            text = mtext.Text(
                x,
                center_y,
                " Style",
                ha="center",
                va="center",
                fontsize=fontsize,
                transform=trans,
            )
            artists.append(text)

            return artists

        # def get_xwidth(self, renderer):
        #     """
        #     Return the actual total width (in points) that this custom handle occupies.
        #     Uses Matplotlib's renderer to compute precise text width instead of assuming proportions.

        #     Parameters
        #     ----------
        #     renderer : RendererBase
        #         The Matplotlib renderer used to compute the text width.

        #     Returns
        #     -------
        #     float
        #         The total width of the iteration labels in points.
        #     """
        #     from matplotlib.font_manager import FontProperties  # Ensure import is here

        #     n = len(self.iterations)
        #     if n == 0 or self.fontsize is None:
        #         return 0
        #     n_cols = n + 1  # +1 for the extra "line-style" label

        #     # Define font properties based on legend font size
        #     font_properties = FontProperties(size=self.fontsize)

        #     # Measure the total width of all labels
        #     total_width = 0.0
        #     for it_number in self.iterations:
        #         label_str = f"i={it_number}"
        #         text_width, _, _ = renderer.get_text_width_height_descent(
        #             label_str, font_properties, ismath=False
        #         )
        #         total_width += text_width

        #     # Account for spacing between labels (spacing factor may need adjustment)
        #     spacing = 0.3 * self.fontsize  # Adjust if necessary
        #     total_width += (n - 1) * spacing  # Add spacing between iteration labels

        #     # --- add width of the extra 'line-style' label ------------------------  ← NEW
        #     extra_str = "line-style"
        #     text_width_extra, _, _ = renderer.get_text_width_height_descent(
        #         extra_str, font_properties, ismath=False
        #     )
        #     total_width += spacing + text_width_extra

        #     return total_width

        def get_xwidth(self, renderer):
            """
            Compute the minimal width (in points) so that, when
            create_artists uses dx = (width/n_cols)*spacing, no text
            will spill outside [0, width]. Returns that width.
            """
            from matplotlib.font_manager import FontProperties

            n = len(self.iterations)
            if n == 0 or self.fontsize is None:
                return 0.0

            # total columns = one column per iteration + one for "Style"
            n_cols = n + 1

            # how many points is one "em"?
            em_pts = self.fontsize

            # measure all "i=k" labels:
            fontp = FontProperties(size=self.fontsize)
            w_iter = []
            for k in self.iterations:
                txt = f"i={k}"
                w, _, _ = renderer.get_text_width_height_descent(
                    txt, fontp, ismath=False
                )
                w_iter.append(w)

            # measure the "Style" label:
            w_style, _, _ = renderer.get_text_width_height_descent(
                "Style", fontp, ismath=False
            )

            # spacing in points
            spacing_pts = self.spacing * em_pts

            # 1) Ensure the **first** iteration label doesn't run off the left:
            #    0.5 * dx >= w_iter[0]/2  ⇒  dx >= max(w_iter)
            #    dx = spacing_pts * (W / n_cols) / em_pts  [but since spacing is em-based,
            #          dx = (W/n_cols)*spacing  directly]
            #    => W >= n_cols * max(w_iter) / spacing
            W1 = 0.0
            if w_iter:
                W1 = n_cols * max(w_iter) / self.spacing

            # 2) Ensure the **Style** label doesn't run off the right:
            #    center_style + w_style/2 <= W
            #    center_style = (n + 0.5)*dx
            #    dx = (W/n_cols)*spacing
            #    ⇒ W*(1 - spacing*(n+0.5)/n_cols) >= w_style/2
            denom = 1.0 - self.spacing * (n + 0.5) / n_cols
            if denom <= 0:
                raise ValueError(
                    f"spacing={self.spacing:.2f} is too large for {n} iterations."
                )
            W2 = (w_style / 2.0) / denom

            # Finally, the minimal width that satisfies both:
            return max(W1, W2)

        def get_width(self, legend, renderer):
            """
            Return the exact width (in points) that the iteration-label row needs.
            We already computed that in `get_xwidth`, so just reuse it here.
            Matplotlib will then multiply this by `handlelength` (which we will
            keep at the default value 1.0).
            """
            return self.get_xwidth(renderer)

    class MultiColorHandler(HandlerBase):
        """
        A legend handler that draws multiple markers in a single row,
        each with potentially different colors (including invisible placeholders).
        All markers share the same shape, but differ in color/alpha.

        If you pass in a 'placeholder' color like (1,1,1,0) for alpha=0,
        it won't be visible, but it will still occupy space
        so that columns align properly across rows.
        """

        def __init__(
            self,
            colors,
            marker="o",
            markersize=8,
            spacing=1.0,
            line_style=None,
            line_color="black",
        ):
            """
            Parameters
            ----------
            colors : list
                List of colors/RGBA values (e.g. ['red', 'blue', (0.8,0.8,0.8,1.0), ...]).
                Use (1,1,1,0) for an invisible placeholder (alpha=0).
            marker : str
                Matplotlib marker style (e.g. 'o', 's', '^', etc.)
            markersize : float
                Size of the marker in points.
            spacing : float
                Horizontal spacing factor (bigger => more gap).
            line_style : str, optional
                Style of the line (e.g. '-', '--', '-.', etc.). Default is None.
            line_color : str, optional
                Color of the line. Default is "black".
            """
            super().__init__()
            self.colors = colors
            self.marker = marker
            self.markersize = markersize
            self.spacing = spacing
            self.line_style = line_style
            self.line_color = line_color

        def create_artists(
            self,
            legend,
            orig_handle,
            xdescent,
            ydescent,
            width,
            height,
            fontsize,
            trans,
        ):
            artists = []
            n = len(self.colors)
            if n == 0:
                return artists
            n_cols = n + (1 if self.line_style else 0)

            dx = (width / n_cols) * self.spacing
            center_y = (height / 2) - ydescent

            for i, c in enumerate(self.colors):
                x = xdescent + i * dx + dx / 2
                artist = mlines.Line2D(
                    [x],
                    [center_y],
                    marker=self.marker,
                    color=c,  # sets line/edge color
                    markeredgecolor=c,
                    markerfacecolor=c,
                    markersize=self.markersize,
                    linestyle="",
                    transform=trans,
                )
                artists.append(artist)

            # -- optional line column (the new “extra column”) --
            if self.line_style is not None:
                x = xdescent + (n_cols - 1) * dx + dx / 2
                half_len = self.markersize * 0.9
                artists.append(
                    mlines.Line2D(
                        [x - half_len, x + half_len],
                        [center_y, center_y],
                        linestyle=self.line_style,
                        color=self.line_color,
                        linewidth=1.5,
                        transform=trans,
                    )
                )

            return artists

        def get_xwidth(self, legend):
            """
            Tell Matplotlib how wide this handle is, in "em" units,
            so the legend can place the label correctly.

            Calculated based on the number of markers and spacing.
            """
            n = len(self.colors)
            n_cols = n + (1 if self.line_style else 0)
            # Define how much 'em' each marker and spacing consumes
            # Adjust these values based on your specific needs
            per_marker_em = 0.6  # Approximate width per marker
            per_spacing_em = 0.3  # Approximate width per spacing

            extra_line_em = 0.8 if self.line_style else 0.0  # NEW

            return per_marker_em * n + per_spacing_em * (n_cols - 1)

        def get_width(self, legend, renderer):
            """
            Return how many points wide the handle is. Matplotlib multiplies this
            by `handlelength`. If we return W, the total allocated is
            handlelength * W points.
            """
            n = len(self.colors)
            if n == 0:
                return 0

            n_cols = n + (1 if self.line_style else 0)

            # Let's say each marker is about `markersize` points wide,
            # plus we add some fraction for the spacing:
            # This is just an approximation to ensure there's enough space.
            per_marker_pts = self.markersize * 1.0
            per_spacing_pts = self.markersize * self.spacing
            extra_line_pts = self.markersize * 1.2 if self.line_style else 0.0

            return n_cols * per_marker_pts + (n_cols - 1) * per_spacing_pts


# ---------- Helper classes ----------------------
class PercentileNorm(Normalize):
    """
    Normalize data based on percentile boundaries.

    This normalization ensures that each color block in the colormap corresponds
    to an equal portion of the data distribution, based on the specified number
    of discretization bins.

    Parameters
    ----------
    all_dchi2 : array-like
        Array of delta chi-square values from which percentiles are computed.
    n_discretize_cmap : int
        Number of discrete color blocks to create based on percentiles.
    clip : bool, optional
        Whether to clip values outside the [vmin, vmax] range. Default is False.
    """

    def __init__(self, all_dchi2, n_discretize_cmap, clip=False):
        # Convert input to a NumPy array to handle operations
        all_dchi2 = np.asarray(all_dchi2, dtype=np.float64)  # Use high precision

        # Filter out NaN values for percentile calculation
        valid_dchi2 = all_dchi2[~np.isnan(all_dchi2)]

        # Compute percentile boundaries from non-NaN values
        self.n_discretize_cmap = n_discretize_cmap
        self.boundaries = np.percentile(
            valid_dchi2, np.linspace(0, 100, n_discretize_cmap + 1, dtype=np.float64)
        )

        # Increase decimal precision
        # precision = 10  # Adjust as needed
        # self.boundaries = np.around(self.boundaries, decimals=precision)

        # Add small perturbation to ensure monotonicity
        epsilon = 1e-10
        for i in range(1, len(self.boundaries)):
            if self.boundaries[i] <= self.boundaries[i - 1]:
                self.boundaries[i] = self.boundaries[i - 1] + epsilon

        # Remove duplicates to ensure strictly increasing values
        # self.boundaries = np.unique(self.boundaries)

        # Check if boundaries are valid
        if len(self.boundaries) < 2:
            raise ValueError(
                "PercentileNorm boundaries must contain at least two unique values."
            )

        # Set vmin and vmax based on boundaries
        vmin = self.boundaries[0]
        vmax = self.boundaries[-1]
        super().__init__(vmin=vmin, vmax=vmax, clip=clip)

    def __call__(self, value, clip=None):
        """
        Normalize the data based on percentile boundaries.

        Parameters
        ----------
        value : array-like
            Data values to normalize.
        clip : bool, optional
            Whether to clip values outside the [vmin, vmax] range.

        Returns
        -------
        normed : array-like
            Normalized data in the range [0, 1].
        """
        value = np.asarray(value, dtype=np.float64)  # Ensure high precision
        if self.vmin is None or self.vmax is None:
            raise ValueError("vmin and vmax must be set.")

        if clip is None:
            clip = self.clip

        # Initialize masked array
        result = np.ma.masked_array(np.empty_like(value, dtype=float))

        # Clip values if required
        if clip:
            value = np.clip(value, self.vmin, self.vmax)

        # Digitize the values into bins
        bin_indices = np.digitize(value, self.boundaries, right=True) - 1
        # That yields possible bin indices 0 ... 15 (for 16 boundaries).
        bin_indices = np.clip(bin_indices, 0, self.n_discretize_cmap)
        # Note: the upper clip is now `n_discretize_cmap` not `n_discretize_cmap - 1`
        result = bin_indices / self.n_discretize_cmap

        # Mask NaN values in the input
        result = np.ma.masked_where(np.isnan(value), result)

        return result

    def inverse(self, value):
        """
        Inverse map from normalized data [0, 1] to data space based on percentile boundaries.

        Parameters
        ----------
        value : array-like
            Normalized data in the range [0, 1].

        Returns
        -------
        inv : array-like
            Data values corresponding to the normalized input.
        """
        value = np.asarray(value, dtype=np.float64)  # Ensure high precision

        # Mask NaN values in the input
        result = np.ma.masked_array(np.empty_like(value, dtype=float))
        mask_nan = np.isnan(value)

        # Ensure value is within [0,1]
        value = np.clip(value, 0, 1)

        # Determine the bin index
        bin_indices = np.floor(value * self.n_discretize_cmap).astype(int)
        # Now bin index can go up to `n_discretize_cmap` when value=1.0
        bin_indices = np.clip(bin_indices, 0, self.n_discretize_cmap)
        result = self.boundaries[bin_indices]

        # Restore NaN values in the result
        result[mask_nan] = np.nan

        return result


# Parameters() class from CONNECT (likelihood-filter CONNECT)
# It is used to read the param file from the project folder (the file used to specify all the parameters for the training and sampling) and use its variable names.
# You might need to update this class below, if CONNECTs Parameters() class has been updated.
class Parameters:
    def __init__(self, param_file):
        param = SourceFileLoader(param_file, param_file).load_module()
        jobname = param_file.split("/")[-1].split(".")[0]

        self.param_file = param_file

        default = {
            ### Training parameters
            "train_ratio": (0.9, float),
            "val_ratio": (0.01, float),
            "epochs": (200, int),
            "batchsize": (64, int),
            "N_hidden_layers": (4, int),
            "N_nodes": (512, int),
            "loss_function": ("cosmic_variance", str),
            "activation_function": ("alsing", str),
            "normalisation_method": ("standardisation", str),
            ### Sampling parameters
            "parameters": ({}, dict),
            "extra_input": ({}, dict),
            "output_Cl": (["tt"], list),
            "output_Pk": ([], list),
            "k_grid": (self.get_k_grid(param), list),
            "z_Pk_list": ([0.0], list),
            "output_bg": ([], list),
            "z_bg_list": ([], list),
            "output_th": ([], list),
            "z_th_list": ([], list),
            "extra_output": ({}, dict),
            "output_derived": ([], list),
            "N": (10000, int),
            "sampling": ("lhc", str),
            ### Additional parameters for iterative sampling
            "N_max_points": (10000, int),
            "mcmc_sampler": ("cobaya", str),
            "initial_model": (None, str),
            "initial_sampling": ("hypersphere", str),
            "mcmc_tol": (0.01, float),
            "iter_tol": (0.1, float),
            "temperature": (5.0, (float, list)),
            "sampling_likelihoods": (["Planck_lite"], list),
            "prior_ranges": ({}, dict),
            "bestfit_guesses": ({}, dict),
            "sigma_guesses": ({}, dict),
            "log_priors": ([], list),
            "keep_first_iteration": (True, bool),
            "resume_iterations": (False, bool),
            "extra_cobaya_lkls": ({}, dict),
            ### Parameters for the likelihood filter
            "use_likelihood_filter": (True, bool),
            "delta_chi2_threshold": (9, float),
            "min_points_to_keep": (5000, int),
            "min_points_to_keep_initial": (10000, int),
            "nuisance_params_lkl": ({"A_planck": 1.0}, dict),
            "keep_initial_data": (True, bool),
            "discard_worst_first": (False, bool),
            ### Additional parameters for other kinds of sampling
            "hypersphere_surface": (False, bool),
            "hypersphere_covmat": (None, str),
            "pickle_data_file": (None, str),
            ### Saving parameters
            "jobname": (jobname, str),
            "save_name": (None, str),
            "overwrite_model": (False, bool),
            ### Memory overload parameters
            "memory_safe": (False, bool),
        }

        for key, val in default.items():
            setattr(self, key, getattr(param, key, val[0]))
        self.error_handling(param, default)

        self.adjust_defaults_based_on_likelihood_filter()

    def get_k_grid(self, param):
        if hasattr(param, "output_Pk") and len(param.output_Pk) > 0:
            k_grid = np.sort(
                np.concatenate(
                    [
                        np.logspace(np.log10(5e-6), np.log10(5e-5), 15),
                        np.logspace(np.log10(1e-4), np.log10(0.008), 8),
                        np.logspace(np.log10(0.76), np.log10(5), 17),
                        np.logspace(np.log10(0.009), np.log10(0.75), 60),
                    ]
                )
            )
            k_grid *= 0.67556  # value of h in LCDM
            return k_grid
        return None

    def try_cast_to_native_type(self, value, target_type):
        try:
            return target_type(value)
        except:
            pass

    def error_handling(self, param, default):
        sep = "\n" + "    " * 2 + "- "
        name_errors = self.get_name_errors(param, default)
        type_errors = self.get_type_errors(param, default)
        input_error = "Error with inputs in parameter file"
        if len(type_errors) > 1 and len(name_errors) > 1:
            input_errors = (
                input_error
                + "\n"
                + sep.join(name_errors)
                + "\n"
                + sep.join(type_errors)
            )
            raise Exception(input_errors)
        elif len(name_errors) > 1:
            name_errors = input_error + "\n" + sep.join(name_errors)
            raise Exception(name_errors)
        elif len(type_errors) > 1:
            type_errors = input_error + "\n" + sep.join(type_errors)
            raise Exception(type_errors)
        if not self.parameters:
            raise ValueError(
                "No set of parameters were given. Please provide a dictionary with lower and upper bounds."
            )

    def get_name_errors(self, param, default):
        name_errors = ["NameErrors:"]
        for name in [
            n for n in dir(param) if not n.startswith("__") and not n.endswith("__")
        ]:
            if not name in default:
                matches = dl.get_close_matches(name, default.keys())
                if len(matches) == 0:
                    name_error = f"'{name}' is not a recognised parameter and no close matches exist."
                else:
                    name_error = f"'{name}' is not a recognised parameter. Did you mean '{matches[0]}'?"
                    input_val = getattr(param, name)
                    input_type = type(input_val)
                    target_type = default[matches[0]][1]
                    if type(target_type) is tuple:
                        type_bool = input_type not in target_type
                    else:
                        type_bool = input_type != target_type
                    if type_bool:
                        if target_type == bool:
                            name_error += f" The type should then be 'bool'."
                        elif self.try_cast_to_native_type(
                            input_val, target_type
                        ) == None or (
                            target_type in [list, tuple, set] and input_type == str
                        ):
                            name_error += (
                                f" The type should then be '{target_type.__name__}'."
                            )
                name_errors.append(name_error)
        return name_errors

    def get_type_errors(self, param, default):
        type_errors = ["TypeErrors:"]
        for key, val in default.items():
            input_val = getattr(self, key)
            input_type = type(input_val)
            target_val = val[0]
            target_type = val[1]
            if type(target_type) is tuple:
                type_bool = input_type not in target_type
            else:
                type_bool = input_type != target_type
            if hasattr(param, key) and type_bool and target_val != None:
                if target_type == bool:
                    type_errors.append(
                        f"'{key}' is of type '{input_type.__name__}' and should be of type 'bool'."
                    )
                    continue
                casting = self.try_cast_to_native_type(input_val, target_type)
                if casting == None or (
                    target_type in [list, tuple, set] and input_type == str
                ):
                    if type(target_type) is tuple:
                        msg = "types "
                        for t in target_type:
                            msg += f"'{t.__name__}' or "
                        msg = msg[:-4]
                    else:
                        msg = f"type '{target_type.__name__}'"
                    type_errors.append(
                        f"'{key}' is of type '{input_type.__name__}' and could not be converted to {msg}."
                    )
                else:
                    setattr(self, key, casting)
        return type_errors

    def adjust_defaults_based_on_likelihood_filter(self):
        """
        Adjust default values of keep_initial_data and keep_first_iteration
        based on the value of use_likelihood_filter. But only if keep_initial_data and keep_first_iteration
        are not already specified in the  input parameter file.

        If they are not specified, the following rules apply:
        - If use_likelihood_filter is True, keep_initial_data and keep_first_iteration are set to True.
        - If use_likelihood_filter is False, keep_initial_data and keep_first_iteration are set to False.

        Furthermore, if use_likelihood_filter is True and keep_initial_data is True, keep_first_iteration is also set to True.
        To ensure logical consistency.
        """
        if not hasattr(self, "keep_initial_data") or not hasattr(
            self, "keep_first_iteration"
        ):
            # Ensure these parameters exist before modifying them
            return

        if "use_likelihood_filter" in self.param_file:
            if self.use_likelihood_filter:
                # Default behavior when likelihood filter is used
                if "keep_initial_data" not in self.param_file:
                    self.keep_initial_data = True
                if "keep_first_iteration" not in self.param_file:
                    self.keep_first_iteration = True
            else:
                # Default behavior when likelihood filter is not used
                if "keep_initial_data" not in self.param_file:
                    self.keep_initial_data = False
                if "keep_first_iteration" not in self.param_file:
                    self.keep_first_iteration = False

        # Enforce logical consistency: if keep_initial_data is True, keep_first_iteration must also be True
        if (
            self.keep_initial_data
            and not self.keep_first_iteration
            and self.use_likelihood_filter
        ):
            print(
                "Warning: Enforcing keep_first_iteration=True because keep_initial_data=True."
            )
            self.keep_first_iteration = True


if __name__ == "__main__":
    sys.stdout.reconfigure(line_buffering=True)
    main_instance = Main()
    main_instance.run()
